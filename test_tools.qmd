# Test tools {#sec-tests-tools}

<!---
https://www.freecodecamp.org/news/clean-coding-for-beginners/

-->

```{r}
#| eval: true 
#| echo: false 
#| include: false
source("_common.R")
library(rlang)
```

```{r}
#| label: co_box_dev
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "o", look = "minimal",
  header = "Caution",
  contents = "This chapter is under review.",
  fold = FALSE
)
```

This chapter introduces tools to help write clean and efficient tests. These files, folders and methods can be included in our app-package test suite and are described in-depth in [R Packages, 2ed](https://r-pkgs.org/testing-design.html#sec-testing-design-principles), and the [`testthat` documentation.](https://testthat.r-lib.org/articles/third-edition.html) (but within the context of a standard R package). The sections below contain examples of each tool implemented in our app-package.

## Testing best practices

One of the motivations for developing a Shiny application in an R package is the ability to test your code. R packages have a well-established framework for testing (`testthat`), which allows us to write and run unit tests. At the time of this writing, the second edition of R Packages also dedicates three chapters to testing.[^test_tools-1]

[^test_tools-1]: The three chapters in R Packages, 2ed, dedicated to testing are [Testing basics](https://r-pkgs.org/testing-basics.html), [Designing your test suite](https://r-pkgs.org/testing-design.html), and [Advanced testing techniques](https://r-pkgs.org/testing-advanced.html)

### Test scope

One of the recent updates to the `testthat` package (version 3.2.0) emphasizes limiting code that exists outside of our tests.[^test_tools-2]

[^test_tools-2]: Read a summary of the changes to the `testthat` package on the [tidyverse blog](https://www.tidyverse.org/blog/2023/10/testthat-3-2-0/)

> "*Eliminating (or at least minimizing) top-level code outside of `test_that()` will have the beneficial effect of making your tests more hermetic. This is basically the testing analogue of the general programming advice that it's wise to avoid unstructured sharing of state.*" - [Self-sufficient tests, R Packages, 2ed](https://r-pkgs.org/testing-design.html#self-sufficient-tests)

The code outside of `test_that()` usually serves a specific purpose: functions called to load, create, or manipulate data; setting options; creating folders or files; databases connections; etc. The topics in this chapter will demonstrate how to include these behaviors in our tests while adhering to the advice above by properly using test fixtures, helpers, and mocks.

::: {.callout-note collapse="false" appearance="minimal"}

### BDD refresher: features & scenarios

In BDD, requirements are written plain language 'feature files' using a series of keywords:   

```{bash}
#| eval: false 
#| code-fold: false 
Feature: # <1>
  # <1>
  Background: # <2>
    Given # <3> 
    
  Scenario: # <4>
    When # <5>
    And # <6> 
    Then # <7>
    # <4>
```
1. **High-level description** (title and description)  
2. Steps or conditions that exist **before each scenario**   
3. Used to **describe the initial context** or **preconditions for the scenario**      
4. A series of steps outlining a **concrete examples that illustrates a feature**   
5. Used to **describe an event, or an action**  
6. Use to combine **`Given`**, **`When`**, or **`Then`**     
7. Use to **verify expected outcomes that are observable by a user**  

`Feature`, `Background`, and `Scenario` information can be included in nested `testthat::describe()` blocks, but every `Then` keyword should have a corresponding `testthat::it()` call.  

Read more about Gherkin on the [Cucumber website.](https://cucumber.io/docs/gherkin/reference/).

:::

## Test fixtures

```{r}
#| label: git_box_12b_tests-fixtures
#| echo: false
#| results: asis
#| eval: true
git_margin_box(
  fig_pw = '65%', 
  branch = "12b_tests-fixtures", 
  repo = 'moviesApp')
```

<!-- https://www.tidyverse.org/blog/2020/04/self-cleaning-test-fixtures/ -->

The two principles to keep in mind while developing test fixtures, helpers, and mocks in an app-package are:

1.  Keep test scope logic defined to a single test (or all tests)

2.  Explicitly setup and tear down anything created during a test (and reset test state between each test run)

Test fixtures can be anything used to create repeatable test conditions (data, file paths, functions, etc.). Good test fixtures provide a consistent, well-defined test environment, and then are removed/destroyed when the test is executed. This ensures any changes made during the test don't persist or interfere with future tests.[^test_tools-5]

[^test_tools-5]: For a concrete example, see [this article](https://www.tidyverse.org/blog/2020/04/self-cleaning-test-fixtures/) on self-cleaning tests.

In R packages, test fixtures are stored in the `tests/testthat/fixtures/` folder:[^tests-fixtures-folder-name]

[^tests-fixtures-folder-name]: The name '`fixtures`' isn't required (you can name this folder anything).

```{bash}
#| eval: false 
#| code-fold: false
tests/
├── testthat/
│   └── fixtures/                                         
└── testthat.R
```



### Test data

Large static data files are an example of a test fixture.[^test_tools-6] Any code used to create test data should be stored with the output file (using a clear naming convention).

[^test_tools-6]: Creating a tidied version of `ggplot2movies::movies` would be costly to re-create with every test, so it's advised to store it as an [static test fixture.](https://r-pkgs.org/testing-advanced.html#sec-testing-advanced-concrete-fixture)

For example, I've stored the code used to create a ['tidy' version](https://github.com/mjfrigaard/moviesApp/blob/12b_tests-fixtures/tests/testthat/fixtures/make-tidy_ggp2_movies.R) of the `ggplot2movies::movies` data along with the output dataset in `tests/testthat/fixtures/`:

```{bash}
#| eval: false 
#| code-fold: false 
tests
├── testthat
│   ├── fixtures
│   │   ├── make-tidy_ggp2_movies # <1>
│   │   └── tidy_ggp2_movies.rds # <2>
│   └── test-scatter_plot.R
└── testthat.R

3 directories, 4 files
```

1.  The code used to create the test data (`make-make_tidy_ggp2_movies.R`)
2.  The test data file (i.e., `tidy_ggp2_movies.rds`):

Data files stored in `tests/testthat/fixtures/` can be accessed with `testthat::test_path()` inside each test.

### Example: test fixture

Below is a test that answers the question, '*does the plot generate without producing an error?*' when using the `tidy_ggp2_movies.rds` test fixture with the `scatter_plot()` utility function. This type of test appropriate because we want to confirm the data source (`tidy_ggp2_movies`) will generate a plot object when passed to the `scatter_plot()` utility function, not necessarily the specific contents of the graph.[^test_tools-is.ggplot]

[^test_tools-is.ggplot]: Snapshot tests would be more appropriate for answering the question, ['*is the plot visually correct?*'](https://shiny.posit.co/r/articles/improve/server-function-testing/index.html#complex-outputs-plots-htmlwidgets).

```{r}
#| eval: false 
#| code-fold: false
testthat::describe( # <1>
  "Feature: Scatter plot data visualization

     As a film data analyst
     I want to explore movie review data from IMDB.com
     So that I can analyze relationships between movie reivew metrics

   Background:
     Given I have data with IMDB movie reviews
     And the data contains continuous variables like 'rating'
     And the data contains categorical variables like 'mpaa'", 
     code = { # <1>
      
    testthat::it( # <2>
      "Scenario: Scatter plot initial x, y, color values 
         When I launched the Scatter Plot Data Visualization
         And I have a dataset of movie reviews from IMDB 
         Then the scatter plot should show 'Rating' on the x-axis
         And the scatter plot should show 'Length' on the y-axis
         And the points on the scatter plot should be colored by 'MPAA' rating", 
        code = { # <2>
        
    ggp2_scatter_inputs <- list(  # <3>
          x = "rating",
          y = "length",
          z = "mpaa",
          alpha = 0.75,
          size = 3,
          plot_title = "Enter plot title"
        )              # <3>
        
    tidy_ggp2_movies <- readRDS(test_path("fixtures", # <4>
                                "tidy_ggp2_movies.rds")) # <4>
    app_graph <- scatter_plot(tidy_ggp2_movies,  # <5>
      x_var = ggp2_scatter_inputs$x,
      y_var = ggp2_scatter_inputs$y,
      col_var = ggp2_scatter_inputs$z,
      alpha_var = ggp2_scatter_inputs$alpha,
      size_var = ggp2_scatter_inputs$size
    ) # <5>
    expect_true(ggplot2::is.ggplot(app_graph)) # <6> 
  }) 

}) # <1>
```

1.  Feature   
2.  Scenario   
3.  Test inputs   
4.  Test fixture  
5.  Create observed object  
6.  Expectation

#### Test development 

While developing, using keyboard shortcuts makes it easier to iterate between building fixtures, writing and running tests, and checking code coverage.

```{r}
#| label: hot_key_tf_01
#| echo: false
#| results: asis
#| eval: true
hot_key(fun = 'tf')
```

```{verbatim}
#| eval: false 
#| code-fold: false
devtools:::test_active_file()
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ]
```

```{r}
#| label: hot_key_cf_01
#| echo: false
#| results: asis
#| eval: true
hot_key(fun = 'cf')
```

```{r}
#| eval: false 
#| code-fold: false
devtools:::test_coverage_active_file()
```

![Test coverage on active file](img/11_tests_coverage_scatter_plot.png){width="100%" fig-align="center"}

If the data `tidy_ggp2_movies.rds` in `tests/testthat/fixtures/` are going to be used repeatedly, it might also make sense to store it in `inst/extdata/` or `data-raw/`. Test fixtures are described in-depth in [R Packages, 2ed](https://r-pkgs.org/testing-advanced.html#test-fixtures) and in the [`testthat` documentation](https://testthat.r-lib.org/articles/test-fixtures.html#test-fixtures).

## Test helpers

<!-- https://r-pkgs.org/testing-design.html#testthat-helper-files -->

```{r}
#| label: git_box_12c_tests-helpers
#| echo: false
#| results: asis
#| eval: true
git_margin_box(
  fig_pw = '65%', 
  branch = "12c_tests-helpers", 
  repo = 'moviesApp')
```

> "*Helper files are a mighty weapon in the battle to eliminate code floating around at the top-level of test files.*" Testthat helper files, [R Packages, 2ed](https://r-pkgs.org/testing-design.html#testthat-helper-files)

Test helpers reduce repeated/duplicated test code. In general, objects or values that aren't large enough to justify storing as static test fixtures can be created with helper functions. Helper functions are stored in `tests/testthat/helper.R`, which is automatically loaded with `devtools::load_all()`:

```{bash}
#| eval: false 
#| code-fold: false
tests/
  ├── testthat/
  │   ├── fixtures/                                 # <1>
  │   │   ├── make-tidy_ggp2_movies.R               # <1>
  │   │   └── tidy_ggp2_movies.rds                  # <1>
  │   ├── helper.R                                  # <2>
  │   └── test-scatter_plot.R                       # <3>
  └── testthat.R
```

1.  Test fixture scripts and `.rds` files\
2.  Helper functions\
3.  Test file

Test helpers should only be created if they make testing easier **when the tests fail.** The article, ['Why Good Developers Write Bad Unit Tests'](https://mtlynch.io/good-developers-bad-tests/), provides great advice on complexity vs. clarity when writing unit tests,

> *'think about what will make the problem obvious when a test fails. Refactoring may reduce duplication, but it also increases complexity and potentially obscures information when things break.'*

R programmers resist copy + paste programming, and in most cases this makes sense. After all, R *is* a functional programming language, so it's tempting to bundle any repeated code into a function and store it in the `tests/testthat/helper.R` file.

However, when we're writing tests, it's more important that tests are easy to read and understand **when they fail**.

For example, consider the `ggp2_scatter_inputs` inputs passed to the `scatter_plot()` function in the previous test:

```{r}
#| eval: false 
#| code-fold: false 
ggp2_scatter_inputs <- list(  
        x = "rating",
        y = "length",
        z = "mpaa",
        alpha = 0.75,
        size = 3,
        plot_title = "Enter plot title"
)            
```

We could write a `var_inputs()` function that stores these values in a list. In our tests, this would allow us to use `var_inputs()` with the same 'reactive syntax' we use with `scatter_plot()` in the module server function:

```{r}
#| eval: true 
#| code-fold: false 
#| collapse: true
var_inputs <- function() {
   list(x = "rating",
        y = "length",
        z = "mpaa",
        alpha = 0.75,
        size = 3,
        plot_title = "Enter plot title")
}
var_inputs()
```

While this removes duplicated code, it also makes it less clear for the reader what `var_inputs()` contains and where it was created (without opening the `helper.R` file).

```{r}
#| eval: false 
#| code-fold: false 
#| collapse: true
testthat::it(
  "Scenario: Scatter plot initial x, y, color values 
     When I launched the Scatter Plot Data Visualization
     And I have a dataset of movie reviews from IMDB 
     Then the scatter plot should show 'Rating' on the x-axis
     And the scatter plot should show 'Length' on the y-axis
     And the points on the scatter plot should be colored by 'MPAA' rating", 
  code = {
           
    tidy_ggp2_movies <- readRDS(test_path("fixtures", # <1>
                                "tidy_ggp2_movies.rds")) # <1>
  
app_graph <- scatter_plot(
  tidy_ggp2_movies,
  x_var = var_inputs()$x, # <2>
  y_var = var_inputs()$y,
  col_var = var_inputs()$z,
  alpha_var = var_inputs()$alpha,
  size_var = var_inputs()$size) # <2>

testthat::expect_true(ggplot2::is.ggplot(app_graph))
})
```
1. Load test fixture  
2. This is how we refer to the inputs in `mod_scatter_display_server()`


```{r}
#| label: co_box_dry
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "r", 
  fold = FALSE,
  look = 'simple',
  size = "1.05", 
  hsize = "1.15",
  header = "Violating the DRY principle",
  contents = "
If you have repeated code in your tests, consider the following questions below before creating a helper function: 
  
1. Does the code help explain what behavior is being tested? 
  
2. Would a helper make it harder to debug the test when it fails?  
  
It’s more important that test code is obvious than DRY, because it’s more likely you’ll be dealing with this test when it fails (and you're not likely to remember why all the top-level code is there).
  
"
)
```

In contrast, the `make_ggp2_inputs()` function below creates inputs for the `scatter_plot()` utility function:

```{r}
#| eval: false 
#| code-fold: false 
make_ggp2_inputs <- function() {
  glue::glue_collapse("list(x = 'rating',
     y = 'length',
     z = 'mpaa',
     alpha = 0.75,
     size = 3,
     plot_title = 'Enter plot title'
     )"
  )
}
```

I can call `make_ggp2_inputs()` in the **Console** and it will return the list of values to paste into each test:

```{r}
#| eval: false 
#| code-fold: false 
make_ggp2_inputs()
list(y = 'audience_score', 
     x = 'imdb_rating',
     z = 'mpaa_rating',
     alpha = 0.5,
     size = 2,
     plot_title = 'Enter plot title'
    )
```

This reduces the number of keystrokes per test, but doesn't obscure the source of the values in the test.

`glue::glue_collapse()` is your friend when you want to quickly reproduce code for your tests. `make_var_inputs()` creates the list of inputs for testing the original `movies` data:

```{r}
#| eval: true 
#| code-fold: false 
make_var_inputs <- function() {
  glue::glue_collapse("list(y = 'audience_score', 
     x = 'imdb_rating',
     z = 'mpaa_rating',
     alpha = 0.5,
     size = 2,
     plot_title = 'Enter plot title'
    )")
}
```

#### Logging

I prefer test outputs to be verbose, so I usually create a `test_logger()` helper function that allows me to give more context and information with each test:

```{r}
#| code-fold: false
#| eval: true
# test logger helper
test_logger <- function(start = NULL, end = NULL, msg) {
  if (is.null(start) & is.null(end)) {
    cat("\n")
    logger::log_info("{msg}")
  } else if (!is.null(start) & is.null(end)) {
    cat("\n")
    logger::log_info("\n[ START {start} = {msg}]")
  } else if (is.null(start) & !is.null(end)) {
    cat("\n")
    logger::log_info("\n[ END {end} = {msg}]")
  } else {
    cat("\n")
    logger::log_info("\n[ START {start} = {msg}]")
    cat("\n")
    logger::log_info("\n[ END {end} = {msg}]")
  }
}
```

`test_logger()` can be used to 'log' the `start` and `end` of each test, and it includes a message argument (`msg`) I'll use to reference the test `description` argument in each `it()` call.[^test_tools-logging]

[^test_tools-logging]: If you like verbose logging outputs, check out the [`logger` package](https://daroczig.github.io/logger/)

I tend to use functions like `test_logger()` enough to justify placing them in a testing utility file ([`R/testthat.R`](https://github.com/mjfrigaard/moviesApp/blob/10c_tests-helpers/R/testthat.R)) below `R/`. Including testing functions in the `R/` folder also ensures they're documented and any dependencies become part of your app-package).[^test_tools-helpers-r-dir]

[^test_tools-helpers-r-dir]: Placing common files for testing below `R/` is covered in [R Packages, 2ed](https://r-pkgs.org/testing-design.html#hiding-in-plain-sight-files-below-r)

#### Example: snapshots

Writing tests for graph outputs can be difficult because the "correctness" of a graph is somewhat subjective and requires human judgment. If the expected output we're interesting in testing is cumbersome to describe programmatically, we can consider using a snapshot tests. Examples of this include UI elements (which are mostly HTML created by Shiny's UI layout and input/output functions) and data visualizations.[^test_tools-snapshot-advice]

[^test_tools-snapshot-advice]: Mastering Shiny covers [creatng a snapshot file](https://mastering-shiny.org/scaling-testing.html#user-interface-functions) to test UI elements, but also notes this is probably not the best approach.

If we want to create a graph snapshot test, the [`vdiffr`](https://vdiffr.r-lib.org/) package allows us to perform a 'visual unit test' by capturing the expected output as an `.svg` file that we can compare with future versions.

The `expect_doppelganger()` function from `vdiffr` is designed specifically to work with ['graphical plots'](https://vdiffr.r-lib.org/reference/expect_doppelganger.html).

```{r}
#| eval: false 
#| code-fold: false
vdiffr::expect_doppelganger(
      title = "name of graph", 
      fig = # ...code to create graph...
  )
```

Another option for using snapshots for testing is the `expect_snapshot_file()` function [^test_tools-12] but `expect_doppelganger()` is probably the better option for comparing graph outputs.

[^test_tools-12]: Follow the `expect_snapshot_file()` example from the [`testthat` documentation](https://testthat.r-lib.org/reference/expect_snapshot_file.html#ref-examples)

In this test, I'll list the `Feature` in a top-level `describe()` block, then nest the `Scenario` for the functional requirement in `it()` statements (with each snapshot the test will capture). Combining `Scenario`s in the same test file is helpful if we're trying to keep a 1:1 between the `test/testthat/` file names and file names in `R/`.[^test_tools-13]

[^test_tools-13]: matching files names between `R/` and `tests/testthat/` keeps our code organized and ensures the `devtools::test_coverage_active_file()` function [works.](https://testthat.r-lib.org/articles/special-files.html)

```{r}
#| eval: false 
#| code-fold: false
testthat::it( # <1>
  "Scenario: Create scatter plot
      Given I have launched the movie review exploration app,
      When the scatter plot renders,
      Then the points on the x axis should represent 'Ratings'
      And the points on the y axis should represent 'Length'
      And the points should be colored by 'MPAA' rating
      And the size of the points should be set to '2'
      And the opacity of the points should be set to '0.5'",
  code = {
    
    test_logger( # <2>
      start = "snap scatter_plot()", 
      msg = "initial x,y,z,size,alpha")  # <2>

    scatter_inputs <- list(  # <3>
      x = "imdb_rating",
      y = "audience_score",
      z = "mpaa_rating",
      alpha = 0.5,
      size = 2,
      plot_title = "Enter plot title"
    ) # <3>

    vdiffr::expect_doppelganger( # <4>
      title = "Initial x y z axes",
      fig = scatter_plot(movies,
        x_var = scatter_inputs$x,
        y_var = scatter_inputs$y,
        col_var = scatter_inputs$z,
        alpha_var = scatter_inputs$alpha,
        size_var = scatter_inputs$size
      ) +
        ggplot2::labs(
          title = scatter_inputs$plot_title,
          x = stringr::str_replace_all(
            tools::toTitleCase(
              scatter_inputs$x
            ), "_", " "
          ),
          y = stringr::str_replace_all(
            tools::toTitleCase(
              scatter_inputs$y
            ), "_", " "
          )
        ) +
        ggplot2::theme_minimal() +
        ggplot2::theme(legend.position = "bottom")
    ) # <4>

    test_logger( # <5>
      end = "snap scatter_plot()", 
      msg = "initial x,y,z,size,alpha") # <5>
    
  }
)# <1>
```
1.  Test scope (**T2**)\
2.  Log start (**T2**)\
3.  Initial `movies` variable inputs for `x`, `y`, and `z` from UI\
4.  Snapshot with initial values\
5.  Log end (**T2**)\

In this test, I've separated the scenarios into it's own `it()` sections, and the test results return the output from `test_logger()` for more context of what's being tested:

```{verbatim}
#| eval: false 
#| code-fold: false
INFO [2023-10-27 10:58:25] [ START snap scatter_plot() = initial x,y,z,size,alpha]
[ FAIL 0 | WARN 1 | SKIP 0 | PASS 3 ]
INFO [2023-10-27 10:58:25] [ END snap scatter_plot() = initial x,y,z,size,alpha]

INFO [2023-10-27 10:58:25] [ START snap scatter_plot() = updated x,y,z]
[ FAIL 0 | WARN 2 | SKIP 0 | PASS 4 ]
INFO [2023-10-27 10:58:26] [ END snap scatter_plot() = updated x,y,z]
```

We also see a warning when the snapshot has been saved in the `tests/testthat/_snaps/` folder the first time the test is run:

```{verbatim}
#| eval: false 
#| code-fold: false
── Warning (test-scatter_plot.R:124:9): 
      Scenario: Create scatter plot
          Given I have launched the movie review exploration app,
          When the scatter plot renders,
          Then the points on the x axis should represent 'Ratings'
          And the points on the y axis should represent 'Length'
          And the points should be colored by 'MPAA' rating
          And the size of the points should be set to '2'
          And the opacity of the points should be set to '0.5' ──
Adding new file snapshot: 'tests/testthat/_snaps/initial-x-y-z-axes.svg'

── Warning (test-scatter_plot.R:186:7): 
      Scenario: Change x, y, color values for plotting
        When I launch the Scatter Plot Data Visualization
        And I select the variable 'Audience Score' for the x-axis
        And I select the variable 'IMDB Rating' for the y-axis
        And I select the variable 'Critics Rating' for the color
        Then the scatter plot should show 'Audience Score' on the x-axis
        And the scatter plot should show 'IMDB Rating' on the y-axis
        And the points on the scatter plot should be colored by 'Critics Rating' ──
Adding new file snapshot: 'tests/testthat/_snaps/updated-x-y-color.svg'
[ FAIL 0 | WARN 2 | SKIP 0 | PASS 2 ]
```

On subsequent runs, this warning will disappear (as long as there are no changes to the `.svg` files). 

```{r}
#| label: co_box_expect_doppelganger
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "g", fold = TRUE, 
  size = "1.05",
  header = "Reviewing snapshots",
  contents = "
Placing the functional requirement in the `title` argument of `expect_doppelganger()` gives us a clear idea of what the snapshot file *should* contain.
"
)
```

#### Advice on snapshots

Snapshots are brittle. The term "brittle" in the context of testing refers to a susceptibility to fail with small changes. Brittleness can produce false negatives test failures (i.e., due to inconsequential changes in the graph) when comparing a new graph to the baseline image.

Below is the output from `diffobj::diffObj()` comparing our custom plotting function (`scatter_plot()`) against a graph built with analogous `ggplot2` code:

```{r}
#| eval: false 
#| code-fold: false
ggp_graph <- ggplot2::ggplot(mtcars, 
              ggplot2::aes(x = mpg, y = disp)) + 
              ggplot2::geom_point(
                ggplot2::aes(color = cyl), 
                             alpha = 0.5, 
                             size = 3)
  
app_graph <- scatter_plot(mtcars, 
                  x_var = "mpg", 
                  y_var = "disp", 
                  col_var = "cyl", 
                  alpha_var = 0.5, 
                  size_var = 3)

diffobj::diffObj(ggp_graph, app_graph)
```

::: column-page-inset-right
::: {#fig-11_tests_diffobj_scatter_plot}
![`diffobj::diffObj()` on graph outputs](img/11_tests_diffobj_scatter_plot.png){width="100%" align="center"}

Graph objects are difficult to use as test objects
:::
:::

The output shows us all the potential points of failure when comparing complex objects like graphs (despite the actual outputs appearing identical), so it's best to limit the number of 'visual unit tests' unless they're absolutely necessary.

## Test mocks

```{r}
#| label: git_box_12f_tests-mocks
#| echo: false
#| results: asis
#| eval: true
git_margin_box(
  fig_pw = '65%', 
  branch = "12f_tests-mocks", 
  repo = 'moviesApp')
```

Test code may rely on external systems, behavior, functions, or objects. To ensure that our unit tests remain fast and focused solely on the functional requirement being tested, it's important to minimize these external dependencies.

Test mocking functions are a relatively new addition to `testthat` [^tests-mocking-testthat-3e]. The mocking functions can be used to substitute functions by emulating their behavior within the test scope (in BDD terms, mocks are creating the `Given` conditions).

[^tests-mocking-testthat-3e]: Read more about mocking in the updates to [`testthat`](https://www.tidyverse.org/blog/2023/10/testthat-3-2-0/#mocking)

### Example: mocking add-on functions

Instead of real-time computations, mocks return predefined responses to given inputs. Consider the `check_installed()` function below:[^tests-pkg-dev-wrkshp]

[^tests-pkg-dev-wrkshp]: This example comes from the package development masterclass [workshop at `posit::conf(2023)`](https://github.com/posit-conf-2023/pkg-dev-masterclass/tree/main).


```{r}
#| eval: true 
#| code-fold: false
check_installed <- function(package) {
  if (is_installed(package)) {
    return(invisible())
  } else {
    stop("Please install '{package}' before continuing")
  }
}
```

Below is a feature  description for `check_installed()` and two scenarios for each expected behavior:

```{verbatim}
#| eval: false
#| code-fold: false
Feature: Checking if an R package is installed

  Scenario: Checking an installed package
    Given the R package 'base' is installed
    When I call the `check_installed()` function with 'base'
    Then the function should return without any error

  Scenario: Checking an uninstalled package
    Given the R package 'foo' is not installed
    When I call the `check_installed()` function with 'foo'
    Then the function should raise an error with the message
      `Please install 'nonexistent_package' before continuing`
```

The `check_installed()` shouldn't be confused with [`rlang::check_installed()`](https://rlang.r-lib.org/reference/is_installed.html), which checks if a package is installed, and if it isn't, prompts the user install the package using [`pak::pkg_install()`](https://pak.r-lib.org/reference/pkg_install.html). 

Lets review how `is_installed()` behaves with installed and missing packages: 

```{r}
#| code-fold: false
#| collapse: true
rlang::is_installed('foo')
rlang::is_installed('base')
```

The version of `check_installed()` in `moviesApp` will check if a package is installed and return `invisible()` if it is (which, when assigned to an object, evaluates to `NULL`):

```{r}
#| code-fold: false
#| collapse: true
#| error: true
check_installed('base')
```

```{r}
#| code-fold: false
#| collapse: true
#| error: true
x <- check_installed('base')
x
```

If the package is not installed, `check_installed()` prints an error message:

```{r}
#| code-fold: false
#| collapse: true
#| error: true
check_installed('foo')
```

To use mocking with `is_installed()`, we'll use the following syntax: 

```{verbatim}
#| eval: false
#| code-fold: false
local_mocked_bindings(
  {local function} = function(...) {value}
)
```

In this case, `{local function}` is `is_installed()` from `rlang`, and we want to test the two possible `{value}`s (`TRUE`/`FALSE`). 

In the first test, we'll use `expect_error()` to confirm that the error message is returned for an uninstalled package by using `local_mocked_bindings()` and setting the `is_installed()` value to `FALSE`:

```{r}
#| eval: false
#| code-fold: false
describe("Feature: Checking if an R package is installed", {
  
  test_that(
    "Scenario: Checking an uninstalled package
        Given the R package 'foo' is not installed
        When I call the `check_installed()` function with 'foo'
        Then the function should raise an error with the message
        `Please install 'nonexistent_package' before continuing`", {
          
    test_logger(start = "mock is_installed", msg = "FALSE") # <1>
    local_mocked_bindings(is_installed = function(package) FALSE) # <2>
    expect_error(object = check_installed("foo")) # <3>
    test_logger(end = "mock is_installed", msg = "FALSE") # <1>
    
  })
  
})
```
1. Log test `start` and `end`
2. Set `{value}` to `FALSE`   
3. Pass a package we know **is not installed**   


To test installed packages, we'll confirm `check_installed('foo')` with `expect_invisible()`: 

```{r}
#| eval: false
#| code-fold: false
describe("Feature: Checking if an R package is installed", {
  
  test_that(
    "Scenario: Checking an installed package
        Given the R package 'base' is installed
        When I call the `check_installed()` function with 'base'
        Then the function should return without any error", {
          
    test_logger(start = "mock is_installed", msg = "TRUE") # <1>
    local_mocked_bindings(is_installed = function(package) TRUE) # <2>
    expect_invisible(check_installed("base")) # <3>
    test_logger(end = "mock is_installed", msg = "TRUE") # <1>
    
  })
})
```
1. Log test `start` and `end`
2. Set `{value}` to `TRUE`   
3. Pass a package we know **is installed**   

The output from the tests above is provided below: 

```{verbatim}
#| eval: false 
#| code-fold: false
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 0 ]
INFO [2023-10-08 22:59:43] [ START mock is_installed = FALSE]
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ]
INFO [2023-10-08 22:59:43] [ END mock is_installed = FALSE]

INFO [2023-10-08 22:59:43] [ START mock is_installed = TRUE]
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 2 ]
INFO [2023-10-08 22:59:43] [ END mock is_installed = TRUE]
```

#### Notes on mocking

The `roxygen2` documentation for `check_installed()` uses the `@importFrom` tag to import `is_installed` and add it to the `moviesApp` namespace (using explicit namespacing alone won't work):

```{r}
#| eval: false 
#| code-fold: false
#' Check if package is installed
#' 
#' @description
#' An example function for demonstrating how to use `testthat`'s
#' mocking functions.
#' 
#' @param package string, name of package
#'
#' @return invisible 
#'
#' @importFrom rlang is_installed # <1>
#'
#' @export
#'
#' @examples
#' check_installed("foo")
#' check_installed("base")
```
1. Fortunately we already included rlang in our `DESCRIPTION` file for `.data` in `scatter_plot()`

Using `testthat`'s mocking functions allow us to craft unit tests that evaluate a single, specific behavior. Read more about mocking functions on the [`testthat` webite](https://testthat.r-lib.org/reference/local_mocked_bindings.html).

## Recap

```{r}
#| label: co_box_app_recap
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "b",
  header = "Recap: test suite",
  contents = "

**Shiny app-packages test suite**
  
- **Fixtures**: Fixtures prepare the test environment and the initial state. The `tidy_ggp2_movies.rds` data is a test fixture, and it creates the `tidy_ggp2_movies` data within the test scope. 

- **Helpers**: If find yourself writing small, reusable pieces of code to perform specific tasks inside your tests, consider converting them into a function in `tests/testthat/helper.R`.

- **Mocks**: Mocking allows us to isolate a function we're testing from external dependencies by simulating behaviors inside the test. We used `local_mocked_bindings()` from `testthat` to mock the bahavior of `rlang::is_installed()`.
  
  ", 
  fold = FALSE
)
```
