# Tests {.unnumbered}

```{r}
#| eval: true 
#| echo: false 
#| include: false
source("_common.R")
library(testthat)
```

```{r}
#| label: co_box_dev
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "y",
  header = "Caution!",
  contents = "This section is currently being revised. Thank you for your patience."
)
```

Writing tests for your app-package poses some unique challenges. Shiny functions are written in the context of its [reactive model](https://shiny.posit.co/r/articles/build/reactivity-overview/), so some conventional testing techniques and methods for regular R packages don’t directly apply. Fortunately, the infrastructure for storing and running tests in your app-package is identical to a standard R package.

This chapter will cover three layers of tests: unit tests, integration/module tests, and end-to-end or system tests. I'll focus on *what to test* and *why to test it*, not *how to write tests*, because plenty of those resources exist[^unit-tests-1], [^system-tests-2]. The only exception being some tricks I've learned for using `testServer()` with module server functions (because I've found the available documentation to be lacking[^test-server-tests-3]). 

I’ll also touch on the links between user requirements, functional specifications, and using a traceability matrix to track issues & features.

[^unit-tests-1]: Unit tests are covered extensively in [R Packages, 2ed](https://r-pkgs.org/testing-basics.html) and the [`testthat` documentation](https://testthat.r-lib.org/index.html)

[^system-tests-2]: `shinytest2` has [excellent documentation](https://rstudio.github.io/shinytest2/) (and [videos](https://www.youtube.com/watch?v=Gucwz865aqQ)), and I highly recommend reading through those resources.

[^test-server-tests-3]: The [`testServer()`](https://shiny.posit.co/r/articles/improve/server-function-testing/) documentation is sparse, so I'll provide a few tips and tricks I've learned for testing module server functions.

The code chunk below will load the necessary testing packages.

```{r}
#| eval: false
#| code-fold: false
install.packages(c("testthat", "shinytest2", "covr"))
library(testthat)
library(shinytest2)
library(covr)
```

(*If you're using `devtools`, you won't have to worry about installing `testthat` and `covr`*)

## Running [`test()`]{style="font-size: 1.05em;"}s

```{r}
#| label: co_box_tests_pkgApp
#| echo: false
#| eval: false
```

 
The fourth `devtools` [habit]{style="font-weight: bold; font-size: 1.0em; color: #772953"} to adopt is regularly writing and running tests. If you're using Posit Workbench and have `devtools` installed, you can test your app-package using the **Build** pane or the keyboard shortcut:

::: {.column-margin}

[<kbd>Ctrl/Cmd</kbd> + <kbd>Shift</kbd> + <kbd>T</kbd>]{style="font-weight: bold; font-style: italic; font-size: 1.20em"}

:::


::: {#fig-08_tests_build_pane_test}

![Test you app-package](img/08_tests_build_pane_test.png){#fig-08_tests_build_pane_test width='100%' align='center'}

`devtools::test()` (run all tests in your `tests/` folder)
:::

When we initially run `devtools::test()` in `pkgApp`, we see the following:

```{verbatim}
#| eval: false
#| code-fold: false
==> devtools::test()

ℹ No testing infrastructure found.
• Setup testing with `usethis::use_testthat()`.
```

This shouldn't be surprising--we haven't written any tests for `pkgApp` yet!

```{r}
#| label: co_box_monthApp_tests
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "g", fold = TRUE,
  header = "Mastering Shiny `monthApp` tests",
" 
If you downloaded, loaded and installed [`monthApp` example from Mastering Shiny](https://github.com/hadley/monthApp), then clicked on **Test** in the **Build** pane, you also saw the following:

![Testing `monthApp` app-package](img/monthApp_test.png){width='100%' fig-align='center'}

"
  )
```

The error is informative because it tells us that `pkgApp` doesn’t have the testing infrastructure set up. In packages using `devtools`, the unit testing infrastructure is built with `usethis::use_testthat()`

## The test suite

Multiple strategies exist for testing code. For example, if you've adopted test-driven development (TDD)[^test-driven-dev-2], you'll develop tests before writing utility functions, modules, or your standalone app function. However, if you're a mere mortal like the rest of us, you'll typically develop your tests and functions in tandem. 

[^test-driven-dev-2]: "The tests should be written before the functionality that is to be tested. This has been claimed to have many benefits. It helps ensure that the application is written for testability, as the developers must consider how to test the application from the outset rather than adding it later." - [TDD, Wikipedia](https://en.wikipedia.org/wiki/Test-driven_development) 

Regardless of the testing strategy, we should follow the advice presented to us in the output above and set up the testing infrastructure with the [`testthat` package](https://testthat.r-lib.org/):

### [`use_testthat()`]{style="font-size: 1.05em;"}

The 'infrastructure' created by running `usethis::use_testthat()` is detailed below: 

```{r}
#| eval: false
#| code-fold: false
usethis::use_testthat()
```

-   Set active project to current working directory: 

    ```{verbatim}
    #| eval: false
    #| code-fold: false
    ✔ Setting active project to '/path/to/pkgApp'
    ```

-   In the `DESCRIPTION` file, add the `Suggests` field and include `testthat (>= 3.0.0)` and the testthat edition (`Config/testthat/edition: 3`)

    ```{verbatim}
    #| eval: false
    #| code-fold: false
    ✔ Adding 'testthat' to Suggests field in DESCRIPTION
    ✔ Adding '3' to Config/testthat/edition
    ```
    
-   A new `tests/` folder is created, with a `testthat/` subfolder:

    ```{verbatim}
    #| eval: false
    #| code-fold: false
    ✔ Creating 'tests/testthat/'
    ```
    
-   The `testthat.R` file is created (sometimes referred to as the test 'runner' because it runs all your tests).
    
    ```{verbatim}
    #| eval: false
    #| code-fold: false
    ✔ Writing 'tests/testthat.R'
    ```

Finally, we're given some advice on the next step for creating our first test: 

```{verbatim}
#| eval: false
#| code-fold: false
• Call `use_test()` to initialize a basic test file and open it for editing.
```

Our new `tests/` folder structure is below: 

```{bash}
#| eval: false
#| code-fold: false
tests/
  ├── testthat
  └── testthat.R

2 directories, 1 file
```


## Unit tests

<!--
> Test Core Functions: Use the `testthat` package to write unit tests for all core logic functions. These are the building blocks of your application and should be thoroughly tested.

> Mock Inputs: For functions that rely on external data or user input, use mock data to simulate various scenarios.

> Boundary Conditions: Test edge cases and boundary conditions to ensure stability.

> Test Coverage: Use tools like covr to measure test coverage and aim for a high percentage.
-->

If I'm writing write a unit test for the `scatter_plot()` function in `R/scatter_plot.R`, I'll create test file with `usethis::use_test("scatter_plot")`.

### New tests with [`use_test()`]{style="font-size: 1.05em;"}

```{r}
#| eval: false
#| code-fold: false
usethis::use_test("scatter_plot")
```

#### Unit test files

The IDE will automatically open the new test file: 

```{verbatim}
#| eval: false
#| code-fold: false
✔ Writing 'tests/testthat/test-scatter_plot.R'
• Modify 'tests/testthat/test-scatter_plot.R'
```

#### Tests

The new test file contains a boilerplate test (I've included the argument names):

```{r}
#| eval: false
#| code-fold: false
test_that(desc = "multiplication works", code = {
  expect_equal(2 * 2, 4)
})
```

Each `testthat` test has a test context (supplied to the `desc` argument) followed by the test `code` (supplied in curly brackets). When a test is run, you'll see feedback on whether it passes or fails:

```{r}
#| eval: true
#| code-fold: false 
#| collapse: true
test_that(desc = "multiplication works", code = {
  expect_equal(2 * 2, 4)
})
```

#### Expectations

Most expectation have two parts: an `observed` object, and an `expected` object. The `observed` object is an artifact of some code you've written, and it's being compared against a known result (i.e., what is `expected`)

```{r}
#| eval: true 
#| code-fold: false
expect_equal(
  object = 2 * 2,
  expected = 4)
```

### Comparisons

All tests are based on comparisons, and I've found knowing what package is performing the underlying comparison often saves me from surprising tests results. 

For example, `testthat::expect_equal()` compares the `observed` and `expected` objects with the [`waldo` package](https://www.tidyverse.org/blog/2021/08/waldo-0-3-0/), with some help from [`diffobj`](https://github.com/brodieG/diffobj).

```{r}
#| eval: true 
#| code-fold: false
#| collapse: true 
#| message: false 
#| warning: false
library(waldo)
library(diffobj)
library(tibble)
```

#### [`waldo`]{style="font-size: 1.05em;"}

If you'd like a preview of a comparison before writing a formal test, you can pass the your `observed` and `expected` objects to `waldo::compare()` to see what the result will be, but be mindful of the difference in argument names:

```{r}
#| eval: true 
#| include: false
old <- tibble(
  chr = LETTERS[2:4],
  num = as.double(c(1, 2, 3)),
  fct = factor(c("low", "med", "high"), 
        levels = c("low", "med", "high"), 
        labels = c("L", "M", "H"),
        ordered = TRUE)
)
new <- data.frame(
  CHR = LETTERS[2:4],
  num = as.integer(c(1, 2, 3)),
  fct = factor(c("low", "med", "high"),
        levels = c("low", "med", "high"),
        labels = c("low", "med", "high"))
)
```


For example, suppose we have two objects: `old` and `new`

```{r}
#| eval: true 
#| code-fold: false
#| collapse: true
str(old)
```

```{r}
#| eval: true 
#| code-fold: false
#| collapse: true
str(new)
```

::: {layout="[[30,70]]"}

```{r}
#| eval: true 
#| code-fold: false
#| collapse: true
compare(
  x = old, 
  y = old)
```


```{r}
#| eval: true 
#| code-fold: false
#| collapse: true
compare(
  x = old, 
  y = new)
```

:::

#### [`diffobj`]{style="font-size: 1.05em;"}

If you're using Posit Workbench, the [`diffobj` package](https://github.com/brodieG/diffobj) has a colorful display for making comparisons in the IDE. 

The differences can be displayed vertically with `diffobj::diffObj()`:

::: {layout="[[30,70]]"}

```{r}
#| eval: false 
#| code-fold: false
diffObj(
  old, 
  new)
```

![Viewer ouput from `diffobj::diffObj()`](img/08_tests_diffobj.png){#fig-08_tests_diffobj width='85%' align='center'}

:::

If you want to view the structure (`str()`) differences, you can use `diffobj::diffStr()`:

::: {layout="[[30, 70]]"}

```{r}
#| eval: false 
#| code-fold: false
diffStr(
  old, 
  new)
```

![Viewer ouput from `diffobj::diffStr()`](img/08_tests_diffstr.png){#fig-08_tests_diffobj width='100%' align='center'}
:::

After seeing the `old` vs `new` comparisons with `waldo` and `diffobj`, you should notice the similarities in the results of a `testthat` test on the same objects:

```{verbatim}
#| eval: false 
#| code-fold: false
[ FAIL 1 | WARN 0 | SKIP 0 | PASS 0 ]

── Failure (test-old-vs-new.R:18:3): old vs. new ──────────
`old` (`actual`) not equal to `new` (`expected`).

`class(actual)`:   "tbl_df" "tbl" "data.frame"
`class(expected)`:                "data.frame"

`names(actual)`:   "chr" "num" "fct"
`names(expected)`: "CHR" "num" "fct"

`actual$chr` is a character vector ('B', 'C', 'D')
`expected$chr` is absent

`class(actual$fct)`:   "ordered" "factor"
`class(expected$fct)`:           "factor"

`levels(actual$fct)`:   "L"   "M"   "H"   
`levels(expected$fct)`: "low" "med" "high"

`actual$CHR` is absent
`expected$CHR` is a character vector ('B', 'C', 'D')
[ FAIL 1 | WARN 0 | SKIP 0 | PASS 0 ]
```


### Test supporting files

It's often useful to include helper functions or data to use for testing. Test data can be helpful for experimenting with various graph outputs and application performance. 

#### Test data 

Test data are  can be stored in `tests/testthat/fixtures/`. 

```{r}
#| eval: false 
#| code-fold: true 
#| code-summary: 'show/hide make-ggp2-movies.R'
## code to prepare `ggp2movies` dataset goes here
# pkgs <- c('ggplot2movies', 'tidyr', 'dplyr', 'stringr', 'purrr')
# install.packages(pkgs, quiet = TRUE)

# load packages --------------------
library(tidyr)
library(dplyr)
library(stringr)
library(purrr)

ggp2movies <- ggplot2movies::movies |>
  dplyr::mutate(id = 1:n()) |>
  tidyr::pivot_longer(
    cols = Action:Short,
    names_to = "Genre",
    values_to = "GenreMember"
  ) |>
  dplyr::group_by(
    dplyr::across(-c(Genre, GenreMember))) |>
  tidyr::nest() |>
  dplyr::mutate(
    Genres = purrr::map(data, ~ if (all(.x$GenreMember == 0)) {
      character(0)
    } else {
      .x$Genre[.x$GenreMember == 1]
    })
  ) |>
  dplyr::ungroup() |>
  dplyr::select(title, Genres, length, year, 
                budget, rating, votes, mpaa) |>
  dplyr:::mutate(
    genres = purrr::map_chr(
      .x = Genres,
      .f = stringr::str_c, collapse = ", "
    )
  ) |>
  dplyr::select(title, genres, length, year, 
                budget, avg_rating = rating, 
                votes, mpaa) |>
  dplyr::mutate(
    mpaa = dplyr::na_if(x = mpaa, y = ""),
    mpaa = factor(mpaa,
      levels = c("G", "PG", "PG-13", "R", "NC-17"),
      labels = c("G", "PG", "PG-13", "R", "NC-17")
    ),
    genres = dplyr::na_if(x = genres, ""),
    genres = factor(genres)
  ) |> 
  tidyr::drop_na()
# save to tests/testthat/fixtures/
saveRDS(object = ggp2movies, file = "tests/testthat/fixtures/ggp2_movies.rds")
```


#### Test helpers 

Test helpers are typically functions that make testing a little easier. I've included an example function includes messages for testing, `test_cmt()`.


```{r}
#| code-fold: false 
# test comment helper
test_cmt <- function(start = NULL, end = NULL, msg) {
  if (is.null(start) & is.null(end)) {
    cat("\n", glue::glue("[{Sys.time()}| {msg}]"), "\n")
  } else if (!is.null(start) & is.null(end)) {
    cat("\n", glue::glue("[ START | {Sys.time()} | {start} = {msg}]"), "\n")
  } else if (is.null(start) & !is.null(end)) {
    cat("\n", glue::glue("[ END   | {Sys.time()} | {end} = {msg}]"), "\n")
  } else {
    cat("\n", glue::glue("[ START | {Sys.time()} | {start} = {msg}]"), "\n")
    cat("\n", glue::glue("[ END   | {Sys.time()} | {end} = {msg}]"), "\n")
  }
}
```

```{r}
#| collapse: true
#| code-fold: false
test_cmt(start = "mod_fun()", msg = "messge")
# test contents
test_cmt(end = "mod_fun()", msg = "messge")
```

Functions like these can be stored in `tests/testthat/helper.R`:

```{verbatim}
#| eval: false 
#| code-fold: false
tests/
  └── testthat/
      ├── fixtures/
      │   ├── make-ggp2-movies.R
      │   └── ggp2_movies.rds
      ├── helper.R
      └── ... all test files...
```


### Snapshots

Writing tests for graph outputs can be difficult, because we're evaluating the output from a custom plotting function like `scatter_plot()`. If we try to compare the output from `scatter_plot()` against a graph built with analogous `ggplot2` code, this test will fail: 

```{r}
#| eval: false 
#| code-fold: false
test_that(desc = "scatter_plot() works", code = {
  
  ggp_graph <- ggplot2::ggplot(mtcars, 
                ggplot2::aes(x = mpg, y = disp)) + 
                ggplot2::geom_point(
                  ggplot2::aes(color = cyl), 
                               alpha = 0.5, 
                               size = 3)
  
  app_graph <- scatter_plot(mtcars, 
                    x_var = "mpg", 
                    y_var = "disp", 
                    col_var = "cyl", 
                    alpha_var = 0.5, 
                    size_var = 3)
  
  expect_equal(
    object = ggp_graph, 
    expected = app_graph)
})
```

We can see why with `diffobj::diffObj()`:

```{r}
#| eval: false 
#| code-fold: false
diffobj::diffObj(ggp_graph, app_graph)
```


:::: {.column-page-inset-right}

:::{#fig-08_tests_diffobj_scatter_plot}

![`diffobj::diffObj()` on graph outputs](img/08_tests_diffobj_scatter_plot.png){#fig-08_tests_diffobj_scatter_plot width='100%' align='center'}

Graph objects are difficult to use as test objects 
:::

::::

Another option for testing the `scatter_plot()` function is to use the `expect_snapshot_file()` function.[^expect-snapshot-file-2]

[^expect-snapshot-file-2]: Follow the example from the [`testthat` documentation](https://testthat.r-lib.org/reference/expect_snapshot_file.html#ref-examples)

#### [`vdiffr`]{style="font-size: 1.05em;"}

The [`vdiffr` package ](https://vdiffr.r-lib.org/) has it's own expectation, `expect_doppelganger()`, that has `title` and `fig` arguments:

```{r}
#| eval: false 
#| code-fold: false
test_that(desc = "scatter_plot() works", code = {
  
  test_cmt(start =  "scatter_plot()", 
           msg = "ggplot2movies::movies snapshot")
  
  ggp2_movies <- readRDS(test_path("fixtures", "ggp2_movies.rds"))
  
    vdiffr::expect_doppelganger(
      title = "scatter_plot() graph", 
      fig = scatter_plot(ggp2_movies, 
                    x_var = "avg_rating", 
                    y_var = "length", 
                    col_var = "mpaa", 
                    alpha_var = 0.4, 
                    size_var = 2.5))
    
    test_cmt(end =  "scatter_plot()", 
             msg = "ggplot2movies::movies snapshot")
    
})
```

On the initial run, a snapshot is saved into the `tests/testthat/_snaps/` folder:

```{verbatim}
#| eval: false 
#| code-fold: false
==> Testing R file using 'testthat'

ℹ Loading pkgApp
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 0 ]
 [ START | 2023-09-11 13:55:49 | scatter_plot() = ggplot2movies::movies snapshot] 
[ FAIL 0 | WARN 1 | SKIP 0 | PASS 1 ]
 [ END   | 2023-09-11 13:55:50 | scatter_plot() = ggplot2movies::movies snapshot] 


── Warning (test-scatter_plot.R:4:5): scatter_plot() works ─────────────────────
Adding new file snapshot: 'tests/testthat/_snaps/scatter-plot-graph.svg'
[ FAIL 0 | WARN 1 | SKIP 0 | PASS 1 ]

Test complete
```

```{bash}
#| eval: false 
#| code-fold: false
tests/
  ├── testthat
  │   ├── _snaps
  │   │   └── scatter_plot
  │   │       └── scatter-plot-graph.svg
  │   └── test-scatter_plot.R
  └── testthat.R

```

The `scatter-plot-graph.svg` file becomes our baseline comparison object, when is used in future tests. 

After writing tests, we'll load, document, and build:

[<kbd>Ctrl/Cmd</kbd> + <kbd>Shift</kbd> + <kbd>L</kbd> / <kbd>D</kbd> / <kbd>B</kbd>]{style="font-style: italic; font-weight: bold; font-size: 1.10em"}

Then we test `pkgApp`:

[<kbd>Ctrl/Cmd</kbd> + <kbd>Shift</kbd> + <kbd>T</kbd>]{style="font-style: italic; font-weight: bold; font-size: 1.10em"}

```{verbatim}
#| eval: false 
#| code-fold: false
devtools::test()

ℹ Testing pkgApp
✔ | F W S  OK | Context
✔ |         1 | scatter_plot  

══ Results ═══════════════════
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ]
```


## Module tests

Testing modules can be accomplished with `shiny`s `testServer()` function. Although typically stored in a single file, modules consist of a pair of UI and server functions. You might want to consider module server function tests 'integration' or 'interaction' testing, because we're testing how the module's server function 'integrates' or 'interacts' with input values, or how it will communicate with other modules. 

<!--
> Data Flow: Test how well the different units work together by simulating a complete data flow.

> Database Interactions: If your package interacts with a database, write tests to ensure that read/write operations are working as expected.

> API Calls: If your package makes API calls, use package like httptest to mock API responses and test the integration.
-->



### [`testServer()`]{style="font-size: 0.95em;"}

Whichever definition you choose, the goal with `testServer()` is to test *reactive interactions*. If we wanted to test the reactive interactions between the `mod_var_input_server()` and `mod_scatter_display_server()`, we could use the following: 

```{r}
#| eval: false 
#| code-fold: false
test_that("mod_scatter_display_server() works", {
  shiny::testServer(
    app = mod_scatter_display_server,
    args = list(
      var_inputs = shiny::reactive(
        list(
          x = "audience_score",
          y = "imdb_rating",
          z = "mpaa_rating",
          alpha = 0.5,
          size = 2,
          plot_title = "Test title")
        )
      ), expr = {
      expect_true(
        object = shiny::is.reactive(inputs)
      )
      test_cmt("mod_scatter_display_server", "is.reactive(inputs())")
    }
  )
})
```


### [`args = list()`]{style="font-size: 0.95em;"}


## Behavior-driven development (BDD)

> "*[BDD] encourages teams to use conversation and concrete examples to formalize a shared understanding of how the application should behave.*" - [BDD, Wikipedia](https://en.wikipedia.org/wiki/Behavior-driven_development#:~:text)

### [`describe()`]{style="font-size: 0.95em;"}

The `testthat::describe()` function follows a BDD format: 

### [`it()`]{style="font-size: 0.95em;"}

`testthat::it()`

## System/end-to-end tests

<!--
> Manual Testing: Initially, you may have to manually interact with the Shiny app to ensure it behaves as expected.

> Automated Browser Testing: Use tools like shinytest to automate browser-based testing for the Shiny application. shinytest captures the state of the application and allows you to compare it to expected states.

> User Scenarios: Script typical user interaction scenarios to ensure that the whole system, including front-end and back-end, work seamlessly.
-->


### [`shinytest2`]{style="font-size: 1.05em;"}

## Test coverage 

### [`covr`]{style="font-size: 1.05em;"}

### [`covrpage`]{style="font-size: 1.05em;"}

## Continuous Integration (CI)

<!--

> Consider setting up a CI pipeline using services like GitHub Actions, GitLab CI, or Travis CI to automate the running of these tests.

> By implementing these various levels of tests, you can build confidence that your R package and Shiny app are robust, reliable, and ready for deployment.

-->

end `testing.qmd`

<!--
Testing an R package with a Shiny application can be challenging due to the various components and layers of interactivity. Here's a general strategy to approach this:
-->