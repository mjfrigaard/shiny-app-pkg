# Tests {#sec-tests}

```{r}
#| eval: true 
#| echo: false 
#| include: false
source("_common.R")
library(testthat)
library(gt)
```

```{r}
#| label: co_box_dev
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "o", look = "minimal",
  header = "Caution",
  contents = "This section is still being developed--it's contents are subject to change.",
  fold = FALSE
)
```


This chapter covers an alternative approach to developing tests in your app-package. I'll introduce how to combine `testthat`'s behavior-driven development (BDD) functions and a traceability matrix to  ensure the user's needs are met (and the app's features are implemented correctly).

```{r}
#| label: co_box_tldr
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "b", look = "minimal",
  header = "TLDR", fold = TRUE,
  contents = "
  
##### Application Requirements
  
- **User Specifications**: Describe what the end-users expect the application to accomplish`\n
- **Feature Requirements**: User-focused descriptions of the high-level capabilities or characteristics of an application, and often represent a bundle of smaller functionalities\n
- **Functional Requirements**: The precise, measurable, and testable 'functions' an application should perform\n
- **Traceability Matrix**: Maps the user specifications to the functional requirements and tests, ensuring that all needs are met and functions are tested\n
  
##### Test suite
  
- `use_testthat()`: creates testing infrastructure in your app-package\n
  
- `use_test()`: create new test file (with `test-` prefix)\n
  
- **Fixtures**: ...\n
  
- **Helpers**: ...\n
  
- `test_active_file()`: runs tests in the current open file\n
  
##### Behavior-driven development
  
- `describe()`: ...`\n

- `it()`:...\n
  
#### Test coverage
  
- `covr`:
    
**Workflow:**...\n
  
  "
)
```

The code chunk below will load the necessary packages for testing.

```{r}
#| eval: false
#| code-fold: false
install.packages(c("testthat", "shinytest2", "covr"))
library(testthat)
library(shinytest2)
library(covr)
```

(*If you're using `devtools`, you won't have to worry about installing `testthat` and `covr`*)

## Testing app-packages

Testing shiny applications poses some unique challenges. Shiny functions are written in the context of its reactive model,[^tests-shiny-reactive] so some standard testing techniques and methods for regular R packages don't directly apply. Fortunately, every developer looking to test their code is faced with the same two questions:

[^tests-shiny-reactive]: The ['Reactivity - An overview'](https://shiny.posit.co/r/articles/build/reactivity-overview/) article gives an excellent description (and mental module) of reactive programming.

1. What should I test?  
2. How should I test it?   

We're going to focus on *what* to test. I'll give examples but won't go into depth on *how* to write tests because plenty of those resources exist.[^tests-intro-unit-tests] [^tests-intro-shinytest2] [^tests-intro-ms-testing] The tests in this chapter illustrate the connections between the user's needs, the code below `R/`, tests, and test coverage.[^testing-introduce-bdd] The only exception is some `testServer()` tricks I've learned for testing modules.[^tests-intro-testserver]

[^testing-introduce-bdd]: The BDD functions aren't covered in [R Packages, 2ed](https://r-pkgs.org/testing-basics.html) or the [Testing chapter](https://mastering-shiny.org/scaling-testing.html) of Mastering Shiny, but I've found them to be particularly useful for building app-packages.

[^tests-intro-unit-tests]: Unit tests are covered extensively in [R Packages, 2ed](https://r-pkgs.org/testing-basics.html) and the [`testthat` documentation](https://testthat.r-lib.org/index.html)

[^tests-intro-shinytest2]: `shinytest2` has [excellent documentation](https://rstudio.github.io/shinytest2/) (and [videos](https://www.youtube.com/watch?v=Gucwz865aqQ)), and I highly recommend reading through those resources.

[^tests-intro-ms-testing]: Mastering shiny dedicates an entire [Chapter to Testing](https://mastering-shiny.org/scaling-testing.html), which covers [unit tests](https://mastering-shiny.org/scaling-testing.html#basic-structure) and [`testServer()`](https://mastering-shiny.org/scaling-testing.html#testing-reactivity), and also includes some tips for using JavaScript with [`shinytest`](https://mastering-shiny.org/scaling-testing.html#testing-javascript) (not to be confused with [`shinytest2`](https://rstudio.github.io/shinytest2/)) 
[^tests-intro-testserver]: The [`testServer()`](https://shiny.posit.co/r/articles/improve/server-function-testing/) documentation is sparse, so I'll provide a few tips and tricks I've learned for testing module server functions.

### Where to look

Applications typically have some accompanying resources to address what *should* be tested, the most common source being a software requirements specification (SRS) document.[^tests-srs] The SRS breaks down an application's intended purpose (i.e., the problem it's designed to solve) into three general areas: user specifications, feature requirements, and functional requirements: 

-   **The user specifications capture the needs and expectations of the end-user.** These are usually non-technical and focused on the "why" and the "what" of the application.

-   **The feature requirements describe the high-level capabilities of the application.** Features are defined early in the life of a project and often become the talking points during discussions between stakeholders and developers. Features can be used for scoping and prioritization and may comprise various functional (and sometimes non-functional) requirements.

-   **Functional requirements are the testable, specific actions, inputs, and outputs.** Functional requirements provide the technical details of how the features will be implemented, and a single feature can give rise to multiple functional requirements.

-   A **traceability matrix** is a table that ‘traces’ the user specifications to features and functional requirements (and the tests they give rise to) to verify that the application has been developed correctly. 

[^tests-srs]: Read more about what goes in the [Software Requirements Specification](https://en.wikipedia.org/wiki/Software_requirements_specification)

These guidelines direct the development process, albeit from slightly different perspectives. However, understanding the relationship between user specifications, features, and functional requirements helps us build applications that satisfy the technical standards while addressing the user's needs.

The traceability matrix is a tool to ensure the tests cover all the functionalities and address the user specifications, *and* that every user need corresponds to a functionality that's been tested. 

I'll use `shinyAppPkg` to illustrate a brief example in the sections below. 

### User Specifications

**User specifications** are what the end-user (in this case, a film analyst) wants to achieve with the application. The language used for these descriptions is non-technical but should provide a basis for deriving the more technical (but still high-level) feature requirements. [^tests-specs-user-specs]

[^tests-specs-user-specs]: User Specifications are sometimes referred to as "user stories," "use cases," or "general requirements"

```{r}
#| label: co_box_user_specs
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "b", fold = FALSE, look = "minimal", 
  size = '1.10', header = "Scatter plot user specification",
  contents = 
" 

**US1**: '*As an film analyst, I want to view an interactive scatter plot in a dashboard that consolidates movie reviews from multiple sources so that I can compare and analyze trends and outliers in movie ratings quickly.*'
  
  ")
```

### Feature Requirements

**Feature requirements** (or just **features**) translate the user's expectations into language that describes the tasks a film analyst should be able to accomplish with the application (i.e., explore data with a scatter plot). However, features are still phrased to satisfy the user needs outlined in the specifications.

```{r}
#| label: co_box_feat_reqs
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "b", fold = FALSE, look = "minimal",
  size = '1.10', header = "Scatter plot feature requirements",
  contents = 
" 

**FE1**: '*Given that movie reviews are available on multiple websites, when the user selects a rating metric from the application display, the interactive scatter plot should allow for comparisons from at least two data sources (i.e., IMDb, Rotten Tomatoes), and include options for selecting other variables of interest (i.e., audience scores, runtime, etc.).*'
  
  ")
```

### Functional Requirements

**Functional requirements** are written for the developer and provide technical details on the feature (i.e., the scatter plot) and how the application should behave. A single feature will often give rise to multiple functional requirements (these are where the end-user's expectations come into direct contact with code).[^tests-specs-feat]

[^tests-specs-feat]: 'Feature requirements' and 'functional requirements' are sometimes used interchangeably, but they refer to different aspects of the application. **Feature requirements** are high-level capabilities an application *should* have, and often contain a collection of smaller functionalities (broken down into the specific **functional requirements**).

```{r}
#| label: co_box_fun_reqs
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "b", fold = FALSE, look = "minimal",
  size = '1.10', header = "Scatter plot functional requirements",
  contents = 
" 
  - **FR1**: The system will display movie reviews from IMDb and Rotten Tomatoes collected from their respective APIs.

  - **FR2**: The scatter plot will be displayed on the dashboard and updated with new user inputs.

  - **FR3**: Each data point on the scatter plot will represent a movie and be color-coded based on the following categories: MPAA ratings, genre, title type, critics rating, and audience rating.
  
  - **FR4**: The scatter plot will have labeled axes, a legend to differentiate between data sources, and a customizable title.
  
  ")
```


Documenting an application's requirements will follow this general pattern:

1. Understand the user's needs.  

2. Capture that need in a feature.  

3. Translate the feature into functions the application will require

If you've documented these three elements, you'll have a clearer picture of what tests to write.

### Traceability Matrix

After translating the user needs into functional requirements, we can quickly identify what needs to be tested by building a look-up table (i.e., a matrix). I like to store early drafts of the application requirements in a vignettes:

```{r}
#| eval: false 
#| code-fold: false
usethis::use_vignette("test-specs")
```

Adding our first vignette to the `vignettes/` folder does the following:

- [x]   Adds the `knitr` and `rmarkdown` packages to the `Suggests` field in `DESCRIPTION`[^test-specs-suggests]

    ```bash
    Suggests: 
        knitr,
        rmarkdown
    ```

- [x]   Adds `knitr` to the `VignetteBuilder` field[^test-specs-vignette-builder]

    ```bash
    VignetteBuilder: knitr
    ```

- [x]   Adds `inst/doc` to `.gitignore` and `*.html`, `*.R` to `vignettes/.gitignore`

[^test-specs-suggests]: We briefly covered the `Suggests` field in [Dependencies](dependencies.qmd), but in this case it specifically applies to "*packages that are not necessarily needed. This includes packages used only in examples, tests or vignettes...*" - [Writing R Extensions, Package Dependencies](https://cran.r-project.org/doc/manuals/R-exts.html#Package-Dependencies)

[^test-specs-vignette-builder]: The [documentation](https://cran.r-project.org/doc/manuals/R-exts.html#The-DESCRIPTION-file) on `VignetteBuilder` gives a great example of why `knitr` and `rmarkdown` belong in `Suggests` and not `Imports`.

The first column in the traceability matrix contains the user specifications, which we can 'trace' over to the functional requirements and their relevant  tests.[^tests-visual-markdown]

[^tests-visual-markdown]: When building tables in vignettes, I highly recommend using the [Visual Markdown mode](https://rstudio.github.io/visual-markdown-editing/).

```{r}
#| label: co_box_trace_matrix_tests
#| echo: false
#| code-fold: false
#| include: true
trace_matrix <- tibble::tibble(
  `User Specification` = c(
    "US1: scatter plot data visualization",
    NA_character_,
    NA_character_,
    NA_character_
  ),
  `Feature Requirement` = c(
    "FE1: interactive scatter plot (two data sources, drop-down variable options)",
    NA_character_,
    NA_character_,
    NA_character_
  ),
  `Functional Requirements` = c(
    "FR1: data source",
    "FR2: user-input updating",
    "FR3: color-coded data points",
    "FR4: plot axis, legend & title"
  ),
  Test = c(
  "?", "?", "?", "?"
  ),
) 
trace_matrix |> 
  gt::gt(auto_align = TRUE) |> 
  gt::sub_missing(
  columns = gt::everything(),
  rows = gt::everything(),
  missing_text = "-"
)
```

Building a traceability matrix ensures:  

1. All user specifications have accompanying application features. 

2. Each feature has been broken down into precise, measurable, and testable functional requirements.

3. Tests have been written for each functional requirement.


Documenting the traceability matrix in vignettes are great for developers, but it's also a good idea use an issue-tracking system with version control, like GitHub Projects or Azure DevOps.

The following section covers the `testthat` infrastructure and basic unit tests.

## [`testthat`]{style="font-size: 1.05em;"} framework

```{r}
#| label: git_box_08_tests-specs
#| echo: false
#| results: asis
#| eval: true
git_margin_box(
  fig_pw = '65%', 
  branch = "10a_tests-specs", 
  repo = 'shinyAppPkg')
```

The `testthat` package has been around for over a decade and thus has undergone various changes that require us to specify the edition we intend to use (currently, it's the third).[^tests-testthat-edition]

[^tests-testthat-edition]: Read more about changes to the third edition to `testthat` in [R Packages, 2ed](https://r-pkgs.org/testing-basics.html#introducing-testthat)

In the following sections, I'll introduce methods that align our tests with current best practices or principles,[^tests-testthat-best-practices] but know that multiple strategies exist for writing tests. For example, if you've adopted test-driven development (TDD),[^tests-tdd] you'll write tests before developing utility functions, modules, or a standalone app function.

Regardless of your testing strategy, the infrastructure for storing and running tests in app-packages is identical to a standard R package.

[^tests-testthat-best-practices]: Most of these are described in the [High-level principles for testing](https://r-pkgs.org/testing-design.html#sec-testing-design-principles) section of [R Packages, 2ed](https://r-pkgs.org/).

[^tests-tdd]: Read more about [Test-driven development](https://en.wikipedia.org/wiki/Test-driven_development) 

### [Setup with `use_testthat()`]{style="font-size: 0.95em;"}

Setting up your testing infrastructure with `usethis::use_testthat(3)` does the following (`3` is the edition):

- [x]   In the `DESCRIPTION` file, `testthat (>= 3.0.0)` is listed under `Suggests`

- [x]   `Config/testthat/edition: 3` is also listed in the  `DESCRIPTION` to specify the `testthat` edition
    
- [x]   A new `tests/` folder is created, with a `testthat/` subfolder

- [x]   The `tests/testthat/testthat.R` file is created 

We now have a `tests/` folder to store our `testthat` tests.

```{bash}
#| eval: false
#| code-fold: false
tests/
  ├── testthat/
  └── testthat.R #<1>

2 directories, 1 file
```
1. Referred to as the 'test runner,' because it runs all our tests (do not edit this file).

#### Creating unit tests

The standard workflow for writing `testthat` unit tests consists of the following:

- [x]   New tests are created with `usethis::use_test()`.

```{r}
#| eval: false
#| code-fold: false
usethis::use_test("scatter_plot") # <1>
```
1. In standard R packages, there is a file named for every function in the `R/` folder, and a corresponding test file (with the `test-` prefix) in the `tests/testthat/` folder

##### [`test-`]{style="font-size: 0.95em;"} files

- [x]   **Test files**: the IDE will automatically create and open the new test file: 

```{verbatim}
#| eval: false
#| code-fold: false
✔ Writing 'tests/testthat/test-scatter_plot.R'
• Modify 'tests/testthat/test-scatter_plot.R'
```

##### [`test_that()`]{style="font-size: 0.95em;"} tests 

- [x]   **Tests**: Each new test file contains a boilerplate `test_that()` test 

```{r}
#| eval: false
#| code-fold: false 
test_that(desc = "multiplication works", code = { # <1>
 
})
```
1. `desc` is the test context (supplied in `"quotes"`), and `code` is the test code (supplied in `{curly brackets}`).

##### [`expect_`]{style="font-size: 0.95em;"}ations 

- [x]   **Expectations**: most expectation have two parts: an `observed` object, and an `expected` object. The `observed` object is an artifact of some code you've written, and it's being compared against an `expected` result.

```{r}
#| eval: false
#| code-fold: false 
#| collapse: true
expect_equal( # <1> 
  object = 2 * 2, # <2> 
  expected = 4 # <3> 
  ) 
```
1. A `testthat` expectation function  
2. The output or behavior being tested  
3. A predefined output or behavior    

##### Running tests

- [x]   **Running tests**: Another `devtools` habit to adopt is regularly writing and running tests. If you're using Posit Workbench and have `devtools` installed, you can test your app-package using the **Build** pane or the keyboard shortcut.

:::{.column-margin}

![Run all tests](img/10_tests_build_pane_test.png){width='100%'}

:::

R Packages, 2ed also [suggests](https://r-pkgs.org/testing-basics.html#run-tests) binding `test_active_file()` and `test_coverage_active_file()` to keyboard shortcuts. I **highly** recommend using a shortcut while developing tests because it will improve your ability to iterate quickly.

::: {layout="[39, -1, 60]" layout-valign="bottom"}

#### Shortcut

[<kbd>Ctrl/Cmd</kbd> + <kbd>Shift</kbd> + <kbd>T</kbd>]{style="font-weight: bold; font-style: italic; font-size: 0.80em"}

#### Function

[`devtools::test()`]{style="font-weight: bold; font-size: 0.90em"}

:::

::: {layout="[39, -1, 60]" layout-valign="bottom"}

[<kbd>Ctrl/Cmd</kbd> + <kbd>T</kbd>]{style="font-weight: bold; font-style: italic; font-size: 0.80em"} 

[`devtools::test_active_file()`]{style="font-weight: bold; font-size: 0.90em"}

:::

::: {layout="[39, -1, 60]" layout-valign="bottom"}

[<kbd>Ctrl/Cmd</kbd> + <kbd>Shift</kbd> + <kbd>R</kbd>]{style="font-weight: bold; font-style: italic; font-size: 0.80em"} 

[`devtools::test_coverage_active_file()`]{style="font-weight: bold; font-size: 0.90em"}

:::


When the test is run, you'll see feedback on whether it passes or fails (and occasionally some encouragement):

```{r}
#| eval: true
#| code-fold: false
#| echo: true 
#| collapse: true
test_that("multiplication works", { 
  expect_equal( 
    object = 2 * 2, 
    expected = 4 
    ) 
})
```

The next section covers two important principles to keep in mind while writing tests for you app-package: 

1. Make sure to clean up any 'leftover' states between test executions  
2. Keep test scope logic defined to a single test (or all tests)

We'll introduce tools to ensure tests follow these principles--fixtures, helpers, and mocks--and introduce `testthat`'s `describe()` and `it()` functions.

[^tests-self-sufficient]: For more on this topic, consult the [Self-sufficient tests](https://r-pkgs.org/testing-design.html#self-sufficient-tests) section in [R Packages, 2ed](https://r-pkgs.org/)

## Behavior-driven development

Behavior-driven development (BDD) (or behavior-driven testing) emphasizes writing human-readable descriptions of the application's behavior, which are then converted into a series of tests.

> "*[BDD] encourages teams to use conversation and concrete examples to formalize a shared understanding of how the application should behave.*" - Behavior-driven development, Wikipedia[^tests-bdd-read-more]

We can apply BDD is easier by combining the contents of our traceability matrix with `testthat`'s `describe()` and `it()` functions.

[^tests-bdd-read-more]: Read more about [behavior-driven development](https://en.wikipedia.org/wiki/Behavior-driven_development)

### [`describe()`]{style="font-size: 0.95em;"} a feature

The `testthat::describe()` function follows a BDD format and '*specifies a larger component or function and contains a set of specifications*'

In `describe()`, we can use the language from our traceability matrix to reference the feature I'm testing in the `description` argument (**FE1**):

```{r}
#| eval: false 
#| code-fold: false
testthat::describe(
  description = "FE1: interactive scatter plot (two data sources, drop-down variable options",
  code = {
  
})
```

We can nest `describe()` functions, which means we could also include the user specification that precedes the feature in the traceability matrix: 

```{r}
#| eval: false 
#| code-fold: false
testthat::describe("US1: scatter plot data visualization", # <1>
  code = { 
  
  testthat::describe("FE1: interactive scatter plot (two data sources, drop-down variable options)", # <2>
    code = { # <2>
    # <2>
  }) # <2>
  
}) # <1>
```
1. User specification     
2. Feature requirement   

### Confirm [`it()`]{style="font-size: 0.95em;"} with a test

Inside `describe()`, we can include multiple `it()` blocks which "*functions as a test and is evaluated in its own environment.*" 

In the example below, I use an `it()` block to test the first functional requirement:[^tests-it-blocks]

```{r}
#| eval: false 
#| code-fold: false
  testthat::describe("FR1: data source", # <1>
    code = { 
    
    testthat::it("T1: data source", # <2>
      code = { # <3>
      # test code # <3>
    }) # <2>
    
  }) # <1>
```
1. Functional requirement   
2. Test scope  
3. Test code  

[^tests-it-blocks]: Each [`it()`](https://testthat.r-lib.org/reference/describe.html) block contains the expectations (or what you would traditionally include in `test_that()`).[^tests-bdd-describe] 

[^tests-bdd-describe]: Read more about `describe()` and `it()` in the [`testthat` documentation.](https://testthat.r-lib.org/reference/describe.html)

Preceding each `it()` call with `describe()` has the added benefit of removing any code that exists outside of our tests:

> "*Eliminating (or at least minimizing) top-level code outside of `test_that()` will have the beneficial effect of making your tests more hermetic. This is basically the testing analogue of the general programming advice that it's wise to avoid unstructured sharing of state.*"

If we've created a traceability matrix, nothing is stopping us from using `describe()` and `it()` to scope all the functional requirements: 

```{r}
#| eval: false 
#| code-fold: false
testthat::describe("US1: scatter plot data visualization", # <1>
  code = {
    testthat::describe("FE1: interactive scatter plot (two data sources, drop-down variable options)", # <2>
      code = {
        
      testthat::describe("FR1: data source", code = { # <3>
        testthat::it("T1: data source", # <4>
          code = { # <5>
            # test code # <5>
          }) # <4>
      }) # <3>

      testthat::describe("FR2: user-input updating", code = { # <6>
        testthat::it("T2: user-input updating", code = { # <7>
          # test code # <8>
        }) # <7>
      }) # <6>

      testthat::describe("FR3: color-coded data points", code = { # <9>
        testthat::it("T3: color-coded data points", code = { # <10>
          # test code # <11>
        }) # <10>
      }) # <9>

      testthat::describe("FR4: plot axis, legend & title", code = { # <12>
        testthat::it("T4: plot axis, legend & title", code = { # <13>
          # test code # <14>
        }) # <13>
      }) # <12>
        
    }) # <2>
    
}) # <1>
```
1. User specification scope (**US1**)  
2. Feature scope (**FE1**)  
3. Functional requirement scope (**FR1**)  
4. Test scope (**T1**)   
5. Test code (**T1**)     
6. Functional requirement scope (**FR2**)   
7. Test scope (**T2**)  
8. Test code (**T2**)  
9. Functional requirement scope (**FR3**)  
10. Test scope (**T3**)  
11. Test code (**T3**)  
12. Functional requirement scope (**FR4**)  
13. Test scope (**T4**)  
14. Test code (**T4**) 


Now we can add a **Test** column to the matrix and reference the tests from the `it()` calls:

```{r}
#| label: co_box_bdd_trace_all
#| echo: false
#| code-fold: false
#| include: true
trace_matrix <- tibble::tibble(
  `User Specification` = c(
    "US1: scatter plot data visualization",
    NA_character_,
    NA_character_,
    NA_character_
  ),
  `Feature Requirement` = c(
    "FE1: interactive scatter plot (two data sources, drop-down variable options)",
    NA_character_,
    NA_character_,
    NA_character_
  ),
  `Functional Requirements` = c(
    "FR1: data source",
    "FR2: user-input updating",
    "FR3: color-coded data points",
    "FR4: plot axis, legend & title"
  ),
  Test = c(
    "T1", "T2", "T3", "T4"
  ),
) 
trace_matrix |> 
  gt::gt(auto_align = TRUE) |> 
  gt::sub_missing(
  columns = gt::everything(),
  rows = gt::everything(),
  missing_text = "-"
)
```

`testthat`'s BDD functions provide context and reduce the need to place any code outside of our tests (or in this case, outside of `it()`):

In the following sections we're going to cover various tools to improve the tests in your app-package. The overarching goal of these tools is to remove any additional code executed *outside* of your tests (i.e., placed above the call to `test_that()`).[^tests-self-sufficient]

## Fixtures 

<!-- https://www.tidyverse.org/blog/2020/04/self-cleaning-test-fixtures/ -->

Test fixtures are resources (data, file paths, functions, etc.) used to create repeatable test conditions (even when the test is run in different environments).

Good test fixtures provide a consistent, well-defined test environment, and then are removed/destroyed when the test is run. This ensures any changes made during the test don't persist or interfere with future tests.[^tests-self-cleaning]

[^tests-self-cleaning]: For a concrete example, see [this article](https://www.tidyverse.org/blog/2020/04/self-cleaning-test-fixtures/) on self-cleaning tests.

In R packages, test fixtures are stored in the `tests/testthat/fixtures/` folder:

```{bash}
#| eval: false 
#| code-fold: false
tests/
├── testthat/
│   └── fixtures/                                          # <1>
└── testthat.R
```
1. The name '`fixtures`' isn't required (you can name this folder anything)

### Test data

Large static data files are an example of a test fixture.[^tests-fixtures-static] Any code used to create test data should be stored with the output file (using a clear naming convention).

[^tests-fixtures-static]: Creating a tidied version of `ggplot2movies::movies` would be costly to re-create with every test, so it's advised to store it as an [static test fixture.](https://r-pkgs.org/testing-advanced.html#sec-testing-advanced-concrete-fixture)

For example, I've stored the code used to create a ['tidy' version](https://github.com/mjfrigaard/shinyAppPkg/blob/10b_tests-helpers-fixtures/tests/testthat/fixtures/make-ggp2_movies.R) of  the `ggplot2movies::movies` data along with the output dataset in `tests/testthat/fixtures/`:

```{bash}
#| eval: false 
#| code-fold: false 
tests
├── testthat
│   ├── fixtures
│   │   ├── make_tidy_ggp2_movies.R # <1>
│   │   └── tidy_ggp2_movies.rds # <2>
│   └── test-<name>.R
└── testthat.R

3 directories, 4 files
```
1. The code used to create the test data (`make-make_tidy_ggp2_movies.R`) 
2. The test data file (i.e., `tidy_ggp2_movies.rds`):

Data files stored in `tests/testthat/fixtures/` can be accessed with `testthat::test_path()` inside each test. 

### Example: test fixture

Below is a test that answers the question, '*does the plot generate without producing an error,*' for `scatter_plot()`. This type of test appropriate because we want to confirm the data source (`movies`) will generate a plot object when passed to the `scatter_plot()` utility function, not necessarily the specific contents of the graph.[^tests-graphs]

[^tests-graphs]: Snapshot tests would be more appropriate for answering the question, ['*is the plot visually correct?*'](https://shiny.posit.co/r/articles/improve/server-function-testing/index.html#complex-outputs-plots-htmlwidgets).

```{r}
#| eval: false 
#| code-fold: false
testthat::describe("FR1: data source", code = { # <1>
  
  testthat::it("T1 movies data source", code = { # <2>
    # inputs
    scatter_inputs <- list(                # <3>
      y = "audience_score",
      x = "imdb_rating",
      z = "mpaa_rating",
      alpha = 0.5,
      size = 2,
      plot_title = "Enter plot title"
    )                                  # <3>
    app_graph <- scatter_plot(movies,  # <4>
      x_var = scatter_inputs$x,
      y_var = scatter_inputs$y,
      col_var = scatter_inputs$z,
      alpha_var = scatter_inputs$alpha,
      size_var = scatter_inputs$size
    ) # <4>
    expect_true(ggplot2::is.ggplot(app_graph)) # <5> 
  }) # <2>

}) # <1>
```
1. Functional requirement     
2. Test code     
3. Test inputs    
4. Create observed object  
5. Expectation  

#### Developing tests  

While developing, using keyboard shortcuts makes it easier to iterate between building fixtures, writing and running tests, and checking code coverage. 

```{r}
#| label: hot_key_tf_01
#| echo: false
#| results: asis
#| eval: true
hot_key(fun = 'tf')
```

```{verbatim}
#| eval: false 
#| code-fold: false
devtools:::test_active_file()
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ]
```

```{r}
#| label: hot_key_cf_01
#| echo: false
#| results: asis
#| eval: true
hot_key(fun = 'cf')
```

```{r}
#| eval: false 
#| code-fold: false
devtools:::test_coverage_active_file()
```

![Test coverage on active file](img/10_tests_coverage_scatter_plot.png){width='100%' fig-align='center'}

It's gratifying to see 100% test coverage, but in this case it's misleading. The feature (**FE1**) states we need to include a second data source (per the traceability matrix).

Fortunately, including a second data source only requires adding a nested `describe()` function: 

1. The feature (**FE1**) scope adds more context to the functional requirement (**FR1**)   

2. A second test (**T2**) is added for the tidy `ggplot2movies::movies` data source:[^tests-ggp2_movies-raw-data]

[^tests-ggp2_movies-raw-data]: If the data in `tests/testthat/fixtures/` are going to be used repeatedly, it might also make sense to store it in `inst/extdata/` or `data-raw/`.

```{r}
#| eval: false 
#| code-fold: false
testthat::describe("FE1: interactive scatter plot (two data sources, drop-down variable options)",                                    # <1> 
  code = {                                             # <1> 
    
    testthat::describe("FR1: data source", code = {    # <2> 
      
      testthat::it("T1 movies data source", code = {   # <3> 
        scatter_inputs <- list(                        # <4> 
          y = "audience_score",
          x = "imdb_rating",
          z = "mpaa_rating",
          alpha = 0.5,
          size = 2,
          plot_title = "Enter plot title")             # <4> 
        app_graph <- scatter_plot(movies,              # <5> 
          x_var = scatter_inputs$x,
          y_var = scatter_inputs$y,
          col_var = scatter_inputs$z,
          alpha_var = scatter_inputs$alpha,
          size_var = scatter_inputs$size)              # <5> 
        expect_true(ggplot2::is.ggplot(app_graph))     # <6> 
      })                                               # <3> 

      testthat::it("T2 tidy movies data source", code = {   # <7>   
        ggp2_scatter_inputs <- list(                        # <8> 
          x = "avg_rating",
          y = "length",
          z = "mpaa",
          alpha = 0.75,
          size = 3,
          plot_title = "Enter plot title"
        )                                                       # <8> 
        tidy_ggp2_movies <- readRDS(test_path("fixtures",       # <9>
                                      "tidy_ggp2_movies.rds"))  # <9>
        app_graph <- scatter_plot(tidy_ggp2_movies,             # <10>
          x_var = ggp2_scatter_inputs$x,
          y_var = ggp2_scatter_inputs$y,
          col_var = ggp2_scatter_inputs$z,
          alpha_var = ggp2_scatter_inputs$alpha,
          size_var = ggp2_scatter_inputs$size)                 # <10>
        expect_true(object = ggplot2::is.ggplot(app_graph))    # <11>
        
      })                                                       # <7>   
      
    }) # <2> 
  }) # <1> 
```
1. Feature scope       
2. Functional requirement scope  
3. `movies` data test   
4. `movies` inputs  
5. `movies` observed plot object  
6. `movies` expectation   
7. `tidy_ggp2_movies` data test 
8. `tidy_ggp2_movies` data inputs    
9. Load test data fixture   
10. `tidy_ggp2_movies` observed plot object  
11. `tidy_ggp2_movies` expectation   

```{r}
#| label: git_box_10b_tests-helpers-fixtures
#| echo: false
#| results: asis
#| eval: true
git_margin_box(
  fig_pw = '65%', 
  branch = "10b_tests-fixtures", 
  repo = 'shinyAppPkg')
```

Test fixtures are described in-depth in [R Packages, 2ed](https://r-pkgs.org/testing-advanced.html#test-fixtures) and in the [`testthat` documentation](https://testthat.r-lib.org/articles/test-fixtures.html#test-fixtures).


## Helpers 

<!-- https://r-pkgs.org/testing-design.html#testthat-helper-files -->

> "*Helper files are a mighty weapon in the battle to eliminate code floating around at the top-level of test files.*" Testthat helper files, [R Packages, 2ed](https://r-pkgs.org/testing-design.html#testthat-helper-files)

Test helpers reduce repeated/duplicated test code. In general, objects or values that aren't large enough to justify storing as static test fixtures can be created with helper functions. Helper functions are stored in `tests/testthat/helper.R`, which is automatically loaded with `devtools::load_all()`:

```{bash}
#| eval: false 
#| code-fold: false
tests/
  ├── testthat/
  │   ├── fixtures/                                         # <1>
  │   │   ├── make-ggp2-movies.R
  │   │   └── ggp2_movies.rds
  │   ├── helper.R                                          # <2>
  │   └── test-<name>.R                                     # <3>
  └── testthat.R
```
1. Test fixture scripts and `.rds` files  
2. Helper functions  
3. Test file  

Test helpers should only be created if they make testing easier **when the tests fail.** The article, ['Why Good Developers Write Bad Unit Tests'](https://mtlynch.io/good-developers-bad-tests/), provides great advice on complexity vs. clarity when writing unit tests,
  
> *'think about what will make the problem obvious when a test fails. Refactoring may reduce duplication, but it also increases complexity and potentially obscures information when things break.'*
  
R programmers resist copy + paste programming, and in most cases this makes sense. After all, R *is* a functional programming language, so it's tempting to bundle any repeated code into a function and store it in the `tests/testthat/helper.R` file. 

However, when we're writing tests, it's more important that tests are easy to read and understand **when they fail**. 

For example, consider the inputs passed to the `scatter_plot()` function in the previous test:

```{r}
#| eval: false 
#| code-fold: false 
scatter_inputs <- list(
  y = "audience_score",
  x = "imdb_rating",
  z = "mpaa_rating",
  alpha = 0.5,
  size = 2,
  plot_title = "Enter plot title")
```

We could write a `var_inputs()` function that stores these values in a list:

```{r}
#| eval: true 
#| code-fold: false 
#| collapse: true
var_inputs <- function() {
  list(y = 'audience_score', 
     x = 'imdb_rating',
     z = 'mpaa_rating',
     alpha = 0.5,
     size = 2,
     plot_title = 'Enter plot title'
    )
}
var_inputs()
```

In our tests, this would allow us to use `var_inputs()` with the same 'reactive syntax' we use with `scatter_plot()` in the module server function: 

```{r}
#| eval: false 
#| code-fold: false 
#| collapse: true
testthat::it("T1 movies data source", code = {
  
app_graph <- scatter_plot(
  movies,
  x_var = var_inputs()$x, # <1>
  y_var = var_inputs()$y,
  col_var = var_inputs()$z,
  alpha_var = var_inputs()$alpha,
  size_var = var_inputs()$size) # <1>

testthat::expect_true(ggplot2::is.ggplot(app_graph))
})
## Test passed
```
1. This is how we refer to the inputs in `mod_scatter_display_server()` 

While this removes duplicated code, it also makes it less clear for the reader where `var_inputs()` are created (without opening the `helper.R` file or remembering we created this function).

```{r}
#| label: co_box_dry
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "r", 
  fold = FALSE,
  look = 'simple',
  size = "1.05", 
  hsize = "1.15",
  header = "Violating the DRY principle",
  contents = "
If you have repeated code in your tests, consider the following questions below before creating a helper function: 
  
1. Does the code help explain what behavior is being tested? 
  
2. Would a helper make it harder to debug the test when it fails?  
  
It’s more important that test code is obvious than DRY, because it’s more likely you’ll be dealing with this test when it fails (and you don’t remember why all the top-level code is there).
  
"
)
```

In contrast, the `make_var_inputs()` function below creates inputs for the `scatter_plot()` utility function:

```{r}
#| eval: false 
#| code-fold: false 
make_var_inputs <- function() {
  glue::glue_collapse("list(y = 'audience_score', 
     x = 'imdb_rating',
     z = 'mpaa_rating',
     alpha = 0.5,
     size = 2,
     plot_title = 'Enter plot title'
    )")
}
```

I can call `make_var_inputs()` in the **Console** and it will return the list of values to paste into each test:

```{r}
#| eval: false 
#| code-fold: false 
make_var_inputs()
list(y = 'audience_score', 
     x = 'imdb_rating',
     z = 'mpaa_rating',
     alpha = 0.5,
     size = 2,
     plot_title = 'Enter plot title'
    )
```

This reduces the amount of typing required while developing tests, but doesn't obscure the source of the values in the test.

`glue::glue_collapse()` is your friend when you want to quickly reproduce code for your tests. `make_ggp2_inputs()` creates the list of inputs for testing the tidy `ggplot2movies::movies` data:

```{r}
#| eval: true 
#| code-fold: false 
make_ggp2_inputs <- function() {
glue::glue_collapse("list(x = 'avg_rating',
     y = 'length',
     z = 'mpaa',
     alpha = 0.75,
     size = 3,
     plot_title = 'Enter plot title'
     )"
  )
}
```

#### Logging

I prefer test outputs to be verbose, so I usually create a `test_logger()` helper function that allows me to give more context and information with each test:

```{r}
#| code-fold: false
#| eval: true
# test logger helper
test_logger <- function(start = NULL, end = NULL, msg) {
  if (is.null(start) & is.null(end)) {
    cat("\n")
    logger::log_info("{msg}")
  } else if (!is.null(start) & is.null(end)) {
    cat("\n")
    logger::log_info("\n[ START {start} = {msg}]")
  } else if (is.null(start) & !is.null(end)) {
    cat("\n")
    logger::log_info("\n[ END {end} = {msg}]")
  } else {
    cat("\n")
    logger::log_info("\n[ START {start} = {msg}]")
    cat("\n")
    logger::log_info("\n[ END {end} = {msg}]")
  }
}
```

`test_logger()` can be used to 'log' the `start` and `end` of each test, and it includes a message argument (`msg`) I'll use to reference the test `description` argument in each `it()` call.[^tests-logger]

I tend to use functions like `test_logger()` enough to justify placing them in a testing utility file ([`R/testthat.R`](https://github.com/mjfrigaard/shinyAppPkg/blob/10c_tests-helpers/R/testthat.R)) below `R/`. Including testing functions in the `R/` folder also ensures it's documented and it's dependencies become part of your app-package).[^tests-r-files]

[^tests-logger]: If you like verbose logging outputs, check out the [`logger` package](https://daroczig.github.io/logger/) 

[^tests-r-files]: Placing common files for testing below `R/` is covered in [R Packages, 2ed](https://r-pkgs.org/testing-design.html#hiding-in-plain-sight-files-below-r)

#### Example: [`vdiffr`]{style="font-size: 0.95em;"} snapshots

Writing tests for graph outputs can be difficult because the "correctness" of a graph is somewhat subjective and requires human judgment. If the expected output we're interesting in testing is cumbersome to describe programmatically, we can consider using a snapshot tests. Examples of this include UI elements (which are mostly HTML created by Shiny's UI layout and input/output functions) and data visualizations.[^tests-ui-tests]

[^tests-ui-tests]: Mastering Shiny covers [creatng a snapshot file](https://mastering-shiny.org/scaling-testing.html#user-interface-functions) to test UI elements, but also notes this is probably not the best approach.

If we want to create a graph snapshot test, the [`vdiffr`](https://vdiffr.r-lib.org/) package allows us to perform a 'visual unit test' by capturing the expected output as an `.svg` file that we can compare with future versions.

The `expect_doppelganger()` function from `vdiffr` is designed specifically to work with ['graphical plots'](https://vdiffr.r-lib.org/reference/expect_doppelganger.html). 

```{r}
#| eval: false 
#| code-fold: false
vdiffr::expect_doppelganger(
      title = "name of graph", 
      fig = # ...code to create graph...
  )
```

Another option for using snapshots for testing is the `expect_snapshot_file()` function [^tests-10] but `expect_doppelganger()` is probably the better option for comparing graph outputs.

[^tests-10]: Follow the `expect_snapshot_file()` example from the [`testthat` documentation](https://testthat.r-lib.org/reference/expect_snapshot_file.html#ref-examples)

In this test, I'll use `describe()` to combine two features from the traceability matrix (**FE3** and **FE4**), followed by two `it()` statements (with each functional requirement the snapshot will capture).

```{r}
#| label: co_box_trace_matrix_feat_funct_snaps
#| echo: false
#| code-fold: false
#| include: true
trace_matrix <- tibble::tibble(
  `Feature Requirement` = c(
    "FE1: interactive scatter plot (two data sources, drop-down variable options)",
    NA_character_
  ),
  `Functional Requirements` = c(
    "FR3: color-coded data points",
    "FR4: plot axis, legend & title"
  )
) 
trace_matrix |> 
  gt::gt(auto_align = TRUE) |> 
  gt::sub_missing(
  columns = gt::everything(),
  rows = gt::everything(),
  missing_text = "-"
)
```

This method is helpful if we're trying to keep a 1:1 between the `test/testthat/` file names and file names in `R/`.


```{r}
#| eval: false 
#| code-fold: false
testthat::describe("FR3: color-coded data points & FR4: plot axis, legend & title", # <1>
  code = { 
  
  testthat::it("T3: color-coded data points", code = { # <2>

    test_logger(start = "T3", msg = "Tests FR3 (color)") # <3>
    
    scatter_inputs <- list(y = 'audience_score', # <4>
                       x = 'imdb_rating',
                       z = 'mpaa_rating',
                       alpha = 0.5,
                       size = 2,
                       plot_title = 'Enter plot title') # <4>
    
    vdiffr::expect_doppelganger( # <5>
      title = "FR3: color-coded data points", 
      fig = scatter_plot(movies, 
        x_var = scatter_inputs$x, 
        y_var = scatter_inputs$y, 
        col_var = scatter_inputs$z, 
        alpha_var = scatter_inputs$alpha, 
        size_var = scatter_inputs$size 
      )) # <5>
        
    test_logger(end = "T3", msg = "Tests FR3 (color)")          # <6>
    
  }) # <2>
    
  testthat::it("T4: plot axis, legend & title", code = { # <7>

    test_logger(start = "T4", msg = "Tests FR4 (legend/theme)") # <8>
                  
    scatter_inputs <- list(y = 'audience_score', # <9>
                       x = 'imdb_rating',
                       z = 'mpaa_rating',
                       alpha = 0.5,
                       size = 2,
                       plot_title = 'Enter plot title') # <9>
    
    vdiffr::expect_doppelganger( # <10>
      title = "FR4: plot axis, legend & title", 
      fig = scatter_plot(movies, 
        x_var = scatter_inputs$x, 
        y_var = scatter_inputs$y, 
        col_var = scatter_inputs$z, 
        alpha_var = scatter_inputs$alpha, 
        size_var = scatter_inputs$size 
      ) + 
        ggplot2::labs( 
          title = variable$plot_title, 
          x = stringr::str_replace_all( 
                tools::toTitleCase( 
                  scatter_inputs$x), "_", " "), 
          y = stringr::str_replace_all( 
                tools::toTitleCase( 
                  scatter_inputs$y), "_", " ") 
        ) + 
        ggplot2::theme_minimal() + 
        ggplot2::theme(legend.position = "bottom") 
    ) # <10>
    
    test_logger(end = "T4", msg = "Tests FR4 (legend/theme)")      # <11>
    
  }) # <7>
  
  
}) # <1>
```
1. Feature(s) scope    
2. Test scope (**T3**)    
3. Log start (**T3**)   
4. `movies` variable inputs  
5. Snapshot with for colored points   
6. Log end (**T3**)   
7. Test scope (**T4**)    
8. Log start (**T4**)   
9. `movies` variable inputs 
10. Snapshot for labels and theme  
11. Log end (**T4**)   



In this test, I've separated the `ggplot2` layers into different `it()` sections, and the test results return the output from `test_logger()` for more context of what's being tested:

```{verbatim}
#| eval: false 
#| code-fold: false
INFO [2023-10-06 11:41:21] [ START T3 = Tests FR3 (color)]
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 3 ]
INFO [2023-10-06 11:41:21] [ END T3 = Tests FR3 (color)]

INFO [2023-10-06 11:41:21] [ START T4 = Tests FR4 (legend/theme)]
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 4 ]
INFO [2023-10-06 11:41:21] [ END T4 = Tests FR4 (legend/theme)]
```

We also see a warning when the snapshot has been saved in the `tests/testthat/_snaps/` folder the first time the test is run:

```{verbatim}
#| eval: false 
#| code-fold: false
── Warning (test-scatter_plot.R:19:5): T3: color-coded data points ──
Adding new file snapshot:
'tests/testthat/_snaps/fr3-color-coded-data-points.svg'

── Warning (test-scatter_plot.R:51:5): T4: plot axis, legend & title ──
Adding new file snapshot:
'tests/testthat/_snaps/fr4-plot-axis-legend-title.svg'
[ FAIL 0 | WARN 2 | SKIP 0 | PASS 2 ]
```

```{r}
#| label: co_box_expect_doppelganger
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "g", fold = TRUE, 
  size = "1.05",
  header = "Reviewing snapshots",
  contents = "
Placing the functional requirement in the `title` argument of `expect_doppelganger()` gives us a clear idea of what the snapshot file *should* contain.
"
)
```

On subsequent runs, this warning will disappear (as long as there are no changes to the `.svg` files).

We should also update the traceability matrix with the tests we've used to verify the functional requirements:

```{r}
#| label: co_box_trace_matrix_add_t1_and_t2
#| echo: false
#| code-fold: false
tibble::tibble(
  `User Specification` = c(
    "US1: scatter plot data visualization",
    NA_character_,
    NA_character_,
    NA_character_
  ),
  `Feature Requirement` = c(
    "FE1: interactive scatter plot (two data sources, drop-down variable options)",
    NA_character_,
    NA_character_,
    NA_character_
  ),
  `Functional Requirements` = c(
    "FR1: data source",
    "FR2: user-input updating",
    "FR3: color-coded data points",
    "FR4: plot axis and legend"
  ),
  Tests = c(
    "T1 & T2",
    NA_character_,
    "T3",
    "T4"
  )
) |> 
  gt::gt(auto_align = TRUE) |> 
  gt::sub_missing(
  columns = gt::everything(),
  rows = gt::everything(),
  missing_text = "-"
)
```

#### Advice on snapshots 

Snapshots are brittle. The term "brittle" in the context of testing refers to a susceptibility to fail with small changes. Brittleness can produce false negatives test failures (i.e., due to inconsequential changes in the graph) when comparing a new graph to the baseline image. 

Below is the output from `diffobj::diffObj()` comparing our custom plotting function (`scatter_plot()`) against a graph built with analogous `ggplot2` code:

```{r}
#| eval: false 
#| code-fold: false
ggp_graph <- ggplot2::ggplot(mtcars, 
              ggplot2::aes(x = mpg, y = disp)) + 
              ggplot2::geom_point(
                ggplot2::aes(color = cyl), 
                             alpha = 0.5, 
                             size = 3)
  
app_graph <- scatter_plot(mtcars, 
                  x_var = "mpg", 
                  y_var = "disp", 
                  col_var = "cyl", 
                  alpha_var = 0.5, 
                  size_var = 3)

diffobj::diffObj(ggp_graph, app_graph)
```


:::: {.column-page-inset-right}

:::{#fig-08_tests_diffobj_scatter_plot}

![`diffobj::diffObj()` on graph outputs](img/08_tests_diffobj_scatter_plot.png){#fig-08_tests_diffobj_scatter_plot width='100%' align='center'}

Graph objects are difficult to use as test objects 
:::

::::

The output shows us all the potential points of failure when comparing complex objects like graphs (despite the actual outputs appearing identical), so it's best to limit the number of 'visual unit tests' unless they're absolutely necessary. 

```{r}
#| label: git_box_10c_tests-helpers
#| echo: false
#| results: asis
#| eval: true
git_margin_box(
  fig_pw = '65%', 
  branch = "10c_tests-helpers", 
  repo = 'shinyAppPkg')
```

<!--
## BDD tests 

In the sections below, I'll provide several example tests using `testthat`'s BDD functions. Hopefully the following sections convey how versatile and expressive these functions can be (or they inspire you to properly implement what I'm attempting to do in your own app-packages).
-->

## Testing modules 

Integration tests verify that functions and components work together, and often involves instantiating multiple objects to interact with each other in a single test. 

I like using the BDD functions with `testServer()` to test *reactive interactions* between modules. For example, to confirm functional requirement #2 (**FR2**) (that user-inputs are updating in the application), we need to test two changes:

1. Values passed to the UI are returned from `mod_var_input_server()`  
2. Reactive values passed into `mod_scatter_display_server()` are available as the reactive object `inputs()`

### [`session$returned()`]{style="font-size: 0.95em;"}

Inside `testServer()`, we can create a list of graph inputs for `mod_var_input_server()`, then pass identical values to `session$setInputs()`, and confirm the returned object with `session$returned()`:

```{r}
#| eval: false 
#| include: true
#| code-fold: false
testthat::describe("FR2: user-input updating (inputs)", code = { # <1>
  
  testthat::it("T5: inputs change", code = {                     # <2>
    
    shiny::testServer(app = mod_var_input_server, expr = {       # <3>
      
      test_logger(start = "T5", msg = "FR2: returned()")
      
      # create list of output vals
      test_vals <- list(y = "critics_score",                     # <4>
                        x = "imdb_rating",
                        z = "critics_rating",
                        alpha = 0.75,
                        size = 3,
                        plot_title = "Example title")            # <4>

      # change inputs
      session$setInputs(y = "critics_score",                     # <5>
                        x = "imdb_rating",
                        z = "critics_rating",
                        alpha = 0.75,
                        size = 3,
                        plot_title = "Example title")            # <5>

      testthat::expect_equal(
        object = session$returned(),        # <6>
        expected = test_vals                # <6>
      )

      test_logger(end = "T5", msg = "FR2: returned()")
      
    }) # <3>
  })
})
```
1. Functional requirement description (**FR2**)   
2. Test scope (**T5**) for inputs  
3. Call to `testServer()`   
4. Create output values for comparison  
5. Set each input using `setInputs(input = )`  
6. Confirm returned values against `test_vals`     

The test above confirms 1) new input values can be passed into the UI, and 2) these values are returned from `mod_var_input_server()`.

### [`args = list()`]{style="font-size: 0.95em;"}

Now that we've confirmed `mod_var_input_server()` is returning values, we want to make sure reactive values are read correctly by `mod_scatter_display_server()`. 

In `movies_server()`, when we pass `selected_vars` to the `var_inputs` argument, we're not passing the returned values (this is why we don't need the parentheses). We're calling on the method (or function) created by the call to `reactive()` (inside `mod_var_input_server()`).

I've included the `movies_server()` function below to refresh our memory of how this *should* work:[^tests-12]

[^tests-12]: `selected_vars` are the reactive plot values returned from `mod_var_input_server()`.

```{r}
#| eval: false 
#| code-fold: false
movies_server <- function(input, output, session) {

      selected_vars <- mod_var_input_server("vars") # <1>

      mod_scatter_display_server("plot", var_inputs = selected_vars)
      
}
```
1. Calls `return(reactive(list(...)))`  


We can pause execution with Posit Workbench's debugger, [^tests-13] to see the difference between calling `selected_vars` and `selected_vars()`:

[^tests-13]: We'll cover using `browser()` and the IDE's debugger in a future chapter.

::: {layout="[49, -2, 49]"}

```{verbatim}
#| eval: false 
#| code-fold: false
Browse[1]> selected_vars
reactive({
    list(
      y = input$y, 
      x = input$x, 
      z = input$z, 
      alpha = input$alpha, 
      size = input$size, 
      plot_title = input$plot_title
      )
})
```

```{verbatim}
#| eval: false 
#| code-fold: false
Browse[1]> selected_vars()
$y
[1] "audience_score"

$x
[1] "imdb_rating"

$z
[1] "mpaa_rating"

$alpha
[1] 0.5

$size
[1] 2

$plot_title
[1] ""
```

:::

If we're testing a module function that collects the reactive values, we need to wrap those values in `reactive()` in the `args()` argument:

```{r}
#| eval: false 
#| code-fold: false
testthat::describe("FR2: user-input updating", code = { # <1>
  testthat::it(description = "T6: plot outputs", code = {  # <2>
    shiny::testServer(
      app = mod_scatter_display_server,
      args = list(var_inputs = 
          shiny::reactive( # <3>
                    list(
                      y = "audience_score",
                      x = "imdb_rating",
                      z = "mpaa_rating",
                      alpha = 0.5,
                      size = 2,
                      plot_title = "Enter plot title"
                    )
            ) # <3>
        ),
      expr = {
        test_logger(start = "T6", msg = "FR2: inputs()")
        
        testthat::expect_equal(                      # <4>
          object = inputs(),
          expected = list(x = "imdb_rating",
                          y = "audience_score",
                          z = "mpaa_rating",
                          alpha = 0.5,
                          size = 2,
                          plot_title = "Enter Plot Title")
        ) # <4>
        
        test_logger(end = "T6", msg = "FR2: inputs()")
    })
  }) # <2>
}) # <1>
```
1. Functional requirement description (**FR1**)   
2. Test scope    
3. List of reactive variable inputs  
4. Compare `inputs()` to initial values  



I've included the example above because it's not included on the `testServer()` [documentation](https://shiny.posit.co/r/articles/improve/server-function-testing/), and I've found this method works well if you want to confirm two modules are communicating (i.e., returning and collecting outputs). System test with `shinytest2` are a better option if we're trying to capture a more comprehensive execution path (i.e., user story) in the application. 

#### Module test coverage 

When we check the code coverage for the test above, we can see it confirms `var_inputs` is communicating the reactive values to `inputs()` in  `mod_scatter_display_server()`, but this test doesn't execute the call to `scatter_plot()`:

```{r}
#| label: hot_key_cf_02
#| echo: false
#| results: asis
#| eval: true
hot_key(fun = 'cf')
```

```{r}
#| eval: false 
#| code-fold: false
devtools:::test_coverage_active_file()
```

![](img/10_tests_code_coverage_module_01.png){width="85%" fig-aligh='center'}

### Testing [`output$`]{style="font-size: 0.95em;"}s

To confirm the plot is being created properly in `mod_scatter_display_server()`, we can't use the `ggplot2::is.ggplot()` function because the plot is being rendered by `renderPlot()`. However, we can verify the structure of the `output$scatterplot` object using any of the following expectations:

```{r}
#| eval: false 
#| code-fold: false
testthat::expect_true(
  object = is.list(output$scatterplot))

testthat::expect_equal(
  object = names(output$scatterplot),
  expected = c("src", "width", "height", "alt", "coordmap"))

testthat::expect_equal(
  object = output$scatterplot[["alt"]],
  expected = "Plot object")
```

It's also possible to build the graph *inside* the test using the same code from the module server function, then confirm it with `ggplot2::is.ggplot()`:

```{r}
#| eval: false 
#| code-fold: false 
    plot <- scatter_plot(movies, # <1>
      x_var = inputs()$x,
      y_var = inputs()$y,
      col_var = inputs()$z,
      alpha_var = inputs()$alpha,
      size_var = inputs()$size) +
    ggplot2::labs(
      title = inputs()$plot_title,
      x = stringr::str_replace_all(
              tools::toTitleCase(inputs()$x), "_", " "),
      y = stringr::str_replace_all(
              tools::toTitleCase(inputs()$y), "_", " ")) +
    ggplot2::theme_minimal() +
    ggplot2::theme(legend.position = "bottom") # <1>
    
    testthat::expect_true(ggplot2::is.ggplot(plot)) # <2>
```
1. Build graph (same code from module function)   
2. Confirm `ggplot2` object is built  


This executes the code used build the graph in `mod_scatter_display_server()`:

![](img/10_tests_code_coverage_module_02.png){width="85%" fig-aligh='center'}

If we're still skeptical this test is confirming the plot is being built correctly, we can pass `plot` to `print()` in the test and the plot will appear in the **Plots** pane. 


![Passing `plot` to `print()` will send the graph to the **Plots** pane](img/10_tests_print_plot_module.png){width="85%" fig-aligh='center'}

```{r}
#| label: git_box_shinyAppPkg_10d_tests-modules
#| echo: false
#| results: asis
#| eval: true
git_margin_box(
  fig_pw = '65%',
  branch = "10d_tests-modules", 
  repo = 'shinyAppPkg')
```



## Test files 

Up to this point, I've been storing the tests in test files named for each function: 

```{bash}
#| eval: false 
#| code-fold: false
tests
└── testthat/
    ├── test-mod_scatter_display.R
    ├── test-mod_var_input.R
    └── test-scatter_plot.R
```

This is the recommendations in [R Packages, 2ed](https://r-pkgs.org/code.html#sec-code-organising), and gives us the ability to programmatically compare test results with the traceability matrix. For example, we can add the following code to the [`test-specs` vignette](https://github.com/mjfrigaard/shinyAppPkg/blob/10e_tests-system/vignettes/test-specs.Rmd) and capture our test results in a `tibble` display:

<!--
[^tests-naming-files]
[^tests-naming-files]: See Jenny Bryan's [slides/talk](https://github.com/jennybc/how-to-name-files) on naming files. -->

```{r}
#| eval: false 
#| code-fold: false
pkg_tests <- testthat::test_local("../", 
  reporter = testthat::SilentReporter$new()) |> 
  tibble::as_tibble() |> 
  tidyr::pivot_longer(cols = c(passed, skipped, 
                               warning, error, failed), 
                    names_to = 'status', 
                    values_to = "value") |> 
  dplyr::filter(value > 0) |> 
  dplyr::select(
    `Test File` = file,
    Test = test,
    `Test Status` = status)
```

:::{#fig-10_tests_trace_matrix_pkg_tests}

![Traceability matrix & `testthat::test_local()` output](img/10_tests_trace_matrix_pkg_tests.png){#fig-10_tests_trace_matrix_pkg_tests width='100%' align='center'}

Package vignette with with traceability matrix and test results 
:::

Placing the test number in each call to `testthat::it()` will make it easier to link the tests back to the original functional requirements, features and specifications.

## System tests

System (or end-to-end) tests simulate real user interactions in a 'pre-production' environment to verify the whole application (or system) works.[^tests-prod] Approaches to system testing vary, but in general, we'll want to run system tests before a release, which means all of the app's functional requirements are tested and released in lockstep.

[^tests-prod]: System tests should strive to replicate the production *conditions*, even when/if it's not possible to perfectly replicate the environment.

### [`shinytest2`]{style="font-size: 1.05em;"}


[`shinytest2`](https://rstudio.github.io/shinytest2/index.html) requires a few steps to get up and running, most notably the [`chromote` package](https://rstudio.github.io/chromote/).[^shinytest2-start] 

[^shinytest2-start]: A great place to start is the [Getting Started](https://rstudio.github.io/shinytest2/articles/shinytest2.html) vignette. 

```{r}
#| label: co_box_chromote
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "g", fold = TRUE, look = "minimal",
  size = '1.10', header = "Chromium access",
  contents = 
" 
If you're working in an environment without access to the Chromium headless browser,
  
  ")
```

After installing the `shinytest2` dependencies, you'll want to verify you can create a new session with: 

```{r}
#| eval: false 
#| code-fold: false
library(chromote)
b <- ChromoteSession$new()
b$view()
```

:::{#fig-10_tests_trace_matrix_pkg_tests}

![Chromium headless browser](img/10_chromote_new_verify.png){#fig-10_chromote_new_verify width='100%' align='center'}

A new `Chromote` session
:::


```{r}
#| eval: false 
#| code-fold: false
shinytest2::use_shinytest2()
```

```{verbatim}
#| eval: false 
#| code-fold: false
! Runner already found: tests/testthat.R
✔ Adding 'shinytest2::load_app_env()' to 'tests/testthat/setup-shinytest2.R'
```

```{verbatim}
#| eval: false 
#| code-fold: false
✔ Adding '*_.new.png' to '.gitignore'
```

```{verbatim}
#| eval: false 
#| code-fold: false
✔ Adding '_\\.new\\.png$' to '.Rbuildignore'
```

```{verbatim}
#| eval: false 
#| code-fold: false
✔ Setting active project to '/path/to/shinyAppPkg'
```

```{verbatim}
#| eval: false 
#| code-fold: false
✔ Adding 'shinytest2' to Suggests field in DESCRIPTION
```

```{verbatim}
#| eval: false 
#| code-fold: false
• In your package code, use `rlang::is_installed("shinytest2")` or
  `rlang::check_installed("shinytest2")` to test if shinytest2 is installed
• Then directly refer to functions with `shinytest2::fun()`
```

```{verbatim}
#| eval: false 
#| code-fold: false
✔ Setting active project to '<no active project>'
```





<!--

> Manual Testing: Initially, you may have to manually interact with the Shiny app to ensure it behaves as expected.

> Automated Browser Testing: Use tools like shinytest to automate browser-based testing for the Shiny application. shinytest captures the state of the application and allows you to compare it to expected states.

> User Scenarios: Script typical user interaction scenarios to ensure that the whole system, including front-end and back-end, work seamlessly.

-->




<!--

#### [`withr`]{style="font-size: 1.05em;"}

The date/time output from `test_logger()` doesn't provide the level of precision we want to monitor our unit tests (they're supposed to be fast). The R option to change this setting is `options(digits.secs)`, but we don't want to include this options in our test file (we just need it whenever we use `test_logger()`).[^tests-message2]

We can add `withr::local_options()` inside `test_logger()` to make sure this option only applies to the test environment it's called from. Subsequent tests will pass without a warning and provide a more precise date/time message:

[^tests-message2]: This example is similar to the `message2()` function on the [`testthat` website](https://testthat.r-lib.org/articles/test-fixtures.html#test-fixtures), but calls `withr` *inside* the helper function.

Back in the **Console**, I can see the output from `Sys.time()` doesn't include the `test_logger()` option:

```{r}
#| eval: false 
#| code-fold: false
Sys.time()
```

```{verbatim}
#| eval: false 
#| code-fold: false
[1] "2023-09-23 14:16:30 PDT"
```

-->

## Recap 

Behavior-driven development (or behavior-driven testing) fills the void between non-technical stakeholders and developers by encouraging natural, descriptive language (often complete sentences) to define and communicate the application requirements.

The goal of any shiny app should be to create something that helps drive data-driven decisions. Pragmatically, this means not writing code that isn't addressing a need the user or stakeholder has requested. I've found building the traceability matrix and scoping the tests with the BDD functions forces me ask myself, "*does this test address a user need?*" (which saves me from developing a feature no one asked for).

<!--

> Consider setting up a CI pipeline using services like GitHub Actions, GitLab CI, or Travis CI to automate the running of these tests.

> By implementing these various levels of tests, you can build confidence that your R package and Shiny app are robust, reliable, and ready for deployment.

-->

<!--
Testing an R package with a Shiny application can be challenging due to the various components and layers of interactivity. Here's a general strategy to approach this:
-->

