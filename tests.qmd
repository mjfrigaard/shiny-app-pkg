# Tests 

```{r}
#| eval: true 
#| echo: false 
#| include: false
source("_common.R")
library(testthat)
library(gt)
```

```{r}
#| label: co_box_dev
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "y",
  header = "Caution!",
  contents = "This section is currently being revised. Thank you for your patience."
)
```

Testing shiny app-packages poses some unique challenges. Shiny functions are written in the context of its reactive model,[^tests-shiny-reactive] so some standard testing techniques and methods for regular R packages don’t directly apply. Fortunately, the infrastructure for storing and running tests in app-packages is identical to a standard R package.

[^tests-shiny-reactive]: The ['Reactivity - An overview'](https://shiny.posit.co/r/articles/build/reactivity-overview/) article gives an excellent description (and mental module) of reactive programming.

This chapter will cover the differences between user specifications, feature requirements, and functional requirements. I’ll introduce how you can use `testthat`s behavior-driven development (BDD)[^tests-bdd] and a traceability matrix to map each functional requirement to a specific test, ensuring that all user's needs are met (and the app's features implemented correctly).

I'll give examples for three types of testing: unit tests, integration/module tests, and end-to-end or system tests, focusing on *what to test* and *why to test it*, not *how to write tests* (plenty of those resources exist[^tests-intro-unit-tests] [^tests-intro-shinytest2] [^tests-intro-ms-testing]), the only exception is some tricks I've learned for using `testServer()` with module server functions.[^tests-intro-testserver]

[^tests-bdd]: Read more about [behavior-driven development](https://en.wikipedia.org/wiki/Behavior-driven_development)

[^tests-intro-unit-tests]: Unit tests are covered extensively in [R Packages, 2ed](https://r-pkgs.org/testing-basics.html) and the [`testthat` documentation](https://testthat.r-lib.org/index.html)

[^tests-intro-shinytest2]: `shinytest2` has [excellent documentation](https://rstudio.github.io/shinytest2/) (and [videos](https://www.youtube.com/watch?v=Gucwz865aqQ)), and I highly recommend reading through those resources.

[^tests-intro-ms-testing]: Mastering shiny dedicates an entire [Chapter to Testing](https://mastering-shiny.org/scaling-testing.html), which covers [unit tests](https://mastering-shiny.org/scaling-testing.html#basic-structure) and [`testServer()`](https://mastering-shiny.org/scaling-testing.html#testing-reactivity), and also includes some tips for using JavaScript with [`shinytest`](https://mastering-shiny.org/scaling-testing.html#testing-javascript) (not to be confused with [`shinytest2`](https://rstudio.github.io/shinytest2/)) 

[^tests-intro-testserver]: The [`testServer()`](https://shiny.posit.co/r/articles/improve/server-function-testing/) documentation is sparse, so I'll provide a few tips and tricks I've learned for testing module server functions.

The code chunk below will load the necessary testing packages.

```{r}
#| eval: false
#| code-fold: false
install.packages(c("testthat", "shinytest2", "covr"))
library(testthat)
library(shinytest2)
library(covr)
```

(*If you're using `devtools`, you won't have to worry about installing `testthat` and `covr`*)

# User Specifications & Requirements

When you begin developing a Shiny app-package, you’ll be faced with two questions:

1. What do I test?  
2. How do I test it?   

Applications *should* have accompanying resources to help address these questions, the most common being a software requirements specification (SRS) document. The SRS typically breaks down an application's intended purpose (i.e., the problem it's designed to solve) into three general areas: user specifications, feature requirements, and functional requirements: 

-   **The user specifications capture the needs and expectations of the end-user.** These are usually non-technical and focused on the "why" and the "what" of the application.

-   **The feature requirements list the high-level capabilities of the application.** Features are defined early in the life of a project and become the talking points during discussions with stakeholders to be used for scoping and prioritization. Because their scope can be broad, they may be composed of various functional (and sometimes even non-functional).

-   **Functional requirements are the testable, specific actions, inputs, and outputs.** Functional requirements provide the technical details of how the features will be implemented, and a single feature can give rise to multiple functional requirements.

These guidelines direct the development process, albeit from slightly different perspectives. Understanding the interplay between user specifications, functional requirements, and feature requirements is essential for helping developers know if the application meets the technical standards and satisfies the user’s needs.

I'll use `shinyAppPkg` to illustrate a brief example of each in the sections below:

## User Specifications

**User Specifications** are what the end-user (in this case, a film analyst) wants to achieve with the application.[^tests-specs-user-specs]

[^tests-specs-user-specs]: User Specifications are sometimes referred to as "user stories," "use cases," or "general requirements"

```{r}
#| label: co_box_user_specs
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "y", fold = FALSE, look = "minimal", 
  size = '1.10', header = "Scatter plot user specification",
  contents = 
" 

**US1**: '*As an film analyst, I want to view an interactive scatter plot in a dashboard that consolidates movie reviews from multiple sources so that I can compare and analyze trends and outliers in movie ratings quickly.*'
  
  ")
```

## Feature Requirements

The **feature requirement** translates the end user's expectation into specific language describing an application capability (i.e., display a scatter plot), phrased to satisfy a specific end-user need outlining in the specifications.[^tests-specs-feat]

[^tests-specs-feat]: "Feature requirements" and "functional requirements" are sometimes used interchangeably, but they refer to different aspects of the software. **Feature requirements** are the desired high-level characteristics the application *should* have, and often capture a collection of smaller functionalities (which are broken down into specific functional requirements).

```{r}
#| label: co_box_feat_reqs
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "o", fold = FALSE, look = "minimal",
  size = '1.10', header = "Scatter plot feature requirements",
  contents = 
" 

**FE1**: '*Given that movie reviews are available on multiple websites, when the user selects a rating metric from the application display, the interactive scatter plot should allow for comparisons from at least two data sources (i.e., IMDb, Rotten Tomatoes), and include options for selecting other variables of interest (i.e., audience scores, runtime, etc.).*'
  
  ")
```

## Functional Requirements

The **Functional Requirements** are written for the developer and provide technical details on *how* the feature (i.e., the scatter plot) should behave and *what* it needs to do. These requirements are written for developers and represent where the end-users' expectations come into direct contact with code.[^tests-specs-func]

[^tests-specs-func]: **Functional requirements** are precise, measurable, and testable. 

```{r}
#| label: co_box_fun_reqs
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "b", fold = FALSE, look = "minimal",
  size = '1.10', header = "Scatter plot functional requirements",
  contents = 
" 
  - **FR1**: The system will display movie reviews from IMDb and Rotten Tomatoes collected from their respective APIs.

  - **FR2**: The scatter plot will be displayed on the dashboard and updated with new user inputs.

  - **FR3**: Each data point on the scatter plot will represent a movie and be color-coded based on the following categories: MPAA ratings, genre, title type, critics rating, and audience rating.
  
  - **FR4**: The scatter plot will have labeled axes, a legend to differentiate between data sources, and a customizable title.
  
  ")
```

## Traceability matrix

A traceability matrix is a table that ‘traces’ the user specifications to features and functional requirements (and the tests they give rise to) to verify that the application has been developed correctly. Package vignettes are a great place to store a draft of the traceability matrix, and they can be quickly created using `usethis::use_vignette()`:

```{r}
#| label: git_box_08_tests-specs
#| echo: false
#| results: asis
#| eval: true
git_margin_box(
  fig_pw = '75%', 
  branch = "08_tests-specs", 
  repo = 'shinyAppPkg')
```

```{r}
#| eval: false 
#| code-fold: false
usethis::use_vignette("test-specs")
```

Adding our first vignette to the `vignettes/` folder does the following:

- [x]   Adds the `knitr` and `rmarkdown` packages to the `Suggests` field in `DESCRIPTION`[^test-specs-suggests]

    ```bash
    Suggests: 
        knitr,
        rmarkdown
    ```

- [x]   Adds `knitr` to the `VignetteBuilder` field[^test-specs-vignette-builder]

    ```bash
    VignetteBuilder: knitr
    ```

- [x]   Adds `inst/doc` to `.gitignore` and `*.html`, `*.R` to `vignettes/.gitignore`[^test-specs-inst-docs]


[^test-specs-suggests]: We briefly covered the `Suggests` field in [Dependencies](https://mjfrigaard.github.io/shinyap/dependencies.html), but in this case it specifically applies to "*packages that are not necessarily needed. This includes packages used only in examples, tests or vignettes...*" - [Writing R Extensions, Package Dependencies](https://cran.r-project.org/doc/manuals/R-exts.html#Package-Dependencies)

[^test-specs-vignette-builder]: The [documentation](https://cran.r-project.org/doc/manuals/R-exts.html#The-DESCRIPTION-file) on `VignetteBuilder` gives a great description of why `knitr` and `rmarkdown` below in `Suggests`, "*Note that if, for example, a vignette has engine `knitr::rmarkdown`, then `knitr` provides the engine but both `knitr` and `rmarkdown` are needed for using it, so both these packages need to be in the `VignetteBuilder` field and at least suggested (as `rmarkdown` is only suggested by `knitr`, and hence not available automatically along with it).*"

[^test-specs-inst-docs]: We covered the `inst/` folder in the [External Files chapter](https://mjfrigaard.github.io/shinyap/), and you might recall that `docs/` was one of the folders we shouldn't create inside `inst/`.

We want to use the traceability matrix to ensure the tests cover all the functionalities (i.e., the code) and address the user specifications *and* that every user need corresponds to a functionality that’s been tested. 

#### User specs & features 

We'll start with the user specifications. The language used for these descriptions is non-technical but should provide a basis for deriving the more technical (but still high-level) feature requirements.[^tests-visual-markdown]

[^tests-visual-markdown]: When building tables in vignettes, I highly suggest using the [Visual Markdown mode](https://rstudio.github.io/visual-markdown-editing/) (especially when building tables).

```{r}
#| label: co_box_trace_matrix_specs_features
#| echo: false
#| code-fold: false
tibble::tibble(
  `User Specification` = c(
    "US1: scatter plot data visualization"
  ),
  `Feature Requirement` = c(
    "FE1: interactive scatter plot (two data sources, drop-down variable options)"
  ),
  `Functional Requirements` = c(
    NA_character_
  )
) |> 
  gt::gt(auto_align = TRUE) |> 
  gt::sub_missing(
  columns = gt::everything(),
  rows = gt::everything(),
  missing_text = "-"
)
```

#### Functional requirements 

A single feature often produces multiple functional requirements.

```{r}
#| label: co_box_trace_matrix_specs_features_function_01
#| echo: false
#| code-fold: false
#| include: true
trace_matrix <- tibble::tibble(
  `User Specification` = c(
    "US1: scatter plot data visualization",
    NA_character_,
    NA_character_,
    NA_character_
  ),
  `Feature Requirement` = c(
    "FE1: interactive scatter plot (two data sources, drop-down variable options)",
    NA_character_,
    NA_character_,
    NA_character_
  ),
  `Functional Requirements` = c(
    "FR1: data source",
    "FR2: user-input updating",
    "FR3: color-coded data points",
    "FR4: plot axis, legend & title"
  )
) 
trace_matrix |> 
  gt::gt(auto_align = TRUE) |> 
  gt::sub_missing(
  columns = gt::everything(),
  rows = gt::everything(),
  missing_text = "-"
)
```

#### Tests

After translating the user needs into technical requirements, we can quickly identify what needs to be tested.

```{r}
#| label: co_box_trace_matrix_tests
#| echo: false
#| code-fold: false
#| include: true
trace_matrix |> 
  tibble::add_column(Tests = 
      paste0("T", 1:nrow(trace_matrix)), .after = 3) |>
  gt::gt() |> 
  gt::sub_missing(
  columns = gt::everything(),
  rows = gt::everything(),
  missing_text = "-"
)
```


The matrix allows us to ensure:

1. The user specifications have accompanying feature requirements.

2. Each feature has been broken down into precise, measurable, and testable functional requirements.

3. Tests have been written for each functional requirement.

If you'd like, you can include additional columns to track whether the test has passed:

```{r}
#| label: co_box_trace_matrix_add_test_status
#| echo: false
#| code-fold: false
trace_matrix |> 
  tibble::add_column(Tests = 
      paste0("T", 1:nrow(trace_matrix)), .after = 3) |> 
  tibble::add_column(`Test Status` = 
      paste0("FR", 1:nrow(trace_matrix), " test status: "), .after = 4) |> 
  gt::gt() |> 
  gt::sub_missing(
  columns = gt::everything(),
  rows = gt::everything(),
  missing_text = "-"
)
```


Vignettes are a great place to store this information because it's self-contained and travel with the package whenever the code is updated.[^test-specs-7]

[^test-specs-7]: Documenting the traceability matrix in vignettes are great for developers, but it's also a good idea use an issue-tracking system with version control, like GitHub Projects or Azure DevOps.

The following section covers setting up tests with `testthat`, the structure of unit tests, and additional elements you should consider using in your app-package.

# [`testthat`]{style="font-size: 1.05em;"} framework

Multiple strategies exist for writing tests. For example, if you've adopted test-driven development (TDD),[^tests-tdd] you'll write tests before developing utility functions, modules, or a standalone app function.

Regardless of the testing strategy you choose, I recommend setting up the testing infrastructure in our app-package with the [`testthat` package](https://testthat.r-lib.org/):

[^tests-tdd]: Read more about [Test-driven development](https://en.wikipedia.org/wiki/Test-driven_development) 

## [`use_testthat()`]{style="font-size: 1.05em;"}

In packages using `devtools`, the testing infrastructure can be built with `usethis::use_testthat()`:

```{r}
#| eval: false
#| code-fold: false
usethis::use_testthat()
```

- [x]   In the `DESCRIPTION` file, `testthat (>= 3.0.0)` is listed under `Suggests` and `Config/testthat/edition: 3` is used to specify the `testthat` edition

```{verbatim}
#| eval: false
#| code-fold: false
Suggests: 
    testthat (>= 3.0.0)
Config/testthat/edition: 3
```
    
- [x]   A new `tests/` folder is created, with a `testthat/` subfolder. The `testthat.R` file is created (which is sometimes referred to as the 'test runner,' because it runs all our tests).

```{bash}
#| eval: false
#| code-fold: false
tests/
  ├── testthat/
  └── testthat.R #<1>

2 directories, 1 file
```
1. Test runner

### [`use_test()`]{style="font-size: 0.95em;"} unit tests

New tests are created with `usethis::use_test()`.[^test-files-folders]

[^test-files-folders]: Ideally there is a file named for every function in the `R/` folder, and a corresponding test file (with the `test-` prfix) in the `tests/testthat/` folder]

```{r}
#| eval: false
#| code-fold: false
usethis::use_test("scatter_plot")
```

#### [`test-`]{style="font-size: 0.95em;"} files

- [x]   **Test files**: the IDE will automatically create and open the new test file: 

```{verbatim}
#| eval: false
#| code-fold: false
✔ Writing 'tests/testthat/test-scatter_plot.R'
• Modify 'tests/testthat/test-scatter_plot.R'
```

#### [`test_that()`]{style="font-size: 0.95em;"} tests 

- [x]   **Tests**: Each new test file contains a boilerplate `test_that()` test, with `desc` (the test context) and `code` arguments (supplied in curly brackets). 

```{r}
#| eval: false
#| code-fold: false 
test_that(desc = "multiplication works", code = { 
 
})
```

#### [`expect_`]{style="font-size: 0.95em;"}ations 

- [x]   **Expectations**: most expectation have two parts: an `observed` object, and an `expected` object. The `observed` object is an artifact of some code you've written, and it's being compared against an `expected` result.

```{r}
#| eval: false
#| code-fold: false 
#| collapse: true
expect_equal( # <1> 
  object = 2 * 2, # <2> 
  expected = 4 # <3> 
  ) 
```
1. The test expectation
3. What is observed (i.e., the value or object)
4. What was expected (i.e., the predefined criteria for success) 

#### Running tests

- [x]   **Running tests**: Another `devtools` habit to adopt is regularly writing and running tests. If you're using Posit Workbench and have `devtools` installed, you can test your app-package using the **Build** pane or the keyboard shortcut:

::: {.column-margin}

[<kbd>Ctrl/Cmd</kbd> + <kbd>Shift</kbd> + <kbd>T</kbd>]{style="font-weight: bold; font-style: italic; font-size: 1.20em"}

:::


![`devtools::test()` (run all tests in your `tests/` folder)](img/08_tests_build_pane_test.png){#fig-08_tests_build_pane_test width='75%' align='center'}

When the test is run, you'll see feedback on whether it passes or fails (and some encouragement):

```{r}
#| eval: true
#| code-fold: false
#| echo: true 
#| collapse: true
test_that("multiplication works", { 
  expect_equal( 
    object = 2 * 2, 
    expected = 4 
    ) 
})
```

Now that we have our testing framework and the traceability matrix to guide development, I will cover a *slightly* different approach to testing that I’ve found helpful with app-packages (and has excellent support in `testthat`).

# Behavior-driven development

Behavior-driven development (BDD) (or behavior-driven testing) is a methodology that involves collaboration between developers, users, and domain experts to define requirements and write tests using domain-specific language. The BDD approach emphasizes writing human-readable descriptions of the application's behavior, which is then converted into a series of tests.

> "*[BDD] encourages teams to use conversation and concrete examples to formalize a shared understanding of how the application should behave.*" - [BDD, Wikipedia](https://en.wikipedia.org/wiki/Behavior-driven_development#:~:text)

Behavior-driven testing can be performed using a `testthat`'s `describe()` and `it()` functions.[^tests-bdd-describe] Below is an example of how these might look for the **US1** user specification in `shinyAppPkg`:

[^tests-bdd-describe]: Read more about `describe()` and `it()` in the [`testthat` documentation.](https://testthat.r-lib.org/reference/describe.html)

## [`describe()`]{style="font-size: 0.95em;"}

The `testthat::describe()` function follows a BDD format and '*specifies a larger component or function and contains a set of specifications*'

In `describe()`, I'll reference the feature I'm testing (**FE1**): 

```{r}
#| label: co_box_trace_matrix_specs_features_bdd
#| echo: false
#| code-fold: false
tibble::tibble(
  `User Specification` = c(
    "US1: scatter plot data visualization"
  ),
  `Feature Requirement` = c(
    "FE1: interactive scatter plot (two data sources, drop-down variable options)"
  )
) |> 
  gt::gt(auto_align = TRUE) |> 
  gt::sub_missing(
  columns = gt::everything(),
  rows = gt::everything(),
  missing_text = "-"
)
```

```{r}
#| eval: false 
#| code-fold: false
testthat::describe(description = "FE1: interactive scatter plot", code = {
  

})
```

## [`it()`]{style="font-size: 0.95em;"}

Inside `describe()`, we can include multiple `it()` blocks which "*functions as a test and is evaluated in its own environment.*" 

In the example below, I use `it()` blocks to test each functional requirement:[^tests-it-blocks]

```{r}
#| label: co_box_trace_matrix_specs_features_function__bdd
#| echo: false
#| code-fold: false
#| include: true
trace_matrix <- tibble::tibble(
  `Feature Requirement` = c(
    "FE1: interactive scatter plot (two data sources, drop-down variable options)",
    NA_character_,
    NA_character_,
    NA_character_
  ),
  `Functional Requirements` = c(
    "FR1: data source",
    "FR2: user-input updating",
    "FR3: color-coded data points",
    "FR4: plot axis, legend & title"
  )
) 
trace_matrix |> 
  gt::gt(auto_align = TRUE) |> 
  gt::sub_missing(
  columns = gt::everything(),
  rows = gt::everything(),
  missing_text = "-"
)
```

[^tests-it-blocks]: Each [`it()`](https://testthat.r-lib.org/reference/describe.html) block contains the expectations (or what you would traditionally include in `test_that()`).

```{r}
#| eval: false 
#| code-fold: false
testthat::describe(description = "FE1: interactive scatter plot", code = {
  
  testthat::it(description = "FR1: data source", code = {
    # test code 
    testthat::expect_equal(object = , expected = )
  })

  testthat::it(description = "FR2: user-input updating", code = {
    # test code 
    testthat::expect_equal(object = , expected = )
  })

  testthat::it(description = "FR3: color-coded data points", code = {
    # test code 
    testthat::expect_equal(object = , expected = )
  })
  
  testthat::it(description = "FR4: plot axis, legend & title", code = {
    # test code 
    testthat::expect_equal(object = , expected = )
  })
})
```

Next we're going to cover two additional testing tools (fixtures and helpers) you can include in your test suite to make testing your app-package a little easier.

# Fixtures 

```{r}
#| label: git_box_08_tests-helpers-fixtures
#| echo: false
#| results: asis
#| eval: true
git_margin_box(
  fig_pw = '75%', 
  branch = "08_tests-helpers-fixtures", 
  repo = 'shinyAppPkg')
```

Test fixtures are various resources used to ensure a consistent, well-defined environment. Fixtures can be input data, database connections, R options, environment variables, or anything else needed to create a repeatable infrastructure (even when the test is run on different environments or setups). 

A good test fixture also 'cleans itself up' after the test has been run to make sure any changes made during the test doesn't persist or interfere with other tests in your app-package.[^tests-fixtures] 

In `R/` packages, test fixtures are stored in the `tests/testthat/fixtures/` folder:

```{bash}
#| eval: false 
#| code-fold: false
tests/
├── testthat/
│   └── fixtures/                                          # <1>
└── testthat.R
```
1. The name '`fixtures`' isn't required (you can name this folder anything)

[^tests-fixtures]: Test fixtures are described in-depth in [R Packages, 2ed](https://r-pkgs.org/testing-advanced.html#test-fixtures) and in the [`testthat` documentation](https://testthat.r-lib.org/articles/test-fixtures.html#test-fixtures)

## Test data

The code used to create test data files should also be stored in `tests/testthat/fixtures/` with a clear naming convention. I've provided the code used to tidy `ggplot2movies::movies` as an example data fixture below:[^tests-fixtures-naming-conv] 

[^tests-fixtures-naming-conv]: Creating a tidied version of `ggplot2movies::movies` would be costly to re-create with every test, so it's advised to store it as an [static test fixture.](https://r-pkgs.org/testing-advanced.html#sec-testing-advanced-concrete-fixture)

```{r}
#| eval: false 
#| code-fold: true 
#| code-summary: 'show/hide make-ggp2-movies.R'
# pkgs <- c('ggplot2movies', 'tidyr', 'dplyr', 'stringr', 'purrr')
# install.packages(pkgs, quiet = TRUE)

# load packages --------------------
library(tidyr)
library(dplyr)
library(stringr)
library(purrr)

ggp2movies <- ggplot2movies::movies |>
  pivot_longer(c(Action:Short),
    names_to = "genre_key",
    values_to = "genre_value"
  ) |>
  dplyr::mutate(genre_value = as.logical(genre_value)) |>
  dplyr::select(
    title, genre_key, genre_value, length,
    year, budget, avg_rating = rating, votes, mpaa
  ) |>
  dplyr::filter(genre_value == TRUE) |>
  group_by(title) |>
  dplyr:::mutate(
    genres = paste0(genre_key, collapse = ", ")
  ) |>
  dplyr::select(
    title, genres, length, year,
    budget, avg_rating, votes, mpaa
  ) |>
  dplyr::ungroup() |>
  dplyr::distinct(.keep_all = TRUE) |>
  dplyr::mutate(
    genres = dplyr::na_if(x = genres, ""),
    genres = factor(genres),
    mpaa = dplyr::na_if(x = mpaa, y = ""),
    mpaa = factor(mpaa,
      levels = c("G", "PG", "PG-13", "R", "NC-17"),
      labels = c("G", "PG", "PG-13", "R", "NC-17")
    )
  ) |> 
  tidyr::drop_na()
# save to tests/testthat/fixtures/
saveRDS(object = ggp2movies, file = "tests/testthat/fixtures/ggp2_movies.rds") 
```

```{bash}
#| eval: false 
#| code-fold: false 
tests/
  └── testthat/
      └── fixtures/
            ├── ggp2_movies.rds # <1>
            └── make-ggp2-movies.R # <1>
       
3 directories, 2 files
```
1. The code used to create the test data (`make-ggp2-movies.R`) is stored in the same location as the output it creates (i.e., `ggp2_movies.rds`):


# Helpers 

Test helpers are functions code that make creating and running tests easier.

> "*Helper files are a mighty weapon in the battle to eliminate code floating around at the top-level of test files.*" Testthat helper files, [R Packages, 2ed](https://r-pkgs.org/testing-design.html#testthat-helper-files)

## App input helpers 

The `var_inputs()` function below is a test fixture we can use to create inputs for the `scatter_plot()` utility function:

```{r}
#| eval: true 
#| code-fold: false 
#| collapse: true
var_inputs <- function() {
  list(
    y = "audience_score",
    x = "imdb_rating",
    z = "mpaa_rating",
    alpha = 0.5,
    size = 2,
    plot_title = "Enter plot title"
  )
}
var_inputs()$x
```

We can also create a helper for the tidy `ggplot2movies::movies` data:

```{r}
#| eval: true 
#| code-fold: false 
#| collapse: true
ggp2_inputs <- function() {
    list(
      x = "avg_rating",
      y = "length",
      z = "mpaa",
      alpha = 0.75,
      size = 3,
      plot_title = "Enter plot title"
    )
}
ggp2_inputs()$x
```

Using a function to derive test inputs is especially helpful when testing module server function because we don't need to change the way we refer to reactive inputs (as you'll see below). 

Functions like `var_inputs()` and `ggp2_inputs()` can be stored in `tests/testthat/helper.R`, which is automatically loaded with `devtools::test()`:

```{bash}
#| eval: false 
#| code-fold: false
tests/
  └── testthat/
      ├── fixtures/                                         # <1>
      │   ├── make-ggp2-movies.R
      │   └── ggp2_movies.rds
      ├── helper.R                                          # <2>
      └── test-scatter_plot.R                               # <3>
```
1. Test data script and `.rds` file  
2. `var_inputs()` and `ggp2_inputs()` functions  
3. Test file


The next section demonstrates how to use test fixtures and helpers with `testthat`'s [behavior-driven development](https://testthat.r-lib.org/reference/describe.html) functions (`describe()` and `it()`) and the contents of the traceability matrix. 

# Example: testing outputs

If we wanted to confirm the `scatter_plot()` function returns a `ggplot2` object (i.e., with `is.ggplot()`) the `describe()` and`it()` code would look something like this:

```{r}
#| eval: false 
#| code-fold: false
testthat::describe(description = "FE1: interactive scatter plot", code = {
  
  testthat::it(description = "FR1: data source", code = {
    
      p <- scatter_plot(movies,
        x_var = var_inputs()$x,
        y_var = var_inputs()$y,
        col_var = var_inputs()$z,
        alpha_var = var_inputs()$alpha,
        size_var = var_inputs()$size
      )
    testthat::expect_true(object = ggplot2::is.ggplot(p))
    
  })
  
})
```

This test answers the question, '*does the plot generate without producing an error,*'[^tests-graphs] which is appropriate because this is a test of the data source (`movies`), not necessarily the graph output.

[^tests-graphs]: Snapshot tests would be more appropriate for answering the question, ['*Is the plot visually correct?*'](https://shiny.posit.co/r/articles/improve/server-function-testing/index.html#complex-outputs-plots-htmlwidgets).

We can also use a nested `describe()` chunk to load and test a second data source (i.e., the `ggplot2movies::movies` data):[^tests-ggp2_movies-raw-data]

[^tests-ggp2_movies-raw-data]: If the data in `tests/testthat/fixtures/` are going to be used repeatedly, it might also make sense to store it in `inst/extdata/` or `data-raw/`.

```{r}
#| eval: false 
#| code-fold: false
testthat::describe(description = "US1: scatter plot data visualization", code = { # <1>
  
  testthat::describe(description = "FR1: data source (movies)", code = { # <2>
    
    testthat::it(description = "T1: movies data source", code = { 
      p <- scatter_plot(movies, # <3>
        x_var = var_inputs()$x, # <3>
        y_var = var_inputs()$y, # <3>
        col_var = var_inputs()$z, # <3>
        alpha_var = var_inputs()$alpha, # <3>
        size_var = var_inputs()$size # <3>
      )
      expect_true(object = ggplot2::is.ggplot(p))
    })
    
  })
  
  testthat::describe(description = "FR1: data source (ggp2_movies)", code = { # <4>
    
    testthat::it(description = "T2: ggp2_movies.rds data source", code = {
      ggp2_movies <- readRDS(test_path("fixtures", "ggp2_movies.rds")) # <5>
      p <- scatter_plot(ggp2_movies, # <6>
        x_var = ggp2_inputs()$x, # <6>
        y_var = ggp2_inputs()$y, # <6>
        col_var = ggp2_inputs()$z, # <6>
        alpha_var = ggp2_inputs()$alpha, # <6>
        size_var = ggp2_inputs()$size # <6>
      )
      expect_true(object = ggplot2::is.ggplot(p))
    })
    
  })
  
})
```
1. User specification   
2. `movies` data component    
3. Test `movies` data with `var_inputs()` helper   
4. `ggp2_movies` data component    
5. Load `ggp2_movies.rds` test fixture with `testthat::test_path()`   
6. Test `ggp2_movies` data with `ggp2_inputs()` helper   

```{r}
#| eval: false 
#| code-fold: true
#| include: false
testthat::describe(description = "scatter_plot() function", code = {
  
  testthat::describe(description = "movies data", code = {
    testthat::it(description = "is ggplot2 object", code = {
      p <- scatter_plot(movies,
        x_var = var_inputs()$x,
        y_var = var_inputs()$y,
        col_var = var_inputs()$z,
        alpha_var = var_inputs()$alpha,
        size_var = var_inputs()$size
      )
      expect_true(object = ggplot2::is.ggplot(p))
    })
    testthat::it(description = "adds labels", code = {
      p_labels <- scatter_plot(movies,
        x_var = var_inputs()$x,
        y_var = var_inputs()$y,
        col_var = var_inputs()$z,
        alpha_var = var_inputs()$alpha,
        size_var = var_inputs()$size
      ) +
        ggplot2::labs(
          title = var_inputs()$plot_title,
          x = stringr::str_replace_all(
            tools::toTitleCase(var_inputs()$x), "_", " "
          ),
          y = stringr::str_replace_all(
            tools::toTitleCase(var_inputs()$y), "_", " "
          )
        )
      expect_true(object = ggplot2::is.ggplot(p_labels))
    })

    testthat::it(description = "adds theme layer", code = {
      p_theme <- scatter_plot(movies,
        x_var = var_inputs()$x,
        y_var = var_inputs()$y,
        col_var = var_inputs()$z,
        alpha_var = var_inputs()$alpha,
        size_var = var_inputs()$size
      ) +
        ggplot2::labs(
          title = var_inputs()$plot_title,
          x = stringr::str_replace_all(
            tools::toTitleCase(var_inputs()$x), "_", " "
          ),
          y = stringr::str_replace_all(
            tools::toTitleCase(var_inputs()$y), "_", " "
          )
        ) +
        ggplot2::theme_minimal() +
        ggplot2::theme(legend.position = "bottom")

      expect_true(object = ggplot2::is.ggplot(p_theme))
    })
  })

  testthat::describe(description = "ggp2_movies data", code = {
    
    testthat::it(description = "is ggplot2 object", code = {
      ggp2_movies <- readRDS(test_path("fixtures", "ggp2_movies.rds"))
      p <- scatter_plot(ggp2_movies,
        x_var = ggp2_inputs()$x,
        y_var = ggp2_inputs()$y,
        col_var = ggp2_inputs()$z,
        alpha_var = ggp2_inputs()$alpha,
        size_var = ggp2_inputs()$size
      )
      expect_true(object = ggplot2::is.ggplot(p))
    })
    testthat::it(description = "adds labels", code = {
      ggp2_movies <- readRDS(test_path("fixtures", "ggp2_movies.rds"))
      p_labels <- scatter_plot(ggp2_movies,
        x_var = ggp2_inputs()$x,
        y_var = ggp2_inputs()$y,
        col_var = ggp2_inputs()$z,
        alpha_var = ggp2_inputs()$alpha,
        size_var = ggp2_inputs()$size
      ) +
        ggplot2::labs(
          title = ggp2_inputs()$plot_title,
          x = stringr::str_replace_all(
            tools::toTitleCase(ggp2_inputs()$x), "_", " "
          ),
          y = stringr::str_replace_all(
            tools::toTitleCase(ggp2_inputs()$y), "_", " "
          )
        )
      expect_true(object = ggplot2::is.ggplot(p_labels))
    })
    testthat::it(description = "adds theme layer", code = {
      ggp2_movies <- readRDS(test_path("fixtures", "ggp2_movies.rds"))
      p_theme <- scatter_plot(ggp2_movies,
        x_var = ggp2_inputs()$x,
        y_var = ggp2_inputs()$y,
        col_var = ggp2_inputs()$z,
        alpha_var = ggp2_inputs()$alpha,
        size_var = ggp2_inputs()$size
      ) +
        ggplot2::labs(
          title = ggp2_inputs()$plot_title,
          x = stringr::str_replace_all(
            tools::toTitleCase(ggp2_inputs()$x), "_", " "
          ),
          y = stringr::str_replace_all(
            tools::toTitleCase(ggp2_inputs()$y), "_", " "
          )
        ) +
        ggplot2::theme_minimal() +
        ggplot2::theme(legend.position = "bottom")

      expect_true(object = ggplot2::is.ggplot(p_theme))
    })
  })
})
```

# Example: snapshot tests

```{r}
#| label: git_box_08_tests-snapshots
#| echo: false
#| results: asis
#| eval: true
git_margin_box(
  fig_pw = '75%', 
  branch = "08_tests-snapshots", 
  repo = 'shinyAppPkg')
```



If the expected output we're interesting in testing is cumbersome to describe programmatically, we can consider using a snapshot tests. Examples of this include UI elements (which are mostly HTML created by Shiny's UI layout and input/output functions) and data visualizations.[^tests-ui-tests]

Writing tests for graph outputs can be difficult because the "correctness" of a graph is somewhat subjective and requires human judgment.

In cases like this, a snapshot test might be warranted. The [`vdiffr`](https://vdiffr.r-lib.org/) package allows us to perform a 'visual unit test' by saving by capturing the expected output as a snapshot that we can compare with future versions.

[^tests-ui-tests]: Mastering Shiny covers [creatng a snapshot file](https://mastering-shiny.org/scaling-testing.html#user-interface-functions) to test UI elements, but also notes this is probably not the best approach.

### [`vdiffr`]{style="font-size: 1.05em;"}

The `expect_doppelganger()` function from `vdiffr` is designed specifically to work with ['graphical plots'](https://vdiffr.r-lib.org/reference/expect_doppelganger.html). 

```{r}
#| eval: false 
#| code-fold: false
vdiffr::expect_doppelganger(
      title = "name of graph", 
      fig = # ...code to create graph...
  )
```

### Test logging

I like verbose test output, so I've created a `test_logger()` helper function that let's me give more context and information for each test:

```{r}
#| code-fold: false
#| eval: true
#| code-summary: 'show/hide test_logger() helper' 
# test logger helper
test_logger <- function(start = NULL, end = NULL, msg) {
  if (is.null(start) & is.null(end)) {
    cat("\n")
    logger::log_info("\n[{Sys.time()}| {msg}]")
  } else if (!is.null(start) & is.null(end)) {
    cat("\n")
    logger::log_info("\n[ START | {start} = {msg}]")
  } else if (is.null(start) & !is.null(end)) {
    cat("\n")
    logger::log_info("\n[ END   | {end} = {msg}]")
  } else {
    cat("\n")
    logger::log_info("\n[ START | {start} = {msg}]")
    cat("\n")
    logger::log_info("\n[ END   | {end} = {msg}]")
  }
}
```

`test_logger()` can be used to 'log' the `start` and `end` of each test, and it includes a message argument (`msg`) I use to match the test context.[^tests-logger]

[^tests-logger]: If you like verbose logging outputs, check out the [`logger` package](https://daroczig.github.io/logger/) 

In this test, I'll use `testthat::describe()` to list the feature from the traceability matrix (**FE1**), followed by a `testthat::it()` with each functional requirement the snapshot will capture.

```{r}
#| label: co_box_trace_matrix_feat_funct_snaps
#| echo: false
#| code-fold: false
#| include: true
trace_matrix <- tibble::tibble(
  `Feature Requirement` = c(
    "FE1: interactive scatter plot (two data sources, drop-down variable options)",
    NA_character_,
    NA_character_,
    NA_character_
  ),
  `Functional Requirements` = c(
    "FR1: data source",
    "FR2: user-input updating",
    "FR3: color-coded data points",
    "FR4: plot axis, legend & title"
  )
) 
trace_matrix |> 
  gt::gt(auto_align = TRUE) |> 
  gt::sub_missing(
  columns = gt::everything(),
  rows = gt::everything(),
  missing_text = "-"
)
```

```{r}
#| eval: false 
#| code-fold: false
testthat::describe("FE1: interactive scatter plot", code = { # <1>
  
  testthat::it("Has color (FR3) & axis/legend/title (FR4)", code = { # <2>

    test_logger(               # <3>
      start = "T3",            # <3>
      msg = "Tests FR3 & FR4"  # <3>
    )                          # <3>
    
    vdiffr::expect_doppelganger( # <4>
      title = "scatter_plot() + theme", # <4>
      fig = scatter_plot(movies, # <4>
        x_var = var_inputs()$x, # <4>
        y_var = var_inputs()$y, # <4>
        col_var = var_inputs()$z, # <4>
        alpha_var = var_inputs()$alpha, # <4>
        size_var = var_inputs()$size # <4>
      ) + # <4>
        ggplot2::labs( # <4>
          title = var_inputs()$plot_title, # <4>
          x = stringr::str_replace_all( # <4>
                tools::toTitleCase( # <4>
                  var_inputs()$x), "_", " "), # <4>
          y = stringr::str_replace_all( # <4>
                tools::toTitleCase( # <4>
                  var_inputs()$y), "_", " ") # <4>
        ) + # <4>
        ggplot2::theme_minimal() + # <4>
        ggplot2::theme(legend.position = "bottom") # <4>
    ) # <4>
    
    test_logger(               # <5>
      end = "T3",              # <5>
      msg = "Tests FR3 & FR4"  # <5>
    )                          # <5>
    
  })
  
})
```
1. Feature description  
2. Non-technical language for functional requirements  
3. Test logger (`start`)   
4. Snapshot with `movies` and `var_inputs()`  
5. Test logger (`end`)   


The test results have the results from `test_logger()`, and we see warning that the snapshot has been saved in the `tests/testthat/_snaps/` folder: 

```{verbatim}
#| eval: false 
#| code-fold: false
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 0 ]
INFO [2023-09-26 23:16:52] [ START | T1 = Tests FR1]
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ]
INFO [2023-09-26 23:16:52] [ END   | T1 = Tests FR1]

INFO [2023-09-26 23:16:52] [ START | T2 = Tests FR1]
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 2 ]
INFO [2023-09-26 23:16:52] [ END   | T2 = Tests FR1]

INFO [2023-09-26 23:16:52] [ START | T3 = Tests FR3 & FR4]
[ FAIL 0 | WARN 1 | SKIP 0 | PASS 3 ]
INFO [2023-09-26 23:16:52] [ END   | T3 = Tests FR3 & FR4]


─ Warning (test-scatter_plot.R:54:5): Has color (FR3) & axis/legend/title (FR4) ─
Adding new file snapshot: 'tests/testthat/_snaps/scatter-plot-theme.svg'
[ FAIL 0 | WARN 1 | SKIP 0 | PASS 3 ]
```

On subsequent runs, this warning will disappear (as long as there are no changes to the `scatter-plot-theme.svg` file).

We can also update the traceability matrix with the tests used to verify the functional requirements:

```{r}
#| label: co_box_trace_matrix_add_t1_and_t2
#| echo: false
#| code-fold: false
tibble::tibble(
  `User Specification` = c(
    "US1: scatter plot data visualization",
    NA_character_,
    NA_character_,
    NA_character_
  ),
  `Feature Requirement` = c(
    "FE1: interactive scatter plot (two data sources, drop-down variable options)",
    NA_character_,
    NA_character_,
    NA_character_
  ),
  `Functional Requirements` = c(
    "FR1: data source",
    "FR2: user-input updating",
    "FR3: color-coded data points",
    "FR4: plot axis and legend"
  ),
  Tests = c(
    "T1 & T2",
    NA_character_,
    "T3",
    "T3"
  )
) |> 
  gt::gt(auto_align = TRUE) |> 
  gt::sub_missing(
  columns = gt::everything(),
  rows = gt::everything(),
  missing_text = "-"
)
```

# Example: testing modules


<!--

Integration tests verify that multiple components work together. If
you find yourself instantiating multiple objects that interact with each
other in a test, you’re probably writing an integration test. Integration
tests are often slower to execute and require a more elaborate setup than
unit tests. Developers run integration tests less frequently, so the feedback
loop is longer. These tests can flush out problems that are difficult
to identify by testing standalone units individually.

--> 

The BDD functions also allow us to combine tests for *reactive interactions* with `testServer()`, which means we can include tests to verify the inputs, outputs, and returned values from module server functions. 

For example, functional requirement #2 (**FR2**) is a user created behavior. 

To confirm the user-inputs are updating in the application, we need to test three changes:

1. An initial `NULL` value in `mod_var_input_server()` 
2. The returned values from the `mod_var_input_server()`  
3. The final value collected in `mod_scatter_display_server()`

list of graph inputs from `mod_var_input_server()`, we can build the list of inputs, then pass identical values to `session$setInputs()` and confirm the returned object with `session$returned()`:

```{r}
#| eval: false 
#| include: true
#| code-fold: false
shiny::testServer(app = mod_var_input_server, expr = {
  test_vals <- list(
                  y = "audience_score",
                  x = "imdb_rating",
                  z = "genre",
                  alpha = 0.5,
                  size = 2,
                  plot_title = "example title"
                )
  session$setInputs(
                  y = "audience_score",
                  x = "imdb_rating",
                  z = "genre",
                  alpha = 0.5,
                  size = 2,
                  plot_title = "example title"
                )
  test_logger(start = "returned(var_input)", msg = "var_input structure")
  testthat::expect_equal(
    object = session$returned(),
    expected = test_vals
  )
})
```

This confirms the structure of the returned object from `mod_var_input_server()`.

### [`args = list()`]{style="font-size: 0.95em;"}

Now that we've confirmed the returned values from `mod_var_input_server()` are in a list, we want to make sure it's read correctly by the `var_inputs` argument in `mod_scatter_display_server()`. I've included the `movies_server()` function below refresh our memory of how this *should* work:[^tests-12]

```{r}
#| eval: false 
#| code-fold: false
movies_server <- function(input, output, session) {

      selected_vars <- mod_var_input_server("vars")

      mod_scatter_display_server("plot", var_inputs = selected_vars)
      
}
```

[^tests-12]: `selected_vars` are the reactive plot values returned from `mod_var_input_server()` we confirmed `test-mod_var_input_server.R`.

When we pass `selected_vars` to the `var_inputs` argument of `mod_scatter_display_server()`, we're not passing the returned values (this is why we don't need the parentheses). We're calling on the method (or function) created by `reactive()`.

If we pause execution with the debugger,[^tests-13] we can see the difference between calling `selected_vars` and `selected_vars()`:

[^tests-13]: We'll cover using `browser()` and the IDE's debugger in a future chapter.

::: {layout="[49, -2, 49]"}

```{verbatim}
#| eval: false 
#| code-fold: false
Browse[1]> selected_vars
reactive({
    list(
      y = input$y, 
      x = input$x, 
      z = input$z, 
      alpha = input$alpha, 
      size = input$size, 
      plot_title = input$plot_title
      )
})
```

```{verbatim}
#| eval: false 
#| code-fold: false
Browse[1]> selected_vars()
$y
[1] "audience_score"

$x
[1] "imdb_rating"

$z
[1] "mpaa_rating"

$alpha
[1] 0.5

$size
[1] 2

$plot_title
[1] ""
```

:::

This distinction becomes important when we're testing the communication between module server functions. 

```{r}
#| eval: false 
#| code-fold: false
test_vals <- list(x = "audience_score",
                  y = "imdb_rating",
                  z = "mpaa_rating",
                  alpha = 0.5,
                  size = 2,
                  plot_title = "Test title case")
shiny::testServer(
  app = mod_scatter_display_server,
  args = list(
    var_inputs = shiny::reactive(test_vals)
  ), expr = {
    
    test_logger(start = "mod_scatter_display_server", msg = "is.reactive(inputs())")
    
    expect_true(object = is.reactive(inputs))
    
    test_logger(end = "mod_scatter_display_server", msg = "is.reactive(inputs())")
    
  }
)
```


<!--

# Testing app specifications

The `testthat` BDD functions work well in shiny app-packages because testing will often span multiple layers of code: 

  -   **Utility Functions**: handle things like database connections, data processing, visualizations, statistical analysis, etc.

  -   **App UI/module UI functions**: control the design and layout of the application.

  -   **App server/module server functions**: contains the back-end logic and controls how the app behaves in response to user input.


For example, the user specifications will usually include functions for both business logic and shiny UI/server functions, but we want to separate the verification for these features in separate tests. We can use the contents of our traceability matrix to outline the `describe()` and `it()` sections of our tests.

```{r}
#| eval: false 
#| code-fold: false
#| include: false
testthat::describe("scatter_plot() works", code = {
  testthat::it(description = "is.ggplot", code = {
    test_logger(start = "scatter_plot()", msg = "movies is.ggplot")
    # testthat::test_that(desc = "scatter_plot() works", code = {
      p <- scatter_plot(movies,
        x_var = inputs()$x,
        y_var = inputs()$y,
        col_var = inputs()$z,
        alpha_var = inputs()$alpha,
        size_var = inputs()$size)
        expect_true(object = ggplot2::is.ggplot(p))
    test_logger(end = "scatter_plot()", msg = "movies is.ggplot")
  })
  testthat::it(description = "snapshot of scatter_plot()", code = {
    ggp2_movies <- readRDS(test_path("fixtures", "ggp2_movies.rds"))
    test_logger(start = "scatter_plot()", msg = "ggp2_movies scatter_plot() snapshot")
    vdiffr::expect_doppelganger(
      title = "scatter_plot()",
      fig = scatter_plot(ggp2_movies,
        x_var = ggp2_inputs()$x,
        y_var = ggp2_inputs()$y,
        col_var = ggp2_inputs()$z,
        alpha_var = ggp2_inputs()$alpha,
        size_var = ggp2_inputs()$size
        )
      )
    test_logger(end = "scatter_plot()", msg = "ggp2_movies scatter_plot() snapshot")
  })
  
  testthat::it(description = "snapshot of scatter_plot() and labels", code = {
    ggp2_movies <- readRDS(test_path("fixtures", "ggp2_movies.rds"))
    test_logger(start = "scatter_plot()", msg = "ggp2_movies scatter_plot() + labs")
    vdiffr::expect_doppelganger(
      title = "scatter_plot() + labels",
      fig = scatter_plot(ggp2_movies,
        x_var = ggp2_inputs()$x,
        y_var = ggp2_inputs()$y,
        col_var = ggp2_inputs()$z,
        alpha_var = ggp2_inputs()$alpha,
        size_var = ggp2_inputs()$size
      ) + 
        ggplot2::labs(
          title = ggp2_inputs()$plot_title,
          x = stringr::str_replace_all(tools::toTitleCase(ggp2_inputs()$x), "_", " "),
          y = stringr::str_replace_all(tools::toTitleCase(ggp2_inputs()$y), "_", " ")
        )
      )
    test_logger(end = "scatter_plot()", msg = "ggp2_movies scatter_plot() + labs")
  }) 
  
  testthat::it(description = "snapshot of scatter_plot() and theme", code = {
   ggp2_movies <- readRDS(test_path("fixtures", "ggp2_movies.rds"))
   test_logger(start = "scatter_plot()", msg = "ggp2_movies scatter_plot() + theme")
    vdiffr::expect_doppelganger(
      title = "scatter_plot() + theme",
      fig = scatter_plot(ggp2_movies,
        x_var = ggp2_inputs()$x,
        y_var = ggp2_inputs()$y,
        col_var = ggp2_inputs()$z,
        alpha_var = ggp2_inputs()$alpha,
        size_var = ggp2_inputs()$size
      ) + 
        ggplot2::labs(
          title = ggp2_inputs()$plot_title,
          x = stringr::str_replace_all(tools::toTitleCase(ggp2_inputs()$x), "_", " "),
          y = stringr::str_replace_all(tools::toTitleCase(ggp2_inputs()$y), "_", " ")
        ) +
        ggplot2::theme_minimal() +
        ggplot2::theme(legend.position = "bottom")
      )
    test_logger(end = "scatter_plot()", msg = "ggp2_movies scatter_plot() + theme")
  }) 
})

```


`expect_doppelganger()` can be dropped in `test_that()` like any other expectation. 

Below is an example test comparing a graph outputs from `ggplot2` against the custom `scatter_plot()` utility function in `shinyAppPkg`. Notice I've loaded the `ggp2_movies.rds` data[^tests-test_path] and included the `test_logger()` helper with `start` and `end` messages (before and after the test code).

[^tests-test_path]: Accessing test files is made easier with `testtthat::test_path()`

```{r}
#| eval: false 
#| code-fold: false
ggp2_movies <- readRDS(test_path("fixtures", "ggp2_movies.rds")) # <1>
test_logger(start = "scatter_plot()", msg = "ggplot2movies::movies snapshot") # <2>
vdiffr::expect_doppelganger(
  title = "scatter_plot() graph",
  fig = scatter_plot(ggp2_movies,
    x_var = ggp2_inputs()$x, # <3>
    y_var = ggp2_inputs()$y, # <3>
    col_var = ggp2_inputs()$z, # <3>
    alpha_var = ggp2_inputs()$alpha, # <3>
    size_var = ggp2_inputs()$size # <3>
  ) + 
    ggplot2::labs(
      title = ggp2_inputs()$plot_title, # <3>
      x = stringr::str_replace_all(tools::toTitleCase(ggp2_inputs()$x), "_", " "), # <3>
      y = stringr::str_replace_all(tools::toTitleCase(ggp2_inputs()$y), "_", " ") # <3>
    ) +
    ggplot2::theme_minimal() +
    ggplot2::theme(legend.position = "bottom")
  )
test_logger(end = "scatter_plot()", msg = "ggplot2movies::movies snapshot") # <4>
```
1. Load test data from `tests/testthat/fixtures`  
2. Call `test_logger()` at beginning of test  
3. Create inputs using `ggp2_inputs()` function in `tests/testthat/helper.R`
4. Call `test_logger()` at end of test  

When we initially run the test it passes, but with a warning that tells us the baseline snapshot was saved in `tests/testthat/_snaps/`:

```{verbatim}
#| eval: false 
#| code-fold: false
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 0 ]
i [ START | 2023-09-23 14:06:20 | scatter_plot() = movies is.ggplot]
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ]
i [ END   | 2023-09-23 14:06:20 | scatter_plot() = movies is.ggplot]

i [ START | 2023-09-23 14:06:20 | scatter_plot() = ggplot2movies::movies snapshot]
[ FAIL 0 | WARN 1 | SKIP 0 | PASS 2 ]
i [ END   | 2023-09-23 14:06:20 | scatter_plot() = ggplot2movies::movies snapshot]


── Warning (test-scatter_plot.R:27:5): snapshot ────────────────────────────────
Adding new file snapshot: 'tests/testthat/_snaps/scatter-plot-graph.svg'
```

After the initial test run, if we view the `tests/` folder, we can see the new `tests/testthat/_snaps` is created:

```{bash}
#| eval: false 
#| code-fold: false
tests/
  ├── testthat/
  │   ├── _snaps/
  │   │   └── scatter_plot/
  │   │       └── scatter-plot-graph.svg
  │   └── test-scatter_plot.R
  └── testthat.R

```

The `scatter-plot-graph.svg` file is our baseline comparison object, which is then used in future tests.

#### [`withr`]{style="font-size: 1.05em;"}

The date/time output from `test_logger()` doesn't provide the level of precision we want to monitor our unit tests (they're supposed to be fast). The R option to change this setting is `options(digits.secs)`, but we don't want to include this options in our test file (we just need it whenever we use `test_logger()`).[^tests-message2]

We can add `withr::local_options()` inside `test_logger()` to make sure this option only applies to the test environment it's called from. Subsequent tests will pass without a warning and provide a more precise date/time message:

```{verbatim}
#| eval: false 
#| code-fold: false
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 0 ]
i [ START | 2023-09-23 14:13:03.9792 | scatter_plot() = movies is.ggplot]
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ]
i [ END   | 2023-09-23 14:13:04.0036 | scatter_plot() = movies is.ggplot]

i [ START | 2023-09-23 14:13:04.0212 | scatter_plot() = ggplot2movies::movies snapshot]
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 2 ]
i [ END   | 2023-09-23 14:13:04.2443 | scatter_plot() = ggplot2movies::movies snapshot]
```

[^tests-message2]: This example is similar to the `message2()` function on the [`testthat` website](https://testthat.r-lib.org/articles/test-fixtures.html#test-fixtures), but calls `withr` *inside* the helper function.

Back in the **Console**, I can see the output from `Sys.time()` doesn't include the `test_logger()` option:

```{r}
#| eval: false 
#| code-fold: false
Sys.time()
```

```{verbatim}
#| eval: false 
#| code-fold: false
[1] "2023-09-23 14:16:30 PDT"
```



#### Snapshots are brittle

The term "brittle" in the context of testing refers to their susceptibility to changes (meaningful or not) that can produce false negatives (i.e., a test fails due to inconsequential changes in the graph) when comparing a new graph to the baseline image. 

If we compare the output from a custom plotting function like `scatter_plot()` against a graph built with analogous `ggplot2` code, we can see all the potential points of failure a test would have by passing both objects to `diffobj::diffObj()`:

```{r}
#| eval: false 
#| code-fold: false
ggp_graph <- ggplot2::ggplot(mtcars, 
              ggplot2::aes(x = mpg, y = disp)) + 
              ggplot2::geom_point(
                ggplot2::aes(color = cyl), 
                             alpha = 0.5, 
                             size = 3)
  
app_graph <- scatter_plot(mtcars, 
                  x_var = "mpg", 
                  y_var = "disp", 
                  col_var = "cyl", 
                  alpha_var = 0.5, 
                  size_var = 3)

diffobj::diffObj(ggp_graph, app_graph)
```


:::: {.column-page-inset-right}

:::{#fig-08_tests_diffobj_scatter_plot}

![`diffobj::diffObj()` on graph outputs](img/08_tests_diffobj_scatter_plot.png){#fig-08_tests_diffobj_scatter_plot width='100%' align='center'}

Graph objects are difficult to use as test objects 
:::

::::

The output shows us differences in the `mapping` and plot environment (`plot_env`), which we can assume *would* be different in the test (and the application), so using `testthat::expect_equal()` will fail.

Another option for using snapshots for testing is the `expect_snapshot_file()` function [^tests-10] but `expect_doppelganger()` is probably the better option for comparing graph outputs.

[^tests-10]: Follow the `expect_snapshot_file()` example from the [`testthat` documentation](https://testthat.r-lib.org/reference/expect_snapshot_file.html#ref-examples)




```{r}
#| eval: false 
#| include: false
# shiny::is.reactive(inputs()) ----------------------------------------------
shiny::testServer(
  app = mod_scatter_display_server,
  args = list(
    var_inputs =
      shiny::reactive(
        list(
          x = "audience_score",
          y = "imdb_rating",
          z = "mpaa_rating",
          alpha = 0.5,
          size = 2,
          plot_title = "Test title"
        )
      )
  ), expr = {
    test_logger(start = "mod_scatter_display_server", msg = "is.reactive(inputs())")
    expect_true(
      object = is.reactive(inputs)
    )
    test_logger(end = "mod_scatter_display_server", msg = "is.reactive(inputs())")
  }
)

# is.list(inputs()) ----------------------------------------------
shiny::testServer(
  app = mod_scatter_display_server,
  args = list(
    var_inputs =
      shiny::reactive(
        list(
          x = "audience_score",
          y = "imdb_rating",
          z = "mpaa_rating",
          alpha = 0.5,
          size = 2,
          plot_title = "test title case"
        )
      )
  ), expr = {
    test_logger(start = "mod_scatter_display_server", msg = "is.list(inputs())")
    expect_equal(
      object = inputs(),
      expected = list(
        x = "audience_score",
        y = "imdb_rating",
        z = "mpaa_rating",
        alpha = 0.5,
        size = 2,
        plot_title = "Test Title Case"
      )
    )
    test_logger(end = "mod_scatter_display_server", msg = "is.list(inputs())")
  }
)

# ggplot2::is.ggplot2() ---------------------------------------------------

shiny::testServer(
  app = mod_scatter_display_server,
  args = list(
    var_inputs =
      shiny::reactive(
        list(
          x = "audience_score",
          y = "imdb_rating",
          z = "mpaa_rating",
          alpha = 0.5,
          size = 2,
          plot_title = "Test title"
        )
      )
  ), expr = {
    test_logger(start = "mod_scatter_display_server", msg = "ggplot2::is.ggplot(plot)")
    plot <- scatter_plot(
      df = movies,
      x_var = inputs()$x,
      y_var = inputs()$y,
      col_var = inputs()$z,
      alpha_var = inputs()$alpha,
      size_var = inputs()$size
    ) +
      ggplot2::labs(
        title = inputs()$plot_title,
        x = stringr::str_replace_all(tools::toTitleCase(inputs()$x), "_", " "),
        y = stringr::str_replace_all(tools::toTitleCase(inputs()$y), "_", " ")
      ) +
      ggplot2::theme_minimal() +
      ggplot2::theme(legend.position = "bottom")
    ggplot2::is.ggplot(plot)
    test_logger(end = "mod_scatter_display_server", msg = "ggplot2::is.ggplot(plot)")
  }
)

# print(plot) ---------------------------------------------------
# shiny::testServer(
#   app = mod_scatter_display_server,
#   args = list(
#     var_inputs =
#       shiny::reactive(
#         list(
#           x = "audience_score",
#           y = "imdb_rating",
#           z = "mpaa_rating",
#           alpha = 0.5,
#           size = 2,
#           plot_title = "Test title"
#         )
#       )
#   ), expr = {
#     test_logger(start = "mod_scatter_display_server", msg = "print(plot)")
#     plot <- scatter_plot(
#       # data ----------------------------------------------------
#       df = movies,
#       x_var = inputs()$x,
#       y_var = inputs()$y,
#       col_var = inputs()$z,
#       alpha_var = inputs()$alpha,
#       size_var = inputs()$size
#     ) +
#       ggplot2::labs(
#         title = inputs()$plot_title,
#         x = stringr::str_replace_all(tools::toTitleCase(inputs()$x), "_", " "),
#         y = stringr::str_replace_all(tools::toTitleCase(inputs()$y), "_", " ")
#       ) +
#       ggplot2::theme_minimal() +
#       ggplot2::theme(legend.position = "bottom")
#     print(plot)
#     test_logger(end = "mod_scatter_display_server", msg = "print(plot)")
#   }
# )
# 💅 Your tests are beautiful 💅
```

-->

<!--
> Data Flow: Test how well the different units work together by simulating a complete data flow.

> Database Interactions: If your package interacts with a database, write tests to ensure that read/write operations are working as expected.

> API Calls: If your package makes API calls, use package like httptest to mock API responses and test the integration.
-->





# System/end-to-end tests

<!--
> Manual Testing: Initially, you may have to manually interact with the Shiny app to ensure it behaves as expected.

> Automated Browser Testing: Use tools like shinytest to automate browser-based testing for the Shiny application. shinytest captures the state of the application and allows you to compare it to expected states.

> User Scenarios: Script typical user interaction scenarios to ensure that the whole system, including front-end and back-end, work seamlessly.
-->


## [`shinytest2`]{style="font-size: 1.05em;"}

# Test coverage 

## [`covr`]{style="font-size: 1.05em;"}

## [`covrpage`]{style="font-size: 1.05em;"}

# Continuous Integration (CI)

<!--

> Consider setting up a CI pipeline using services like GitHub Actions, GitLab CI, or Travis CI to automate the running of these tests.

> By implementing these various levels of tests, you can build confidence that your R package and Shiny app are robust, reliable, and ready for deployment.

-->

<!--
Testing an R package with a Shiny application can be challenging due to the various components and layers of interactivity. Here's a general strategy to approach this:
-->


# Comparisons

Comparisons are the backbone of testing. Exploring the mechanics of how tests perform these comparisons (i.e., the underlying package(s)) can save you from surprising results. 

For example, `testthat::expect_equal()` compares whatever is passed to the `observed` and `expected` arguments with the [`waldo` package](https://www.tidyverse.org/blog/2021/08/waldo-0-3-0/), with some help from [`diffobj`](https://github.com/brodieG/diffobj).

```{r}
#| eval: true 
#| code-fold: false
#| collapse: true 
#| message: false 
#| warning: false
library(waldo)
library(diffobj)
library(tibble)
```

## [`waldo`]{style="font-size: 1.05em;"}

If you'd like a preview of a comparison before writing a formal test, you can pass the your `observed` and `expected` objects to `waldo::compare()`[^waldo-compare-args] 

[^waldo-compare-args]: Be mindful of the difference in arguments between expectation functions (i.e., `expect_equal()`) and `waldo::compare()`

```{r}
#| eval: true 
#| include: false
old <- tibble(
  chr = LETTERS[2:4],
  num = as.double(c(1.0, 2.0, 3.0)),
  fct = factor(c("low", "med", "high"), 
        levels = c("low", "med", "high"), 
        labels = c("L", "M", "H"),
        ordered = TRUE)
)
new <- data.frame(
  CHR = LETTERS[2:4],
  num = as.integer(c(1, 2, 3)),
  fct = factor(c("low", "med", "high"),
        levels = c("low", "med", "high"),
        labels = c("low", "med", "high"))
)
```

For example, suppose we have two objects: `old` and `new`

```{r}
#| eval: true 
#| code-fold: false
#| collapse: true
old
```

```{r}
#| eval: true 
#| code-fold: false
#| collapse: true
new
```

The outputs below are example outputs from `waldo::compare()`:

::: {layout="[[30,70]]"}

```{r}
#| eval: true 
#| code-fold: false
#| collapse: true
compare(
  x = old, # <1>
  y = old) # <1> 
```
1. Comparing identical objects


```{r}
#| eval: true 
#| code-fold: false
#| collapse: true
compare(
  x = old, # <1>
  y = new) # <1>
```
1. Comparing different objects

:::

`compare()` displays the differences in classes, names, and any individual value differences. 

## [`diffobj`]{style="font-size: 1.05em;"}

If you're using Posit Workbench, the [`diffobj` package](https://github.com/brodieG/diffobj) has a colorful display for making comparisons in the IDE. 

The differences can be displayed vertically with `diffobj::diffObj()`:

::: {layout="[[30,70]]"}

```{r}
#| eval: false 
#| code-fold: false
diffObj(
  old, 
  new)
```

![Viewer ouput from `diffobj::diffObj()`](img/08_tests_diffobj.png){#fig-08_tests_diffobj width='85%' align='center'}

:::

If you want to view the structure (`str()`) differences, you can use `diffobj::diffStr()`:

::: {layout="[[30, 70]]"}

```{r}
#| eval: false 
#| code-fold: false
diffStr(
  old, 
  new)
```

![Viewer ouput from `diffobj::diffStr()`](img/08_tests_diffstr.png){#fig-08_tests_diffobj width='100%' align='center'}
:::

After viewing the `old` vs `new` comparisons with `waldo` and `diffobj`, you should notice similarities and differences in the results from `testthat`[^compare-tolerance]

[^compare-tolerance]: The results from `testthat` don't include the differences between `old$num` and `new$num`. This is due to the `tolerance` argument, which can be adjusted in both functions.

```{verbatim}
#| eval: false 
#| code-fold: false
[ FAIL 1 | WARN 0 | SKIP 0 | PASS 0 ]

── Failure (test-old_vs_new.R:17:3): old vs. new ───────────────────────────────
`new` (`actual`) not equal to `old` (`expected`).

`class(actual)`:   "data.frame"                   
`class(expected)`: "tbl_df"     "tbl" "data.frame"

`names(actual)`:   "CHR" "num" "fct"
`names(expected)`: "chr" "num" "fct"

`actual$CHR` is a character vector ('B', 'C', 'D')
`expected$CHR` is absent

`class(actual$fct)`:   "factor"          
`class(expected$fct)`: "ordered" "factor"

`levels(actual$fct)`:   "low" "med" "high"
`levels(expected$fct)`: "L"   "M"   "H"   

`actual$chr` is absent
`expected$chr` is a character vector ('B', 'C', 'D')
[ FAIL 1 | WARN 0 | SKIP 0 | PASS 0 ]
```

