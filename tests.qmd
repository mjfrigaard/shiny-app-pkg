# Tests 

```{r}
#| eval: true 
#| echo: false 
#| include: false
source("_common.R")
library(testthat)
library(gt)
```

```{r}
#| label: co_box_dev
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "y",
  header = "Caution!",
  contents = "This section is currently being revised. Thank you for your patience."
)
```

This chapter will cover the differences between user specifications, feature requirements, and functional requirements. I’ll introduce how you can use `testthat`s behavior-driven development (BDD)[^tests-bdd] and a traceability matrix to map each functional requirement to a specific test, ensuring that all user's needs are met (and the app's features are implemented correctly).

[^tests-bdd]: Read more about [behavior-driven development](https://en.wikipedia.org/wiki/Behavior-driven_development)

The code chunk below will load the necessary testing packages.

```{r}
#| eval: false
#| code-fold: false
install.packages(c("testthat", "shinytest2", "covr"))
library(testthat)
library(shinytest2)
library(covr)
```

(*If you're using `devtools`, you won't have to worry about installing `testthat` and `covr`*)

## What should I test?


When you begin developing a Shiny app-package, you’ll be faced with two questions:

1. What should I test?  
2. How should I test it?   

This chapter focuses on *what to test* and *why to test it*. I won't be covering *how to write tests*, because plenty of those resources exist.[^tests-intro-unit-tests] [^tests-intro-shinytest2] [^tests-intro-ms-testing] The only exception being some `testServer()` tricks I've learned for testing modules.[^tests-intro-testserver]

[^tests-intro-unit-tests]: Unit tests are covered extensively in [R Packages, 2ed](https://r-pkgs.org/testing-basics.html) and the [`testthat` documentation](https://testthat.r-lib.org/index.html)

[^tests-intro-shinytest2]: `shinytest2` has [excellent documentation](https://rstudio.github.io/shinytest2/) (and [videos](https://www.youtube.com/watch?v=Gucwz865aqQ)), and I highly recommend reading through those resources.

[^tests-intro-ms-testing]: Mastering shiny dedicates an entire [Chapter to Testing](https://mastering-shiny.org/scaling-testing.html), which covers [unit tests](https://mastering-shiny.org/scaling-testing.html#basic-structure) and [`testServer()`](https://mastering-shiny.org/scaling-testing.html#testing-reactivity), and also includes some tips for using JavaScript with [`shinytest`](https://mastering-shiny.org/scaling-testing.html#testing-javascript) (not to be confused with [`shinytest2`](https://rstudio.github.io/shinytest2/)) 
[^tests-intro-testserver]: The [`testServer()`](https://shiny.posit.co/r/articles/improve/server-function-testing/) documentation is sparse, so I'll provide a few tips and tricks I've learned for testing module server functions.

### Where to look

Applications typically have some accompanying resources to address what *should* be tested, the most common source being a software requirements specification (SRS) document.[^tests-srs] The SRS breaks down an application's intended purpose (i.e., the problem it's designed to solve) into three general areas: user specifications, feature requirements, and functional requirements: 

-   **The user specifications capture the needs and expectations of the end-user.** These are usually non-technical and focused on the "why" and the "what" of the application.

-   **The feature requirements describe the high-level capabilities of the application.** Features are defined early in the life of a project and often become the talking points during discussions between stakeholders and developers. Features can be used for scoping and prioritization, and may be comprised of various functional (and sometimes non-functional) requirements.

-   **Functional requirements are the testable, specific actions, inputs, and outputs.** Functional requirements provide the technical details of how the features will be implemented, and a single feature can give rise to multiple functional requirements.

[^tests-srs]: Read more about what goes in the [Software Requirements Specification](https://en.wikipedia.org/wiki/Software_requirements_specification)

These guidelines direct the development process, albeit from slightly different perspectives. Understanding the interplay between user specifications, features, and functional requirements is essential for developers to know if an application meets the technical standards and satisfies the user’s needs.

I'll use `shinyAppPkg` to illustrate a brief example of each in the sections below:

### User Specifications

**User specifications** are what the end-user (in this case, a film analyst) wants to achieve with the application.[^tests-specs-user-specs]

[^tests-specs-user-specs]: User Specifications are sometimes referred to as "user stories," "use cases," or "general requirements"

```{r}
#| label: co_box_user_specs
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "b", fold = FALSE, look = "minimal", 
  size = '1.10', header = "Scatter plot user specification",
  contents = 
" 

**US1**: '*As an film analyst, I want to view an interactive scatter plot in a dashboard that consolidates movie reviews from multiple sources so that I can compare and analyze trends and outliers in movie ratings quickly.*'
  
  ")
```

### Feature Requirements

The **feature requirement** translates the end user's expectation into specific language describing an application capability (i.e., display a scatter plot), phrased to satisfy a specific end-user need outlining in the specifications.[^tests-specs-feat]

[^tests-specs-feat]: "Feature requirements" and "functional requirements" are sometimes used interchangeably, but they refer to different aspects of the software. **Feature requirements** are the desired high-level characteristics the application *should* have, and often capture a collection of smaller functionalities (which are broken down into specific functional requirements).

```{r}
#| label: co_box_feat_reqs
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "b", fold = FALSE, look = "minimal",
  size = '1.10', header = "Scatter plot feature requirements",
  contents = 
" 

**FE1**: '*Given that movie reviews are available on multiple websites, when the user selects a rating metric from the application display, the interactive scatter plot should allow for comparisons from at least two data sources (i.e., IMDb, Rotten Tomatoes), and include options for selecting other variables of interest (i.e., audience scores, runtime, etc.).*'
  
  ")
```

### Functional Requirements

**Functional requirements** are written for the developer and provide technical details on *how* the feature (i.e., the scatter plot) should behave and *what* it needs to do (they're where the end-users' expectations come into direct contact with code).[^tests-specs-func]

[^tests-specs-func]: **Functional requirements** are precise, measurable, and testable. 

```{r}
#| label: co_box_fun_reqs
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "b", fold = FALSE, look = "minimal",
  size = '1.10', header = "Scatter plot functional requirements",
  contents = 
" 
  - **FR1**: The system will display movie reviews from IMDb and Rotten Tomatoes collected from their respective APIs.

  - **FR2**: The scatter plot will be displayed on the dashboard and updated with new user inputs.

  - **FR3**: Each data point on the scatter plot will represent a movie and be color-coded based on the following categories: MPAA ratings, genre, title type, critics rating, and audience rating.
  
  - **FR4**: The scatter plot will have labeled axes, a legend to differentiate between data sources, and a customizable title.
  
  ")
```

### Traceability matrix

A traceability matrix is a table that ‘traces’ the user specifications to features and functional requirements (and the tests they give rise to) to verify that the application has been developed correctly. Package vignettes are a great place to store a draft of the traceability matrix, and they can be quickly created using `usethis::use_vignette()`:

```{r}
#| label: git_box_08_tests-specs
#| echo: false
#| results: asis
#| eval: true
git_margin_box(
  fig_pw = '75%', 
  branch = "10a_tests-specs", 
  repo = 'shinyAppPkg')
```

```{r}
#| eval: false 
#| code-fold: false
usethis::use_vignette("test-specs")
```

Adding our first vignette to the `vignettes/` folder does the following:

- [x]   Adds the `knitr` and `rmarkdown` packages to the `Suggests` field in `DESCRIPTION`[^test-specs-suggests]

    ```bash
    Suggests: 
        knitr,
        rmarkdown
    ```

- [x]   Adds `knitr` to the `VignetteBuilder` field[^test-specs-vignette-builder]

    ```bash
    VignetteBuilder: knitr
    ```

- [x]   Adds `inst/doc` to `.gitignore` and `*.html`, `*.R` to `vignettes/.gitignore`[^test-specs-inst-docs]


[^test-specs-suggests]: We briefly covered the `Suggests` field in [Dependencies](https://mjfrigaard.github.io/shinyap/dependencies.html), but in this case it specifically applies to "*packages that are not necessarily needed. This includes packages used only in examples, tests or vignettes...*" - [Writing R Extensions, Package Dependencies](https://cran.r-project.org/doc/manuals/R-exts.html#Package-Dependencies)

[^test-specs-vignette-builder]: The [documentation](https://cran.r-project.org/doc/manuals/R-exts.html#The-DESCRIPTION-file) on `VignetteBuilder` gives a great description of why `knitr` and `rmarkdown` below in `Suggests`, "*Note that if, for example, a vignette has engine `knitr::rmarkdown`, then `knitr` provides the engine but both `knitr` and `rmarkdown` are needed for using it, so both these packages need to be in the `VignetteBuilder` field and at least suggested (as `rmarkdown` is only suggested by `knitr`, and hence not available automatically along with it).*"

[^test-specs-inst-docs]: We covered the `inst/` folder in the [External Files chapter](https://mjfrigaard.github.io/shinyap/), and you might recall that `docs/` was one of the folders we shouldn't create inside `inst/`.

We want to use the traceability matrix to ensure the tests cover all the functionalities (i.e., the code) and address the user specifications *and* that every user need corresponds to a functionality that’s been tested. 

#### User specs & features 

We'll start with the user specifications. The language used for these descriptions is non-technical but should provide a basis for deriving the more technical (but still high-level) feature requirements.[^tests-visual-markdown]

[^tests-visual-markdown]: When building tables in vignettes, I highly suggest using the [Visual Markdown mode](https://rstudio.github.io/visual-markdown-editing/) (especially when building tables).

```{r}
#| label: co_box_trace_matrix_specs_features
#| echo: false
#| code-fold: false
tibble::tibble(
  `User Specification` = c(
    "US1: scatter plot data visualization"
  ),
  `Feature Requirement` = c(
    "FE1: interactive scatter plot (two data sources, drop-down variable options)"
  ),
  `Functional Requirements` = c(
    NA_character_
  )
) |> 
  gt::gt(auto_align = TRUE) |> 
  gt::sub_missing(
  columns = gt::everything(),
  rows = gt::everything(),
  missing_text = "-"
)
```

#### Functional requirements 

A single feature often produces multiple functional requirements.

```{r}
#| label: co_box_trace_matrix_specs_features_function_01
#| echo: false
#| code-fold: false
#| include: true
trace_matrix <- tibble::tibble(
  `User Specification` = c(
    "US1: scatter plot data visualization",
    NA_character_,
    NA_character_,
    NA_character_
  ),
  `Feature Requirement` = c(
    "FE1: interactive scatter plot (two data sources, drop-down variable options)",
    NA_character_,
    NA_character_,
    NA_character_
  ),
  `Functional Requirements` = c(
    "FR1: data source",
    "FR2: user-input updating",
    "FR3: color-coded data points",
    "FR4: plot axis, legend & title"
  )
) 
trace_matrix |> 
  gt::gt(auto_align = TRUE) |> 
  gt::sub_missing(
  columns = gt::everything(),
  rows = gt::everything(),
  missing_text = "-"
)
```

#### Tests

After translating the user needs into technical requirements, we can quickly identify what needs to be tested.

```{r}
#| label: co_box_trace_matrix_tests
#| echo: false
#| code-fold: false
#| include: true
trace_matrix |> 
  tibble::add_column(Tests = 
      paste0("T", 1:nrow(trace_matrix)), .after = 3) |>
  gt::gt() |> 
  gt::sub_missing(
  columns = gt::everything(),
  rows = gt::everything(),
  missing_text = "-"
)
```


The matrix allows us to ensure:

1. The user specifications have accompanying feature requirements.

2. Each feature has been broken down into precise, measurable, and testable functional requirements.

3. Tests have been written for each functional requirement.

If you'd like, you can include additional columns to track whether the test has passed:

```{r}
#| label: co_box_trace_matrix_add_test_status
#| echo: false
#| code-fold: false
trace_matrix |> 
  tibble::add_column(Tests = 
      paste0("T", 1:nrow(trace_matrix)), .after = 3) |> 
  tibble::add_column(`Test Status` = 
      paste0("FR", 1:nrow(trace_matrix), " test status: "), .after = 4) |> 
  gt::gt() |> 
  gt::sub_missing(
  columns = gt::everything(),
  rows = gt::everything(),
  missing_text = "-"
)
```


Vignettes are a great place to store this information because they are self-contained and travel with the package whenever the code is updated.[^test-specs-7]

[^test-specs-7]: Documenting the traceability matrix in vignettes are great for developers, but it's also a good idea use an issue-tracking system with version control, like GitHub Projects or Azure DevOps.

The following section covers setting up tests with `testthat` and the structure of unit tests.

## [`testthat`]{style="font-size: 1.05em;"} framework

Testing shiny app-packages poses some unique challenges. Shiny functions are written in the context of its reactive model,[^tests-shiny-reactive] so some standard testing techniques and methods for regular R packages don’t directly apply. Fortunately, the infrastructure for storing and running tests in app-packages is identical to a standard R package.

[^tests-shiny-reactive]: The ['Reactivity - An overview'](https://shiny.posit.co/r/articles/build/reactivity-overview/) article gives an excellent description (and mental module) of reactive programming.

Multiple strategies exist for writing tests. For example, if you've adopted test-driven development (TDD),[^tests-tdd] you'll write tests before developing utility functions, modules, or a standalone app function.

Regardless of the testing strategy you choose, I recommend setting up the testing infrastructure in our app-package with the [`testthat` package](https://testthat.r-lib.org/):

[^tests-tdd]: Read more about [Test-driven development](https://en.wikipedia.org/wiki/Test-driven_development) 

### [`use_testthat()`]{style="font-size: 1.05em;"}

In packages using `devtools`, the testing infrastructure can be built with `usethis::use_testthat()`:

```{r}
#| eval: false
#| code-fold: false
usethis::use_testthat()
```

- [x]   In the `DESCRIPTION` file, `testthat (>= 3.0.0)` is listed under `Suggests` and `Config/testthat/edition: 3` is used to specify the `testthat` edition

```{verbatim}
#| eval: false
#| code-fold: false
Suggests: 
    testthat (>= 3.0.0)
Config/testthat/edition: 3
```
    
- [x]   A new `tests/` folder is created, with a `testthat/` subfolder. The `testthat.R` file is created (which is sometimes referred to as the 'test runner,' because it runs all our tests).

```{bash}
#| eval: false
#| code-fold: false
tests/
  ├── testthat/
  └── testthat.R #<1>

2 directories, 1 file
```
1. Test runner

#### [`use_test()`]{style="font-size: 0.95em;"} unit tests

New tests are created with `usethis::use_test()`.[^test-files-folders]

[^test-files-folders]: Ideally there is a file named for every function in the `R/` folder, and a corresponding test file (with the `test-` prfix) in the `tests/testthat/` folder]

```{r}
#| eval: false
#| code-fold: false
usethis::use_test("scatter_plot")
```

##### [`test-`]{style="font-size: 0.95em;"} files

- [x]   **Test files**: the IDE will automatically create and open the new test file: 

```{verbatim}
#| eval: false
#| code-fold: false
✔ Writing 'tests/testthat/test-scatter_plot.R'
• Modify 'tests/testthat/test-scatter_plot.R'
```

##### [`test_that()`]{style="font-size: 0.95em;"} tests 

- [x]   **Tests**: Each new test file contains a boilerplate `test_that()` test, with `desc` (the test context) and `code` arguments (supplied in curly brackets). 

```{r}
#| eval: false
#| code-fold: false 
test_that(desc = "multiplication works", code = { 
 
})
```

##### [`expect_`]{style="font-size: 0.95em;"}ations 

- [x]   **Expectations**: most expectation have two parts: an `observed` object, and an `expected` object. The `observed` object is an artifact of some code you've written, and it's being compared against an `expected` result.

```{r}
#| eval: false
#| code-fold: false 
#| collapse: true
expect_equal( # <1> 
  object = 2 * 2, # <2> 
  expected = 4 # <3> 
  ) 
```
1. The test expectation
3. What is observed (i.e., the value or object)
4. What was expected (i.e., the predefined criteria for success) 

##### Running tests

- [x]   **Running tests**: Another `devtools` habit to adopt is regularly writing and running tests. If you're using Posit Workbench and have `devtools` installed, you can test your app-package using the **Build** pane or the keyboard shortcut:

::: {.column-margin}

[<kbd>Ctrl/Cmd</kbd> + <kbd>Shift</kbd> + <kbd>T</kbd>]{style="font-weight: bold; font-style: italic; font-size: 1.20em"}

:::


![`devtools::test()` (run all tests in your `tests/` folder)](img/08_tests_build_pane_test.png){#fig-08_tests_build_pane_test width='75%' align='center'}

When the test is run, you'll see feedback on whether it passes or fails (and some encouragement):

```{r}
#| eval: true
#| code-fold: false
#| echo: true 
#| collapse: true
test_that("multiplication works", { 
  expect_equal( 
    object = 2 * 2, 
    expected = 4 
    ) 
})
```

Next we're going to cover two additional testing tools (fixtures and helpers) you can include in your test suite to make testing your app-package a little easier.

## Fixtures 

<!-- https://www.tidyverse.org/blog/2020/04/self-cleaning-test-fixtures/ -->

Test fixtures are various resources used to ensure a consistent, well-defined environment. Fixtures can be input data, database connections, R options, environment variables, or anything else needed to create a repeatable infrastructure (even when the test is run on different environments or setups). 

A good test fixture also 'cleans itself up' after the test has been run to make sure any changes made during the test doesn't persist or interfere with other tests in your app-package.[^tests-fixtures] 

In `R/` packages, test fixtures are stored in the `tests/testthat/fixtures/` folder:

```{bash}
#| eval: false 
#| code-fold: false
tests/
├── testthat/
│   └── fixtures/                                          # <1>
└── testthat.R
```
1. The name '`fixtures`' isn't required (you can name this folder anything)

[^tests-fixtures]: Test fixtures are described in-depth in [R Packages, 2ed](https://r-pkgs.org/testing-advanced.html#test-fixtures) and in the [`testthat` documentation](https://testthat.r-lib.org/articles/test-fixtures.html#test-fixtures)

### Test data

The code used to create test data files should also be stored in `tests/testthat/fixtures/` with a clear naming convention. I've provided the code used to tidy `ggplot2movies::movies` as an example data fixture below:[^tests-fixtures-naming-conv] 

[^tests-fixtures-naming-conv]: Creating a tidied version of `ggplot2movies::movies` would be costly to re-create with every test, so it's advised to store it as an [static test fixture.](https://r-pkgs.org/testing-advanced.html#sec-testing-advanced-concrete-fixture)

```{r}
#| eval: false 
#| code-fold: show 
#| code-summary: 'show/hide make-ggp2-movies.R'
# pkgs <- c('ggplot2movies', 'tidyr', 'dplyr', 'stringr', 'purrr')
# install.packages(pkgs, quiet = TRUE)

# load packages --------------------
library(tidyr)
library(dplyr)
library(stringr)
library(purrr)

ggp2movies <- ggplot2movies::movies |>
  tidyr::pivot_longer(c(Action:Short),
    names_to = "genre_key",
    values_to = "genre_value"
  ) |>
  dplyr::mutate(genre_value = as.logical(genre_value)) |>
  dplyr::select(
    title, genre_key, genre_value, length,
    year, budget, avg_rating = rating, votes, mpaa
  ) |>
  dplyr::filter(genre_value == TRUE) |>
  dplyr::group_by(title) |>
  dplyr:::mutate(
    genres = paste0(genre_key, collapse = ", ")
  ) |>
  dplyr::select(
    title, genres, length, year,
    budget, avg_rating, votes, mpaa
  ) |>
  dplyr::ungroup() |>
  dplyr::distinct(.keep_all = TRUE) |>
  dplyr::mutate(
    genres = dplyr::na_if(x = genres, ""),
    genre = dplyr::case_when(
                stringr::str_detect(genres, ",") ~ 'Multiple genres',
                TRUE ~ genres),
    genre = factor(genre),
    mpaa = dplyr::na_if(x = mpaa, y = ""),
    mpaa = factor(mpaa,
      levels = c("G", "PG", "PG-13", "R", "NC-17"),
      labels = c("G", "PG", "PG-13", "R", "NC-17"))
    ) |> 
  dplyr::select(-genres)
# save to tests/testthat/fixtures/
saveRDS(object = ggp2movies, file = "tests/testthat/fixtures/ggp2_movies.rds") 
```

This file is saved in `tests/testthat/fixtures/` along with the output data.

```{bash}
#| eval: false 
#| code-fold: false 
tests/
  └── testthat/
      └── fixtures/
            ├── ggp2_movies.rds # <1>
            └── make-ggp2-movies.R # <1>
       
3 directories, 2 files
```
1. The code used to create the test data (`make-ggp2-movies.R`) is stored in the same location as the output it creates (i.e., `ggp2_movies.rds`):

Data files stored in `tests/testthat/fixtures/` can be accessed with `testthat::test_path()`


## Helpers 

<!-- https://r-pkgs.org/testing-design.html#testthat-helper-files -->

Test helpers are functions code that make creating and running tests easier.

> "*Helper files are a mighty weapon in the battle to eliminate code floating around at the top-level of test files.*" Testthat helper files, [R Packages, 2ed](https://r-pkgs.org/testing-design.html#testthat-helper-files)

### App input helpers 

The `var_inputs()` function below is a test fixture we can use to create inputs for the `scatter_plot()` utility function:

```{r}
#| eval: true 
#| code-fold: false 
#| collapse: true
var_inputs <- function() {
  list(
    y = "audience_score",
    x = "imdb_rating",
    z = "mpaa_rating",
    alpha = 0.5,
    size = 2,
    plot_title = "Enter plot title"
  )
}
var_inputs()$x
```

We can also create a helper for the tidy `ggplot2movies::movies` data:

```{r}
#| eval: true 
#| code-fold: false 
#| collapse: true
ggp2_inputs <- function() {
    list(
      x = "avg_rating",
      y = "length",
      z = "mpaa",
      alpha = 0.75,
      size = 3,
      plot_title = "Enter plot title"
    )
}
ggp2_inputs()$x
```

Using a function to derive test inputs is especially helpful when testing module server function because we don't need to change the way we refer to reactive inputs (as you'll see below). 

Functions like `var_inputs()` and `ggp2_inputs()` can be stored in `tests/testthat/helper.R`, which is automatically loaded with `devtools::test()`:

```{bash}
#| eval: false 
#| code-fold: false
tests/
  └── testthat/
      ├── fixtures/                                         # <1>
      │   ├── make-ggp2-movies.R
      │   └── ggp2_movies.rds
      ├── helper.R                                          # <2>
      └── test-scatter_plot.R                               # <3>
```
1. Test data script and `.rds` file  
2. `var_inputs()` and `ggp2_inputs()` functions  
3. Test file


The next section demonstrates how to use test fixtures and helpers with `testthat`'s [behavior-driven development](https://testthat.r-lib.org/reference/describe.html) functions (`describe()` and `it()`) and the contents of the traceability matrix. 


Now that we have our testing framework and the traceability matrix to guide development, I will cover a *slightly* different approach to testing that I’ve found helpful with app-packages (and has excellent support in `testthat`).

## Behavior-driven development

Behavior-driven development (BDD) (or behavior-driven testing) is a methodology that involves collaboration between developers, users, and domain experts to define requirements and write tests using domain-specific language. The BDD approach emphasizes writing human-readable descriptions of the application's behavior, which is then converted into a series of tests.

> "*[BDD] encourages teams to use conversation and concrete examples to formalize a shared understanding of how the application should behave.*" - [BDD, Wikipedia](https://en.wikipedia.org/wiki/Behavior-driven_development#:~:text)

Behavior-driven testing can be performed using a `testthat`'s `describe()` and `it()` functions.[^tests-bdd-describe] Below is an example of how these might look for the **US1** user specification in `shinyAppPkg`:

[^tests-bdd-describe]: Read more about `describe()` and `it()` in the [`testthat` documentation.](https://testthat.r-lib.org/reference/describe.html)

### [`describe()`]{style="font-size: 0.95em;"}

The `testthat::describe()` function follows a BDD format and '*specifies a larger component or function and contains a set of specifications*'

In `describe()`, I'll reference the feature I'm testing (**FE1**): 

```{r}
#| label: co_box_trace_matrix_specs_features_bdd
#| echo: false
#| code-fold: false
tibble::tibble(
  `User Specification` = c(
    "US1: scatter plot data visualization"
  ),
  `Feature Requirement` = c(
    "FE1: interactive scatter plot (two data sources, drop-down variable options)"
  )
) |> 
  gt::gt(auto_align = TRUE) |> 
  gt::sub_missing(
  columns = gt::everything(),
  rows = gt::everything(),
  missing_text = "-"
)
```

```{r}
#| eval: false 
#| code-fold: false
testthat::describe(description = "FE1: interactive scatter plot", code = {
  

})
```

The describe() functions can also be nested, which allows we me to include the functional requirement per test, also: 

```{r}
#| eval: false 
#| code-fold: false
testthat::describe("FE1: interactive scatter plot", code = { # <1>
  
  testthat::describe("FR1: data source", code = { # <2>
    # <2>
  }) # <2>
  
}) # <1>
```
1. Feature description  
2. Functional requirement


### [`it()`]{style="font-size: 0.95em;"}

Inside `describe()`, we can include multiple `it()` blocks which "*functions as a test and is evaluated in its own environment.*" 

In the example below, I use an `it()` block to test the first functional requirement:[^tests-it-blocks]

```{r}
#| eval: false 
#| code-fold: false
testthat::describe("FE1: interactive scatter plot", code = { # <1>
  
  testthat::describe("FR1: data source", code = { # <2>
    
    testthat::it("T1: data source", code = { # <3>
      # test code # <3>
    }) # <3>
    
  }) # <2>
  
}) # <1>
```
1. Feature description   
2. Functional requirement   
3. Test code   

```{r}
#| label: co_box_trace_matrix_specs_features_function__bdd
#| echo: false
#| code-fold: false
#| include: true
trace_matrix <- tibble::tibble(
  `Feature Requirement` = c(
    "FE1: interactive scatter plot (two data sources, drop-down variable options)",
    NA_character_,
    NA_character_,
    NA_character_
  ),
  `Functional Requirements` = c(
    "FR1: data source",
    "FR2: user-input updating",
    "FR3: color-coded data points",
    "FR4: plot axis, legend & title"
  )
) 
trace_matrix |> 
  gt::gt(auto_align = TRUE) |> 
  gt::sub_missing(
  columns = gt::everything(),
  rows = gt::everything(),
  missing_text = "-"
)
```

[^tests-it-blocks]: Each [`it()`](https://testthat.r-lib.org/reference/describe.html) block contains the expectations (or what you would traditionally include in `test_that()`).

Below is an example of how we might scope all the tests in the traceability matrix: 

```{r}
#| eval: false 
#| code-fold: false
testthat::describe("FE1: interactive scatter plot", code = { # <1>

  testthat::describe("FR1: data source", code = {
    testthat::it("T1: data source", code = {
      # test code
    })
  })

  testthat::describe("FR2: user-input updating", code = {
    testthat::it("T2: user-input updating", code = {
      # test code
    })
  })

  testthat::describe("FR3: color-coded data points", code = {
    testthat::it("T3: color-coded data points", code = {
      # test code
    })
  })

  testthat::describe("FR4: plot axis, legend & title", code = {
    testthat::it("T4: plot axis, legend & title", code = {
      # test code
    })
  })
})
```


## Example: unit tests

Below is a test that answers the question, '*does the plot generate without producing an error,*'[^tests-graphs] for `scatter_plot()`. This type of test appropriate because we're wanting to confirm the data source (`movies`) will generate a plot object, not necessarily the specific contents of the graph.

[^tests-graphs]: Snapshot tests would be more appropriate for answering the question, ['*Is the plot visually correct?*'](https://shiny.posit.co/r/articles/improve/server-function-testing/index.html#complex-outputs-plots-htmlwidgets).

```{r}
#| eval: false 
#| code-fold: false
testthat::it("T1: movies data source", code = {
  
  p <- scatter_plot(movies, 
    x_var = var_inputs()$x, 
    y_var = var_inputs()$y, 
    col_var = var_inputs()$z, 
    alpha_var = var_inputs()$alpha, 
    size_var = var_inputs()$size 
  )
  
  testthat::expect_true(object = ggplot2::is.ggplot(p))
  
})
```
1. Feature  
2. Functional requirement  
3. Test helper  

We can also use a nested `describe()` chunk to load and test a second data source (i.e., the `ggplot2movies::movies` data):[^tests-ggp2_movies-raw-data]

[^tests-ggp2_movies-raw-data]: If the data in `tests/testthat/fixtures/` are going to be used repeatedly, it might also make sense to store it in `inst/extdata/` or `data-raw/`.

```{r}
#| eval: false 
#| code-fold: false
testthat::describe("FE: interactive scatter plot", code = {  # <1>
  
  testthat::describe("FR1: data source (movies)", code = { # <2>
    
    testthat::it("T1: movies data source", code = { # <3>
      p <- scatter_plot(movies, # <4>
        x_var = var_inputs()$x, # <4>
        y_var = var_inputs()$y, # <4>
        col_var = var_inputs()$z, # <4>
        alpha_var = var_inputs()$alpha, # <4>
        size_var = var_inputs()$size # <4>
      )
      expect_true(object = ggplot2::is.ggplot(p)) # <5>
    }) # <3>
    
  }) # <2>
  
  testthat::describe("FR1: data source (ggp2_movies)", code = { # <6>
    
    testthat::it("T2: ggp2_movies.rds data source", code = { # <7>
      
      ggp2_movies <- readRDS(                             # <8>                  
                      testthat::test_path("fixtures", "ggp2_movies.rds")) # <8>
      
      p <- scatter_plot(ggp2_movies, # <9> 
        x_var = ggp2_inputs()$x, # <9> 
        y_var = ggp2_inputs()$y, # <9> 
        col_var = ggp2_inputs()$z, # <9> 
        alpha_var = ggp2_inputs()$alpha, # <9> 
        size_var = ggp2_inputs()$size # <9> 
      ) # <9> 
      
      expect_true(object = ggplot2::is.ggplot(p)) # <10> 
      
    }) # <7>
    
  }) # <6>
  
}) # <1>
```
1. User specification or feature    
2. `movies` data component  
3. `movies` data test   
4. Test `movies` data with `var_inputs()` helper   
5. `movies` expectation   
6. `ggp2_movies` data component  
7. `ggp2_movies` data test    
8. Load `ggp2_movies.rds` test fixture with `testthat::test_path()`   
9. Test `ggp2_movies` data with `ggp2_inputs()` helper  
10. `ggp2_movies` expectation 

```{r}
#| eval: false 
#| code-fold: true
#| include: false
testthat::describe(description = "scatter_plot() function", code = {
  
  testthat::describe(description = "movies data", code = {
    testthat::it(description = "is ggplot2 object", code = {
      p <- scatter_plot(movies,
        x_var = var_inputs()$x,
        y_var = var_inputs()$y,
        col_var = var_inputs()$z,
        alpha_var = var_inputs()$alpha,
        size_var = var_inputs()$size
      )
      expect_true(object = ggplot2::is.ggplot(p))
    })
    testthat::it(description = "adds labels", code = {
      p_labels <- scatter_plot(movies,
        x_var = var_inputs()$x,
        y_var = var_inputs()$y,
        col_var = var_inputs()$z,
        alpha_var = var_inputs()$alpha,
        size_var = var_inputs()$size
      ) +
        ggplot2::labs(
          title = var_inputs()$plot_title,
          x = stringr::str_replace_all(
            tools::toTitleCase(var_inputs()$x), "_", " "
          ),
          y = stringr::str_replace_all(
            tools::toTitleCase(var_inputs()$y), "_", " "
          )
        )
      expect_true(object = ggplot2::is.ggplot(p_labels))
    })

    testthat::it(description = "adds theme layer", code = {
      p_theme <- scatter_plot(movies,
        x_var = var_inputs()$x,
        y_var = var_inputs()$y,
        col_var = var_inputs()$z,
        alpha_var = var_inputs()$alpha,
        size_var = var_inputs()$size
      ) +
        ggplot2::labs(
          title = var_inputs()$plot_title,
          x = stringr::str_replace_all(
            tools::toTitleCase(var_inputs()$x), "_", " "
          ),
          y = stringr::str_replace_all(
            tools::toTitleCase(var_inputs()$y), "_", " "
          )
        ) +
        ggplot2::theme_minimal() +
        ggplot2::theme(legend.position = "bottom")

      expect_true(object = ggplot2::is.ggplot(p_theme))
    })
  })

  testthat::describe(description = "ggp2_movies data", code = {
    
    testthat::it(description = "is ggplot2 object", code = {
      ggp2_movies <- readRDS(test_path("fixtures", "ggp2_movies.rds"))
      p <- scatter_plot(ggp2_movies,
        x_var = ggp2_inputs()$x,
        y_var = ggp2_inputs()$y,
        col_var = ggp2_inputs()$z,
        alpha_var = ggp2_inputs()$alpha,
        size_var = ggp2_inputs()$size
      )
      expect_true(object = ggplot2::is.ggplot(p))
    })
    testthat::it(description = "adds labels", code = {
      ggp2_movies <- readRDS(test_path("fixtures", "ggp2_movies.rds"))
      p_labels <- scatter_plot(ggp2_movies,
        x_var = ggp2_inputs()$x,
        y_var = ggp2_inputs()$y,
        col_var = ggp2_inputs()$z,
        alpha_var = ggp2_inputs()$alpha,
        size_var = ggp2_inputs()$size
      ) +
        ggplot2::labs(
          title = ggp2_inputs()$plot_title,
          x = stringr::str_replace_all(
            tools::toTitleCase(ggp2_inputs()$x), "_", " "
          ),
          y = stringr::str_replace_all(
            tools::toTitleCase(ggp2_inputs()$y), "_", " "
          )
        )
      expect_true(object = ggplot2::is.ggplot(p_labels))
    })
    testthat::it(description = "adds theme layer", code = {
      ggp2_movies <- readRDS(test_path("fixtures", "ggp2_movies.rds"))
      p_theme <- scatter_plot(ggp2_movies,
        x_var = ggp2_inputs()$x,
        y_var = ggp2_inputs()$y,
        col_var = ggp2_inputs()$z,
        alpha_var = ggp2_inputs()$alpha,
        size_var = ggp2_inputs()$size
      ) +
        ggplot2::labs(
          title = ggp2_inputs()$plot_title,
          x = stringr::str_replace_all(
            tools::toTitleCase(ggp2_inputs()$x), "_", " "
          ),
          y = stringr::str_replace_all(
            tools::toTitleCase(ggp2_inputs()$y), "_", " "
          )
        ) +
        ggplot2::theme_minimal() +
        ggplot2::theme(legend.position = "bottom")

      expect_true(object = ggplot2::is.ggplot(p_theme))
    })
  })
})
```

## Example: snapshot tests

Writing tests for graph outputs can be difficult because the "correctness" of a graph is somewhat subjective and requires human judgment. If the expected output we're interesting in testing is cumbersome to describe programmatically, we can consider using a snapshot tests. Examples of this include UI elements (which are mostly HTML created by Shiny's UI layout and input/output functions) and data visualizations.[^tests-ui-tests]

[^tests-ui-tests]: Mastering Shiny covers [creatng a snapshot file](https://mastering-shiny.org/scaling-testing.html#user-interface-functions) to test UI elements, but also notes this is probably not the best approach.

### [`vdiffr`]{style="font-size: 1.05em;"}

If we want to create a graph snapshot test, the [`vdiffr`](https://vdiffr.r-lib.org/) package allows us to perform a 'visual unit test' by capturing the expected output as an `.svg` file that we can compare with future versions.

The `expect_doppelganger()` function from `vdiffr` is designed specifically to work with ['graphical plots'](https://vdiffr.r-lib.org/reference/expect_doppelganger.html). 

```{r}
#| eval: false 
#| code-fold: false
vdiffr::expect_doppelganger(
      title = "name of graph", 
      fig = # ...code to create graph...
  )
```

### Test logging

I prefer test outputs to be verbose, so I usually create some kind of  `test_logger()` helper function that allows me to give more context and information with each test:

```{r}
#| code-fold: false
#| eval: true
#| code-summary: 'show/hide test_logger() helper' 
# test logger helper
test_logger <- function(start = NULL, end = NULL, msg) {
  if (is.null(start) & is.null(end)) {
    cat("\n")
    logger::log_info("\n[{Sys.time()}| {msg}]")
  } else if (!is.null(start) & is.null(end)) {
    cat("\n")
    logger::log_info("\n[ START | {start} = {msg}]")
  } else if (is.null(start) & !is.null(end)) {
    cat("\n")
    logger::log_info("\n[ END   | {end} = {msg}]")
  } else {
    cat("\n")
    logger::log_info("\n[ START | {start} = {msg}]")
    cat("\n")
    logger::log_info("\n[ END   | {end} = {msg}]")
  }
}
```

`test_logger()` can be used to 'log' the `start` and `end` of each test, and it includes a message argument (`msg`) I use to match the test context.[^tests-logger]

[^tests-logger]: If you like verbose logging outputs, check out the [`logger` package](https://daroczig.github.io/logger/) 

In this test, I'll use `testthat::describe()` to list the feature from the traceability matrix (**FE1**), followed by a `testthat::it()` with each functional requirement the snapshot will capture.

```{r}
#| label: co_box_trace_matrix_feat_funct_snaps
#| echo: false
#| code-fold: false
#| include: true
trace_matrix <- tibble::tibble(
  `Feature Requirement` = c(
    "FE1: interactive scatter plot (two data sources, drop-down variable options)",
    NA_character_,
    NA_character_,
    NA_character_
  ),
  `Functional Requirements` = c(
    "FR1: data source",
    "FR2: user-input updating",
    "FR3: color-coded data points",
    "FR4: plot axis, legend & title"
  )
) 
trace_matrix |> 
  gt::gt(auto_align = TRUE) |> 
  gt::sub_missing(
  columns = gt::everything(),
  rows = gt::everything(),
  missing_text = "-"
)
```

```{r}
#| eval: false 
#| code-fold: false
testthat::describe("FR3: color & FR4: axis/legend/title", code = { # <1>
  
  testthat::it("T3: test scatter_plot(), labels & theme", code = { # <2>

    test_logger(               # <3>
      start = "T3",            # <3>
      msg = "Tests FR3 & FR4"  # <3>
    )                          # <3>
    
    vdiffr::expect_doppelganger( # <4>
      title = "scatter_plot() + theme", # <4>
      fig = scatter_plot(movies, # <4>
        x_var = var_inputs()$x, # <4>
        y_var = var_inputs()$y, # <4>
        col_var = var_inputs()$z, # <4>
        alpha_var = var_inputs()$alpha, # <4>
        size_var = var_inputs()$size # <4>
      ) + # <4>
        ggplot2::labs( # <5>
          title = var_inputs()$plot_title, # <5>
          x = stringr::str_replace_all( # <5>
                tools::toTitleCase( # <5>
                  var_inputs()$x), "_", " "), # <5>
          y = stringr::str_replace_all( # <5>
                tools::toTitleCase( # <5>
                  var_inputs()$y), "_", " ") # <5>
        ) + # <4>
        ggplot2::theme_minimal() + # <6>
        ggplot2::theme(legend.position = "bottom") # <6>
    ) # <6>
    
    test_logger(               # <7>
      end = "T3",              # <7>
      msg = "Tests FR3 & FR4"  # <7>
    )                          # <7>
    
  })
  
})
```
1. Feature description  
2. Non-technical language in test description    
3. Test logger (`start`)   
4. Snapshot with `movies` and `var_inputs()`  
5. Graph labels  
6. Graph theme  
7. Test logger (`end`)    

In this test, I could've separated each `ggplot2` layer into different `it()` sections, but the test results have the output from `test_logger()` to capture the context of what's being tested. 

We also see a warning when the snapshot has been saved in the `tests/testthat/_snaps/` folder the first time the test is run:

```{verbatim}
#| eval: false 
#| code-fold: false
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 0 ]
INFO [2023-10-02 14:15:22] [ START | T1 = Tests FR1]
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ]
INFO [2023-10-02 14:15:23] [ END   | T1 = Tests FR1]

INFO [2023-10-02 14:15:23] [ START | T2 = Tests FR1]
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 2 ]
INFO [2023-10-02 14:15:23] [ END   | T2 = Tests FR1]

INFO [2023-10-02 14:15:23] [ START | T3 = Tests FR3 & FR4]
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 3 ]
INFO [2023-10-02 14:15:23] [ END   | T3 = Tests FR3 & FR4]


─ Warning (test-scatter_plot.R:54:5): Has color (FR3) & axis/legend/title (FR4) ─
Adding new file snapshot: 'tests/testthat/_snaps/scatter-plot-theme.svg'
[ FAIL 0 | WARN 1 | SKIP 0 | PASS 3 ]
```

On subsequent runs, this warning will disappear (as long as there are no changes to the `scatter-plot-theme.svg` file).

We can also update the traceability matrix with the tests used to verify the functional requirements:

```{r}
#| label: co_box_trace_matrix_add_t1_and_t2
#| echo: false
#| code-fold: false
tibble::tibble(
  `User Specification` = c(
    "US1: scatter plot data visualization",
    NA_character_,
    NA_character_,
    NA_character_
  ),
  `Feature Requirement` = c(
    "FE1: interactive scatter plot (two data sources, drop-down variable options)",
    NA_character_,
    NA_character_,
    NA_character_
  ),
  `Functional Requirements` = c(
    "FR1: data source",
    "FR2: user-input updating",
    "FR3: color-coded data points",
    "FR4: plot axis and legend"
  ),
  Tests = c(
    "T1 & T2",
    NA_character_,
    "T3",
    "T3"
  )
) |> 
  gt::gt(auto_align = TRUE) |> 
  gt::sub_missing(
  columns = gt::everything(),
  rows = gt::everything(),
  missing_text = "-"
)
```

#### Snapshots are brittle

The term "brittle" in the context of testing refers to their susceptibility to fail with small changes. Brittleness can produce false negatives test failures (i.e., due to inconsequential changes in the graph) when comparing a new graph to the baseline image. 

If we compare the output from a custom plotting function like `scatter_plot()` against a graph built with analogous `ggplot2` code, we can see all the potential points of failure a test would have by passing both objects to `diffobj::diffObj()`:

```{r}
#| eval: false 
#| code-fold: false
ggp_graph <- ggplot2::ggplot(mtcars, 
              ggplot2::aes(x = mpg, y = disp)) + 
              ggplot2::geom_point(
                ggplot2::aes(color = cyl), 
                             alpha = 0.5, 
                             size = 3)
  
app_graph <- scatter_plot(mtcars, 
                  x_var = "mpg", 
                  y_var = "disp", 
                  col_var = "cyl", 
                  alpha_var = 0.5, 
                  size_var = 3)

diffobj::diffObj(ggp_graph, app_graph)
```


:::: {.column-page-inset-right}

:::{#fig-08_tests_diffobj_scatter_plot}

![`diffobj::diffObj()` on graph outputs](img/08_tests_diffobj_scatter_plot.png){#fig-08_tests_diffobj_scatter_plot width='100%' align='center'}

Graph objects are difficult to use as test objects 
:::

::::

The output shows us differences in the `mapping` and plot environment (`plot_env`), which we can assume *would* be different in the test (and the application), so using `testthat::expect_equal()` will fail.

Another option for using snapshots for testing is the `expect_snapshot_file()` function [^tests-10] but `expect_doppelganger()` is probably the better option for comparing graph outputs.

[^tests-10]: Follow the `expect_snapshot_file()` example from the [`testthat` documentation](https://testthat.r-lib.org/reference/expect_snapshot_file.html#ref-examples)

The goal of any shiny app should be to create something that helps drive data-driven decisions. Pragmatically, this means not writing code that isn't addressing a need the user or stakeholder has requested. I've found scoping building a traceability matrix and scoping the tests with the BDD functions forces me ask myself, "*is this function addressing a user need?*," which saves me from developing a feature no one asked for.

```{r}
#| label: git_box_shinyAppPkg_10c_tests-snapshots
#| echo: false
#| results: asis
#| eval: true
git_margin_box(
  fig_pw = '75%', 
  branch = "10c_tests-snapshots", 
  repo = 'shinyAppPkg')
```

## Example: integration tests 

<!--

Integration tests verify that multiple components work together. If
you find yourself instantiating multiple objects that interact with each
other in a test, you're probably writing an integration test. Integration
tests are often slower to execute and require a more elaborate setup than
unit tests. Developers run integration tests less frequently, so the feedback
loop is longer. These tests can flush out problems that are difficult
to identify by testing standalone units individually.

--> 

The BDD functions also allow us to combine tests for *reactive interactions* with `testServer()`, which means we can include tests to verify the inputs, outputs, and returned values from module server functions. 

To confirm functional requirement #2 (**FR2**) (that user-inputs are updating in the application), we need to test two changes:

1. Values passed to the UI are returned from `mod_var_input_server()`  
2. Reactive values passed into `mod_scatter_display_server()` are available in for `scatter_plot()` as `inputs()`

### [`session$returned()`]{style="font-size: 0.95em;"}

Inside `testServer()`, we can create a list of graph inputs for `mod_var_input_server()`, then pass identical values to `session$setInputs()` and confirm the returned object with `session$returned()`:

```{r}
#| eval: false 
#| include: true
#| code-fold: false
testthat::describe("FR2: user-input updating", code = {
  testthat::it("T4 & T5: creates shiny object", code = {
    shiny::testServer(app = mod_var_input_server, expr = {
      test_logger(
        start = "T6",
        msg = "FR2: var_input()"
      )

      test_vals <- list( # <1>
        y = "critics_score", # <1>
        x = "imdb_rating", # <1>
        z = "critics_rating", # <1>
        alpha = 0.75, # <1>
        size = 3, # <1>
        plot_title = "Example title" # <1>
      ) # <1>

      session$setInputs( # <2>
        y = "critics_score", # <2>
        x = "imdb_rating", # <2>
        z = "critics_rating", # <2>
        alpha = 0.75, # <2>
        size = 3, # <2>
        plot_title = "Example title" # <2>
      ) # <2>

      testthat::expect_equal( # <3>
        object = session$returned(), # <3>
        expected = test_vals # <3>
      ) # <3>

      test_logger(
        end = "T6",
        msg = "FR2: var_input()"
      )
    })
  })
})
```
1. Create list of variable values   
2. Set each input using `setInputs(input = )`  
3. Confirm returned values from module  

The test above confirms 1) new input values can be passed into the UI, and 2) these values are returned from `mod_var_input_server()`.

Now that we've confirmed `mod_var_input_server()` is returning values, we want to make sure reactive values are read correctly into the `var_inputs` argument in `mod_scatter_display_server()`. 

### [`args = list()`]{style="font-size: 0.95em;"}

In `movies_server()`, when we pass `selected_vars` to the `var_inputs` argument of `mod_scatter_display_server()`, we're not passing the returned values (this is why we don't need the parentheses). We're calling on the method (or function) created by `reactive()`. 

I've included the `movies_server()` function below to refresh our memory of how this *should* work:[^tests-12]

[^tests-12]: `selected_vars` are the reactive plot values returned from `mod_var_input_server()`.

```{r}
#| eval: false 
#| code-fold: false
movies_server <- function(input, output, session) {

      selected_vars <- mod_var_input_server("vars")

      mod_scatter_display_server("plot", var_inputs = selected_vars)
      
}
```


We can pause execution with Posit Workbench's debugger, [^tests-13] to see the difference between calling `selected_vars` and `selected_vars()`:

[^tests-13]: We'll cover using `browser()` and the IDE's debugger in a future chapter.

::: {layout="[49, -2, 49]"}

```{verbatim}
#| eval: false 
#| code-fold: false
Browse[1]> selected_vars
reactive({
    list(
      y = input$y, 
      x = input$x, 
      z = input$z, 
      alpha = input$alpha, 
      size = input$size, 
      plot_title = input$plot_title
      )
})
```

```{verbatim}
#| eval: false 
#| code-fold: false
Browse[1]> selected_vars()
$y
[1] "audience_score"

$x
[1] "imdb_rating"

$z
[1] "mpaa_rating"

$alpha
[1] 0.5

$size
[1] 2

$plot_title
[1] ""
```

:::

This distinction becomes important when we're testing the communication between module server functions. If we're testing a module server function that collects the reactive values from another module, we need to wrap the values in `reactive()` to simulate the application's environment:

```{r}
#| eval: false 
#| code-fold: false
testthat::it("T7 & T8: plot outputs", code = {
  shiny::testServer( 
    app = mod_scatter_display_server, # <1>
    args = list(
      var_inputs = shiny::reactive(var_inputs()) # <2>
    ), expr = {
      test_logger(start = "T7", msg = "FR2: is.reactive(inputs)")
      testthat::expect_true(object = is.reactive(inputs)) # <3>
      test_logger(end = "T7", msg = "FR2: is.reactive(inputs)")

      test_logger(start = "T8", msg = "FR2: inputs()")
      testthat::expect_equal(
        object = inputs(), # <4>
        expected = list( # <4>
          x = "imdb_rating", # <4>
          y = "audience_score", # <4>
          z = "mpaa_rating", # <4>
          alpha = 0.5, # <4>
          size = 2, # <4>
          plot_title = "Enter Plot Title" # <4>
        )
      )
      test_logger(end = "T8", msg = "FR2: inputs()")
    }
  )
})
```
1. Scatter-plot display module  
2. Reactive variable input values  
3. Test if `inputs` it's reactive  
4. Test if `inputs()` and values are equal inside the module  


I've included the example above because it's not included on the `testServer()` [documentation](https://shiny.posit.co/r/articles/improve/server-function-testing/), but I've found this method is great when you want to confirm two modules are communicating (i.e., returning and collecting outputs). System test with `shinytest2` are a better option if you're trying to capture a specific execution path (i.e., user story) in the application. 

## Example: system tests

System (or end-to-end) tests verify the whole application (or system) and are run to simulate real user interactions in a pre-production environment. Approaches to system testing vary, but in general, you'll want to run system tests before a release, which means the app's functional requirements are tested and released in lockstep.

### [`shinytest2`]{style="font-size: 1.05em;"}

[`shinytest2`](https://rstudio.github.io/shinytest2/index.html) requires a few steps to get up and running, most notably the [`chromote` package](https://rstudio.github.io/chromote/).[^shinytest2-start] 

[^shinytest2-start]: A great place to start is the [Getting Started](https://rstudio.github.io/shinytest2/articles/shinytest2.html) vignette. 

<!--

> Manual Testing: Initially, you may have to manually interact with the Shiny app to ensure it behaves as expected.

> Automated Browser Testing: Use tools like shinytest to automate browser-based testing for the Shiny application. shinytest captures the state of the application and allows you to compare it to expected states.

> User Scenarios: Script typical user interaction scenarios to ensure that the whole system, including front-end and back-end, work seamlessly.

-->




<!--

#### [`withr`]{style="font-size: 1.05em;"}

The date/time output from `test_logger()` doesn't provide the level of precision we want to monitor our unit tests (they're supposed to be fast). The R option to change this setting is `options(digits.secs)`, but we don't want to include this options in our test file (we just need it whenever we use `test_logger()`).[^tests-message2]

We can add `withr::local_options()` inside `test_logger()` to make sure this option only applies to the test environment it's called from. Subsequent tests will pass without a warning and provide a more precise date/time message:

[^tests-message2]: This example is similar to the `message2()` function on the [`testthat` website](https://testthat.r-lib.org/articles/test-fixtures.html#test-fixtures), but calls `withr` *inside* the helper function.

Back in the **Console**, I can see the output from `Sys.time()` doesn't include the `test_logger()` option:

```{r}
#| eval: false 
#| code-fold: false
Sys.time()
```

```{verbatim}
#| eval: false 
#| code-fold: false
[1] "2023-09-23 14:16:30 PDT"
```

-->

## Test coverage 

```{r}
#| eval: false 
#| code-fold: false
#| include: false
library(dplyr)
library(tidyr)
library(tibble)


pkg_tests <- testthat::test_package(package = "shinyAppPkg", 
  reporter = SilentReporter$new()) |> 
  tibble::as_tibble() |> 
  select(file, test, passed, failed, warning, error)

glimpse(pkg_tests)

pkg_tests |> count(test)
```


### [`covr`]{style="font-size: 1.05em;"}

### [`covrpage`]{style="font-size: 1.05em;"}

## Continuous Integration (CI)

<!--

> Consider setting up a CI pipeline using services like GitHub Actions, GitLab CI, or Travis CI to automate the running of these tests.

> By implementing these various levels of tests, you can build confidence that your R package and Shiny app are robust, reliable, and ready for deployment.

-->

<!--
Testing an R package with a Shiny application can be challenging due to the various components and layers of interactivity. Here's a general strategy to approach this:
-->


## Comparisons

Comparisons are the backbone of testing. Exploring the mechanics of how tests perform these comparisons (i.e., the underlying package(s)) can save you from surprising results. 

For example, `testthat::expect_equal()` compares whatever is passed to the `observed` and `expected` arguments with the [`waldo` package](https://www.tidyverse.org/blog/2021/08/waldo-0-3-0/), with some help from [`diffobj`](https://github.com/brodieG/diffobj).

```{r}
#| eval: true 
#| code-fold: false
#| collapse: true 
#| message: false 
#| warning: false
library(waldo)
library(diffobj)
library(tibble)
```

### [`waldo`]{style="font-size: 1.05em;"}

If you'd like a preview of a comparison before writing a formal test, you can pass the your `observed` and `expected` objects to `waldo::compare()`[^waldo-compare-args] 

[^waldo-compare-args]: Be mindful of the difference in arguments between expectation functions (i.e., `expect_equal()`) and `waldo::compare()`

```{r}
#| eval: true 
#| include: false
old <- tibble(
  chr = LETTERS[2:4],
  num = as.double(c(1.0, 2.0, 3.0)),
  fct = factor(c("low", "med", "high"), 
        levels = c("low", "med", "high"), 
        labels = c("L", "M", "H"),
        ordered = TRUE)
)
new <- data.frame(
  CHR = LETTERS[2:4],
  num = as.integer(c(1, 2, 3)),
  fct = factor(c("low", "med", "high"),
        levels = c("low", "med", "high"),
        labels = c("low", "med", "high"))
)
```

For example, suppose we have two objects: `old` and `new`

```{r}
#| eval: true 
#| code-fold: false
#| collapse: true
old
```

```{r}
#| eval: true 
#| code-fold: false
#| collapse: true
new
```

The outputs below are example outputs from `waldo::compare()`:

::: {layout="[[30,70]]"}

```{r}
#| eval: true 
#| code-fold: false
#| collapse: true
compare(
  x = old, # <1>
  y = old) # <1> 
```
1. Comparing identical objects


```{r}
#| eval: true 
#| code-fold: false
#| collapse: true
compare(
  x = old, # <1>
  y = new) # <1>
```
1. Comparing different objects

:::

`compare()` displays the differences in classes, names, and any individual value differences. 

### [`diffobj`]{style="font-size: 1.05em;"}

If you're using Posit Workbench, the [`diffobj` package](https://github.com/brodieG/diffobj) has a colorful display for making comparisons in the IDE. 

The differences can be displayed vertically with `diffobj::diffObj()`:

::: {layout="[[30,70]]"}

```{r}
#| eval: false 
#| code-fold: false
diffObj(
  old, 
  new)
```

![Viewer ouput from `diffobj::diffObj()`](img/08_tests_diffobj.png){#fig-08_tests_diffobj width='85%' align='center'}

:::

If you want to view the structure (`str()`) differences, you can use `diffobj::diffStr()`:

::: {layout="[[30, 70]]"}

```{r}
#| eval: false 
#| code-fold: false
diffStr(
  old, 
  new)
```

![Viewer ouput from `diffobj::diffStr()`](img/08_tests_diffstr.png){#fig-08_tests_diffobj width='100%' align='center'}
:::

After viewing the `old` vs `new` comparisons with `waldo` and `diffobj`, you should notice similarities and differences in the results from `testthat`[^compare-tolerance]

[^compare-tolerance]: The results from `testthat` don't include the differences between `old$num` and `new$num`. This is due to the `tolerance` argument, which can be adjusted in both functions.

```{verbatim}
#| eval: false 
#| code-fold: false
[ FAIL 1 | WARN 0 | SKIP 0 | PASS 0 ]

── Failure (test-old_vs_new.R:17:3): old vs. new ───────────────────────────────
`new` (`actual`) not equal to `old` (`expected`).

`class(actual)`:   "data.frame"                   
`class(expected)`: "tbl_df"     "tbl" "data.frame"

`names(actual)`:   "CHR" "num" "fct"
`names(expected)`: "chr" "num" "fct"

`actual$CHR` is a character vector ('B', 'C', 'D')
`expected$CHR` is absent

`class(actual$fct)`:   "factor"          
`class(expected$fct)`: "ordered" "factor"

`levels(actual$fct)`:   "low" "med" "high"
`levels(expected$fct)`: "L"   "M"   "H"   

`actual$chr` is absent
`expected$chr` is a character vector ('B', 'C', 'D')
[ FAIL 1 | WARN 0 | SKIP 0 | PASS 0 ]
```

