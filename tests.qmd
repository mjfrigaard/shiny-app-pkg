# Tests {.unnumbered}

```{r}
#| eval: true 
#| echo: false 
#| include: false
source("_common.R")
library(testthat)
```

```{r}
#| label: co_box_dev
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "y",
  header = "Caution!",
  contents = "This section is currently being revised. Thank you for your patience."
)
```

Writing tests for your app-package poses some unique challenges. Shiny functions are written in the context of it's [reactive model](https://shiny.posit.co/r/articles/build/reactivity-overview/), so some conventional testing techniques and methods for regular R packages don’t directly apply. Fortunately, the infrastructure for storing and running tests in your app-package is identical to a standard R package.

This chapter will cover three layers of testing (unit testing, integration/module testing, and end-to-end testing), a summary of the recommendations for testing shiny apps, and where you should focus your tests.

```{r}
#| label: co_box_tests
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "o",
  header = "Testing disclaimer", fold = FALSE, look = "default",
  contents = "
  
I won’t be covering *how* to write tests (plenty of these resources exist, and I’ve listed them below). Instead, I will focus on *what to test and why*. I’ll also touch on the link between user requirements, functional specifications, and using a traceability matrix to track issues & features.
  
- Unit tests are covered extensively in [R Packages, 2ed](https://r-pkgs.org/testing-basics.html) and the [`testthat` documentation](https://testthat.r-lib.org/index.html). 
  
- The [`testServer()`](https://shiny.posit.co/r/articles/improve/server-function-testing/) documentation is sparse, so I'll provide a few tips and tricks I've learned for testing modules. 
  
- `shinytest2` has [excellent documentation](https://rstudio.github.io/shinytest2/) (and [videos](https://www.youtube.com/watch?v=Gucwz865aqQ)), and I highly recommend reading through those resources.
  "
)
```

First, we want to make sure the testing packages are installed.

```{r}
#| eval: false
#| code-fold: false
install.packages(c("testthat", "shinytest2", "covr"))
library(testthat)
library(shinytest2)
library(covr)
```

(*If you're using `devtools`, you won't have to worry about installing `testthat` and `covr`*)

## Running [`test()`]{style="font-size: 1.05em;"}s

```{r}
#| label: co_box_tests_pkgApp
#| echo: false
#| eval: false
```

 
The fourth `devtools` [habit]{style="font-weight: bold; font-size: 1.0em; color: #772953"} to adopt is regularly writing and running tests. If you're using Posit Workbench and have `devtools` installed, you can test your app-package using the **Build** pane or the keyboard shortcut:

::: {.column-margin}

[<kbd>Ctrl/Cmd</kbd> + <kbd>Shift</kbd> + <kbd>T</kbd>]{style="font-weight: bold; font-style: italic; font-size: 1.20em"}

:::


::: {#fig-08_tests_build_pane_test}

![Test you app-package](img/08_tests_build_pane_test.png){#fig-08_tests_build_pane_test width='100%' align='center'}

`devtools::test()` (run all tests in your `tests/` folder)
:::

When we initally run `devtools::test()` in `pkgApp`, we see the following:

```{verbatim}
#| eval: false
#| code-fold: false
==> devtools::test()

ℹ No testing infrastructure found.
• Setup testing with `usethis::use_testthat()`.
```

This shouldn't be surprising--we haven't written any tests for `pkgApp` yet!. 

The error is informative, though, because it tells us `pkgApp` doesn't have the testing infrastructure set up. In packages using `devtools`, the unit testing infrastructure is built with `usethis::use_testthat()`

In the next section we'll cover setting up the test suite in your app-package. 

```{r}
#| label: co_box_monthApp_tests
#| echo: false
#| results: asis
#| eval: true
co_box(
  color = "g", fold = TRUE,
  header = "Mastering Shiny `monthApp` tests",
" 
If you downloaded, loaded and installed [`monthApp` example from Mastering Shiny](https://github.com/hadley/monthApp), then clicked on **Test** in the **Build** pane, you also saw the following:

![Testing `monthApp` app-package](img/monthApp_test.png){width='100%' fig-align='center'}

"
  )
```


## The test suite

If you adopt [test-driven development](https://en.wikipedia.org/wiki/Test-driven_development), you'll develop tests before writing any utility functions, modules, or your standalone app function. However, if you're a mere mortal like the rest of us, you'll typically develop your tests and functions in tandem. 

Regardless of the testing strategy, we'll follow the advice in the output above and set up the testing infrastructure with the [`testthat` package](https://testthat.r-lib.org/):

### [`use_testthat()`]{style="font-size: 1.05em;"}

The 'infrastructure' created by running `usethis::use_testthat()` is detailed below: 

```{r}
#| eval: false
#| code-fold: false
usethis::use_testthat()
```

-   Set active project to current working directory: 

    ```{verbatim}
    #| eval: false
    #| code-fold: false
    ✔ Setting active project to '/path/to/pkgApp'
    ```

-   In the `DESCRIPTION` file, add the `Suggests` field and include `testthat (>= 3.0.0)` and the testthat edition (`Config/testthat/edition: 3`)

    ```{verbatim}
    #| eval: false
    #| code-fold: false
    ✔ Adding 'testthat' to Suggests field in DESCRIPTION
    ✔ Adding '3' to Config/testthat/edition
    ```
    
-   A new `tests/` folder is created, with a `testthat/` subfolder:

    ```{verbatim}
    #| eval: false
    #| code-fold: false
    ✔ Creating 'tests/testthat/'
    ```
    
-   The `testthat.R` file is created (sometimes referred to as the test 'runner' because it runs all your tests).
    
    ```{verbatim}
    #| eval: false
    #| code-fold: false
    ✔ Writing 'tests/testthat.R'
    ```

Finally, we're given some advice on the next step for creating our first test: 

```{verbatim}
#| eval: false
#| code-fold: false
• Call `use_test()` to initialize a basic test file and open it for editing.
```

Our new `tests/` folder structure is below: 

```{bash}
#| eval: false
#| code-fold: false
tests/
  ├── testthat
  └── testthat.R

2 directories, 1 file
```


## Unit tests

<!--
> Test Core Functions: Use the `testthat` package to write unit tests for all core logic functions. These are the building blocks of your application and should be thoroughly tested.

> Mock Inputs: For functions that rely on external data or user input, use mock data to simulate various scenarios.

> Boundary Conditions: Test edge cases and boundary conditions to ensure stability.

> Test Coverage: Use tools like covr to measure test coverage and aim for a high percentage.
-->

If I'm writing write a unit test for the `scatter_plot()` function in `R/scatter_plot.R`, I'll create test file with `usethis::use_test("scatter_plot")`.

### New tests with [`use_test()`]{style="font-size: 1.05em;"}

```{r}
#| eval: false
#| code-fold: false
usethis::use_test("scatter_plot")
```

#### Test files

The IDE will automatically open the new test file: 

```{verbatim}
#| eval: false
#| code-fold: false
✔ Writing 'tests/testthat/test-scatter_plot.R'
• Modify 'tests/testthat/test-scatter_plot.R'
```

#### Tests

The new test file contains a boilerplate test (I've included the argument names):

```{r}
#| eval: false
#| code-fold: false
test_that(desc = "multiplication works", code = {
  expect_equal(2 * 2, 4)
})
```

Each `testthat` test has a test context (supplied to the `desc` argument) followed by the test `code` (supplied in curly brackets). When a test is run, you'll see feedback on whether it passes or fails:

```{r}
#| eval: true
#| code-fold: false
test_that(desc = "multiplication works", code = {
  expect_equal(2 * 2, 4)
})
```

#### Expectations

Most expectation have two parts: an `observed` object, and an `expected` object. The `observed` object is an artifact of some code you've written, and it's being compared against a known result (i.e., what is `expected`)

```{r}
#| eval: true 
#| code-fold: false
expect_equal(
  object = 2 * 2,
  expected = 4)
```

#### Comparisons

All tests are comprised of comparisons, and I've found knowing what package is performing the underlying comparison often saves me from surprising tests results. 

`testthat::expect_equal()` compares the `observed` and `expected` objects with the [`waldo` package](https://www.tidyverse.org/blog/2021/08/waldo-0-3-0/), with some help from [`diffobj`](https://github.com/brodieG/diffobj).

```{r}
#| eval: true 
#| code-fold: false
#| collapse: true 
#| message: false 
#| warning: false
library(waldo)
library(diffobj)
library(tibble)
```

If you'd like a preview before writing test, you can pass the `observed` and `expected` objects to `waldo::compare()` to see what the result will be. 

```{r}
#| eval: true 
#| include: false
old <- tibble(
  chr = LETTERS[2:4],
  num = as.double(c(1, 2, 3)),
  fct = factor(c("low", "med", "high"), 
        levels = c("low", "med", "high"), 
        labels = c("L", "M", "H"),
        ordered = TRUE)
)
new <- data.frame(
  CHR = LETTERS[2:4],
  num = as.integer(c(1, 2, 3)),
  fct = factor(c("low", "med", "high"),
        levels = c("low", "med", "high"),
        labels = c("low", "med", "high"))
)
```


For example, suppose we have two objects: `old` and `new`

```{r}
#| eval: true 
#| code-fold: false
#| collapse: true
str(old)
```

```{r}
#| eval: true 
#| code-fold: false
#| collapse: true
str(new)
```



::: {layout="[[30,70]]"}

```{r}
#| eval: true 
#| code-fold: false
#| collapse: true
compare(
  x = old, 
  y = old)
```


```{r}
#| eval: true 
#| code-fold: false
#| collapse: true
compare(
  x = old, 
  y = new)
```

:::

If you're using Posit Workbench, the [`diffobj` package](https://github.com/brodieG/diffobj) is better for making comparisons in the IDE. You can look at differences vertically with `diffobj::diffObj()`:

::: {layout="[[30,70]]"}

```{r}
#| eval: false 
#| code-fold: false
diffObj(
  old, 
  new)
```

![Viewer ouput from `diffobj::diffObj()`](img/08_tests_diffobj.png){#fig-08_tests_diffobj width='85%' align='center'}

:::

Or you can view the `str()` differences with `diffobj::diffStr()`:

::: {layout="[[30, 70]]"}

```{r}
#| eval: false 
#| code-fold: false
diffStr(
  old, 
  new)
```

![Viewer ouput from `diffobj::diffStr()`](img/08_tests_diffstr.png){#fig-08_tests_diffobj width='100%' align='center'}
:::

Aftering seeing the `old` vs `new` comparisons with `waldo` and `diffobj`, we can see similarities in the results of a `testthat` test: 

```{verbatim}
#| eval: false 
#| code-fold: false
[ FAIL 1 | WARN 0 | SKIP 0 | PASS 0 ]

── Failure (test-old-vs-new.R:18:3): old vs. new ──────────
`old` (`actual`) not equal to `new` (`expected`).

`class(actual)`:   "tbl_df" "tbl" "data.frame"
`class(expected)`:                "data.frame"

`names(actual)`:   "chr" "num" "fct"
`names(expected)`: "CHR" "num" "fct"

`actual$chr` is a character vector ('B', 'C', 'D')
`expected$chr` is absent

`class(actual$fct)`:   "ordered" "factor"
`class(expected$fct)`:           "factor"

`levels(actual$fct)`:   "L"   "M"   "H"   
`levels(expected$fct)`: "low" "med" "high"

`actual$CHR` is absent
`expected$CHR` is a character vector ('B', 'C', 'D')
[ FAIL 1 | WARN 0 | SKIP 0 | PASS 0 ]
```


### Snapshots

Writing tests for graph outputs can be difficult, because we're evaluating the output from a custom plotting function like `scatter_plot()`. If we try to compare it against a graph built with analogous `ggplot2` code, we see the following failure: 

```{r}
#| eval: false 
#| code-fold: false
test_that(desc = "scatter_plot() works", code = {
  
  expect_equal(object = 
              ggplot2::ggplot(mtcars, 
                ggplot2::aes(x = mpg, y = disp)) + 
                  ggplot2::geom_point(
                    ggplot2::aes(color = cyl), 
                                 alpha = 0.5, 
                                 size = 3), 
    expected = scatter_plot(mtcars, 
                    x_var = "mpg", 
                    y_var = "disp", 
                    col_var = "cyl", 
                    alpha_var = 0.5, 
                    size_var = 3))
})

```


#### [`vdiffr`]{style="font-size: 1.05em;"}

The [`vdiffr` package ](https://vdiffr.r-lib.org/) has it's own expectation, `expect_doppelganger()`, that has `title` and `fig` arguments:

```{r}
#| eval: false 
#| code-fold: false
test_that(desc = "scatter_plot() works", code = {
    vdiffr::expect_doppelganger(
      title = "scatter_plot() graph", 
      fig = scatter_plot(mtcars, 
                    x_var = "mpg", 
                    y_var = "disp", 
                    col_var = "cyl", 
                    alpha_var = 0.5, 
                    size_var = 3))
})
```

On the initial run, a snapshot is saved into the `tests/testthat/_snaps/` folder:

```{verbatim}
#| eval: false 
#| code-fold: false
==> Testing R file using 'testthat'

ℹ Loading pkgApp
[ FAIL 0 | WARN 1 | SKIP 0 | PASS 1 ]

── Warning (test-scatter_plot.R:2:5): scatter_plot() works ──────
Adding new file snapshot: 'tests/testthat/_snaps/scatter-plot-graph.svg'
[ FAIL 0 | WARN 1 | SKIP 0 | PASS 1 ]

Test complete
```

```{bash}
#| eval: false 
#| code-fold: false
tests/
├── testthat
│   ├── _snaps
│   │   └── scatter_plot
│   │       └── scatter-plot-graph.svg
│   └── test-scatter_plot.R
└── testthat.R
4 directories, 3 files

```

The `scatter-plot-graph.svg` file becomes our baseline comparison object, when is used in future tests. 

After writing tests, we'll load, document, and build:

[<kbd>Ctrl/Cmd</kbd> + <kbd>Shift</kbd> + <kbd>L</kbd> / <kbd>D</kbd> / <kbd>B</kbd>]{style="font-style: italic; font-weight: bold; font-size: 1.10em"}

Then we test `pkgApp`:

[<kbd>Ctrl/Cmd</kbd> + <kbd>Shift</kbd> + <kbd>T</kbd>]{style="font-style: italic; font-weight: bold; font-size: 1.10em"}

```{verbatim}
#| eval: false 
#| code-fold: false
devtools::test()

ℹ Testing pkgApp
✔ | F W S  OK | Context
✔ |         1 | scatter_plot  

══ Results ═══════════════════
[ FAIL 0 | WARN 0 | SKIP 0 | PASS 1 ]
```


## Module/integration tests

<!--
> Data Flow: Test how well the different units work together by simulating a complete data flow.

> Database Interactions: If your package interacts with a database, write tests to ensure that read/write operations are working as expected.

> API Calls: If your package makes API calls, use package like httptest to mock API responses and test the integration.
-->

Module tests involve both UI and server functions, I consider these integration tests (i.e., we're testing how a module's functions 'integrate' with each other, or with other modules). 

### [`testServer()`]{style="font-size: 1.05em;"}

### [`args = list()`]{style="font-size: 1.05em;"}

## System/end-to-end tests

<!--
> Manual Testing: Initially, you may have to manually interact with the Shiny app to ensure it behaves as expected.

> Automated Browser Testing: Use tools like shinytest to automate browser-based testing for the Shiny application. shinytest captures the state of the application and allows you to compare it to expected states.

> User Scenarios: Script typical user interaction scenarios to ensure that the whole system, including front-end and back-end, work seamlessly.
-->


### [`shinytest2`]{style="font-size: 1.05em;"}

## Test coverage 

### [`covr`]{style="font-size: 1.05em;"}

### [`covrpage`]{style="font-size: 1.05em;"}

## Continuous Integration (CI)

<!--

> Consider setting up a CI pipeline using services like GitHub Actions, GitLab CI, or Travis CI to automate the running of these tests.

> By implementing these various levels of tests, you can build confidence that your R package and Shiny app are robust, reliable, and ready for deployment.

-->

end `testing.qmd`

<!--
Testing an R package with a Shiny application can be challenging due to the various components and layers of interactivity. Here's a general strategy to approach this:
-->